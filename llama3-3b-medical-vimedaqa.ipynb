{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes torch ragas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datasets import load_dataset, Dataset\nfrom kaggle_secrets import UserSecretsClient\nimport random\nimport pandas as pd\n\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    answer_similarity,\n    answer_correctness,\n)\nfrom langchain_openai import ChatOpenAI\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face and OpenAI\n# We retrieve the tokens you stored in Kaggle Secrets.\n\n# Hugging Face Token for the generation model\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    hf_token = None\n\n# OpenAI API Key for the Ragas evaluation model\ntry:\n    openai_api_key = user_secrets.get_secret(\"OPENAI_API_KEY\")\n    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n    # print(\"OpenAI API Key configured successfully.\")\nexcept Exception as e:\n    print(\"Could not retrieve OpenAI API Key. Please ensure it is stored as a Kaggle secret named 'OPENAI_API_KEY'.\")\n    openai_api_key = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\nmodel_id = \"alpha-ai/LLAMA3-3B-Medical-COT\"\ndataset_id = \"tmnam20/ViMedAQA\"\n\n# Step 4: Load the Dataset\n# We load the ViMedAQA dataset from Hugging Face.\n# This dataset contains Vietnamese medical questions and answers.\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(\"Dataset loaded successfully!\")\n    print(\"Example from the dataset:\")\n    print(dataset[0])\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    dataset = None\n\n# Step 5: Load the Model and Tokenizer\n# We will load the model in 4-bit precision (quantization) to save memory,\n# which is highly recommended for running larger models on Kaggle's GPUs.\nif hf_token:\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            token=hf_token,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            quantization_config=None  # You can add a BitsAndBytesConfig here for quantization if needed\n        )\n        print(\"Model and tokenizer loaded successfully!\")\n    except Exception as e:\n        print(f\"Failed to load the model or tokenizer. Error: {e}\")\n        model = None\n        tokenizer = None\nelse:\n    print(\"Hugging Face token not available. Cannot load the model.\")\n    model = None\n    tokenizer = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Set up a Text Generation Pipeline and Prepare for Evaluation\nif model and tokenizer:\n    # The pipeline simplifies the process of using the model for text generation.\n    text_generator = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n    )\n    print(\"Text generation pipeline is ready.\")\n\n    # Step 7: Select Samples, Format Prompts, and Collect Data for Ragas\n    if dataset:\n        num_samples_to_evaluate = 5 # Ragas can be slow, so let's start with a small, representative sample.\n\n        # Generate N random indices to select samples from the dataset\n        random_indices = random.sample(range(len(dataset)), num_samples_to_evaluate)\n        eval_dataset = dataset.select(random_indices)\n\n        prompts = []\n        # These lists will store the data needed for Ragas evaluation\n        ids_for_ragas = []\n        questions_for_ragas = []\n        contexts_for_ragas = []\n        ground_truths_for_ragas = []\n\n        for sample in eval_dataset:\n            question_id = sample['question_idx']\n            question = sample['question']\n            context = sample['context']\n            original_answer = sample['answer']\n\n            # Store data for Ragas\n            ids_for_ragas.append(question_id)\n            questions_for_ragas.append(question)\n            contexts_for_ragas.append([context]) # Ragas expects context to be a list of strings\n            ground_truths_for_ragas.append(original_answer)\n\n            # Format the prompt for the model\n            prompt_template = f\"\"\"\n            <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n            You are a helpful medical assistant. Based *only* on the context provided below, answer the question in Vietnamese.\n\n            Context: {context}\n\n            Question: {question}\n\n            <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n            \"\"\"\n            prompts.append(prompt_template)\n\n        print(f\"\\nPreparing to generate answers for {num_samples_to_evaluate} random samples...\")\n\n        # Step 8: Generate Answers for the entire batch\n        try:\n            generated_outputs_batch = text_generator(\n                prompts,\n                max_new_tokens=256,\n                do_sample=True,\n                temperature=0.01,\n                top_p=0.9,\n                eos_token_id=tokenizer.eos_token_id,\n                padding=True,\n                truncation=True\n            )\n\n            # Collect the generated answers for Ragas\n            generated_answers_for_ragas = []\n            answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n\n            print(\"\\n--- Model Generation Complete. Sample Outputs: ---\")\n            for i, output in enumerate(generated_outputs_batch):\n                generated_text = output[0]['generated_text']\n                clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                generated_answers_for_ragas.append(clean_answer)\n\n                # Print a few examples to see the model's performance\n                if i < 3: # Print first 3 examples\n                    print(f\"\\n--- Sample {i+1}/{num_samples_to_evaluate} ---\")\n                    print(f\"Sample ID: {ids_for_ragas[i]}\") \n                    print(f\"Question: {questions_for_ragas[i]}\")\n                    print(f\"Context: {contexts_for_ragas[i]}\")\n                    print(f\"Model Answer: {clean_answer}\")\n                    print(f\"Ground Truth: {ground_truths_for_ragas[i]}\")\n                    print(\"-\" * 50)\n\n        except Exception as e:\n            print(f\"An error occurred during text generation: {e}\")\nelse:\n    print(\"\\nSkipping text generation due to issues with model/tokenizer loading.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Debugging a single data point","metadata":{}},{"cell_type":"code","source":"# Step 6 & 7: Isolate and Prepare a SINGLE Sample for Debugging\n\n# ---------------------------------------------------------------------------------\n# USER ACTION: CHANGE THIS ID to the specific sample you want to debug.\nTARGET_SAMPLE_ID = \"body-part_1140\" \n# ---------------------------------------------------------------------------------\n\nif model and tokenizer and dataset:\n    # Find the specific sample in the dataset\n    target_sample = None\n    for sample in dataset:\n        if sample['question_idx'] == TARGET_SAMPLE_ID:\n            target_sample = sample\n            break\n            \n    if target_sample:\n        print(f\"Found sample with ID: {TARGET_SAMPLE_ID}\")\n        \n        # Create a new mini-dataset containing only our target sample\n        # Ragas and other functions expect a Dataset object, so we build one.\n        eval_data_dict = {key: [value] for key, value in target_sample.items()}\n        eval_dataset = Dataset.from_dict(eval_data_dict)\n        \n        # --- The rest of the pipeline now runs on this single sample ---\n        \n        text_generator = pipeline(\n            \"text-generation\", model=model, tokenizer=tokenizer,\n            torch_dtype=torch.bfloat16, device_map=\"auto\"\n        )\n        print(\"Text generation pipeline is ready.\")\n\n        # Prepare prompts and data lists (now they will only have one item)\n        prompts, ids_for_ragas, questions_for_ragas, contexts_for_ragas, ground_truths_for_ragas = [], [], [], [], []\n        for sample in eval_dataset:\n            ids_for_ragas.append(sample['question_idx'])\n            questions_for_ragas.append(sample['question'])\n            contexts_for_ragas.append([sample['context']])\n            ground_truths_for_ragas.append(sample['answer'])\n            prompt_template = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n            Based on the context below, answer the question. \n            \n            **Rules:**\n            1. You MUST extract the answer directly from the context.\n            2. The answer must be the exact, continuous text from the context.\n            3. DO NOT add extra words or form a full sentence.\n            \n            **Example:**\n            - Context: \"Đến năm 1327, đây là thị trấn lớn thứ ba tại Warwickshire.\"\n            - Question: \"Vào thế kỉ XIV, Birmingham trở thành thị trấn lớn thứ mấy tại Warwickshire?\"\n            - Correct Answer: \"lớn thứ ba\"\n\n            **Now, perform the task with the following:**\n            \n            Context: {sample['context']}\n            \n            Question: {sample['question']}\n\n            <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n            prompts.append(prompt_template)\n\n        print(f\"\\nPreparing to generate an answer for sample {TARGET_SAMPLE_ID}...\")\n\n        # Step 8: Generate the single answer\n        try:\n            generated_output = text_generator(\n                # prompts, max_new_tokens=256, do_sample=True, temperature=0.1, top_p=0.9,\n                prompts, max_new_tokens=256, do_sample=True, temperature=0.0, # Temperature 0.0 for deterministic extraction\n                eos_token_id=tokenizer.eos_token_id, padding=True, truncation=True\n            )[0] # Get the first and only result\n\n            generated_answers_for_ragas = []\n            answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n            clean_answer = generated_output[0]['generated_text'].split(answer_start_tag)[-1].strip()\n            generated_answers_for_ragas.append(clean_answer)\n\n            print(\"\\n--- Model Generation Complete ---\")\n            print(f\"Sample ID: {ids_for_ragas[0]}\")\n            print(f\"Question: {questions_for_ragas[0]}\")\n            print(f\"Model Answer: {clean_answer}\")\n            print(f\"Ground Truth: {ground_truths_for_ragas[0]}\")\n            \n        except Exception as e:\n            print(f\"An error occurred during text generation: {e}\")\n            \n    else:\n        print(f\"ERROR: Could not find any sample with ID '{TARGET_SAMPLE_ID}' in the dataset.\")\n\nelse:\n    print(\"\\nSkipping generation due to issues with model, tokenizer, or dataset loading.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Prepare the Dataset for Ragas Evaluation\n# Ragas expects a Hugging Face Dataset object with specific column names:\n# - question: The question asked.\n# - contexts: A list of context strings.\n# - answer: The answer generated by the model.\n# - ground_truth: The reference answer from the original dataset.\n\nif 'generated_answers_for_ragas' in locals():\n    # Create a dictionary with the collected data\n    ragas_data = {\n        \"question\": questions_for_ragas,\n        \"contexts\": contexts_for_ragas,\n        \"answer\": generated_answers_for_ragas,\n        \"ground_truth\": ground_truths_for_ragas\n    }\n\n    # Convert the dictionary to a Hugging Face Dataset\n    ragas_dataset = Dataset.from_dict(ragas_data)\n\n    print(\"Dataset prepared for Ragas evaluation.\")\n    print(ragas_dataset)\n\nelse:\n    print(\"Could not find generated answers. Skipping Ragas evaluation.\")\n    ragas_dataset = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Run Ragas Evaluation Sequentially for Robustness\n\n# Disable debug mode for a cleaner output\nimport langchain\nlangchain.debug = False\n\n# Import Python's built-in warnings module\nimport warnings\nimport pandas as pd\n\nif 'generated_answers_for_ragas' in locals() and openai_api_key:\n    # Prepare the single-item dataset for Ragas\n    ragas_data = {\n        \"question\": questions_for_ragas,\n        \"contexts\": contexts_for_ragas,\n        \"answer\": generated_answers_for_ragas,\n        \"ground_truth\": ground_truths_for_ragas\n    }\n    ragas_dataset = Dataset.from_dict(ragas_data)\n    \n    print(\"\\nStarting Ragas evaluation SEQUENTIALLY to improve reliability...\")\n    print(\"=\"*50)\n\n    # Configure the judge LLM\n    evaluation_llm = ChatOpenAI(model=\"gpt-4-turbo\")\n\n    # Define the metrics we want to compute\n    metrics_to_run = [\n        faithfulness,\n        answer_relevancy,\n        answer_similarity,\n        answer_correctness,\n    ]\n    \n    # --- Evaluate metrics one by one ---\n    final_scores = {}\n    for metric in metrics_to_run:\n        metric_name = metric.name\n        print(f\"Evaluating metric: [{metric_name}]...\")\n        try:\n            # Run evaluation for a single metric\n            result = evaluate(\n                dataset=ragas_dataset,\n                metrics=[metric],\n                llm=evaluation_llm\n            )\n            # Store the score\n            final_scores[metric_name] = result[metric_name]\n        except Exception as e:\n            print(f\"  ERROR evaluating {metric_name}: {e}\")\n            final_scores[metric_name] = \"Error\" # Mark as error instead of NaN\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Ragas sequential evaluation complete!\")\n    print(\"=\"*50 + \"\\n\")\n\n    # Display the results in a DataFrame\n    results_df = pd.DataFrame([final_scores])\n    display(results_df)\n\nelse:\n    print(\"Skipping Ragas evaluation. Check if generation was successful and if the OpenAI API Key is configured.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}