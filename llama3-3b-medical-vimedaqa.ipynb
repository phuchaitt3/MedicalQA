{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes torch ragas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T01:54:39.298864Z","iopub.execute_input":"2025-09-08T01:54:39.299160Z","iopub.status.idle":"2025-09-08T01:54:43.926551Z","shell.execute_reply.started":"2025-09-08T01:54:39.299133Z","shell.execute_reply":"2025-09-08T01:54:43.925452Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datasets import load_dataset, Dataset\nfrom kaggle_secrets import UserSecretsClient\nimport random\nimport pandas as pd\n\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    answer_similarity,\n    answer_correctness,\n)\nfrom langchain_openai import ChatOpenAI\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T01:54:43.928165Z","iopub.execute_input":"2025-09-08T01:54:43.928501Z","iopub.status.idle":"2025-09-08T01:54:43.934188Z","shell.execute_reply.started":"2025-09-08T01:54:43.928475Z","shell.execute_reply":"2025-09-08T01:54:43.933253Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face and OpenAI\n# We retrieve the tokens you stored in Kaggle Secrets.\n\n# Hugging Face Token for the generation model\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    hf_token = None\n\n# OpenAI API Key for the Ragas evaluation model\ntry:\n    openai_api_key = user_secrets.get_secret(\"OPENAI_API_KEY\")\n    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n    # print(\"OpenAI API Key configured successfully.\")\nexcept Exception as e:\n    print(\"Could not retrieve OpenAI API Key. Please ensure it is stored as a Kaggle secret named 'OPENAI_API_KEY'.\")\n    openai_api_key = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T01:54:43.934889Z","iopub.execute_input":"2025-09-08T01:54:43.935059Z","iopub.status.idle":"2025-09-08T01:54:44.137130Z","shell.execute_reply.started":"2025-09-08T01:54:43.935045Z","shell.execute_reply":"2025-09-08T01:54:44.136408Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\nmodel_id = \"alpha-ai/LLAMA3-3B-Medical-COT\"\ndataset_id = \"tmnam20/ViMedAQA\"\n\n# Step 4: Load the Dataset\n# We load the ViMedAQA dataset from Hugging Face.\n# This dataset contains Vietnamese medical questions and answers.\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(\"Dataset loaded successfully!\")\n    print(\"Example from the dataset:\")\n    print(dataset[0])\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    dataset = None\n\n# Step 5: Load the Model and Tokenizer\n# We will load the model in 4-bit precision (quantization) to save memory,\n# which is highly recommended for running larger models on Kaggle's GPUs.\nif hf_token:\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            token=hf_token,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            quantization_config=None  # You can add a BitsAndBytesConfig here for quantization if needed\n        )\n        print(\"Model and tokenizer loaded successfully!\")\n    except Exception as e:\n        print(f\"Failed to load the model or tokenizer. Error: {e}\")\n        model = None\n        tokenizer = None\nelse:\n    print(\"Hugging Face token not available. Cannot load the model.\")\n    model = None\n    tokenizer = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T01:54:44.138896Z","iopub.execute_input":"2025-09-08T01:54:44.139125Z","iopub.status.idle":"2025-09-08T01:54:50.259961Z","shell.execute_reply.started":"2025-09-08T01:54:44.139107Z","shell.execute_reply":"2025-09-08T01:54:50.259202Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully!\nExample from the dataset:\n{'question_idx': 'drug_6073', 'question': 'Biviantac có thể điều trị trướng bụng, đầy hơi không?', 'answer': 'Có, Biviantac có thể điều trị các tình trạng như trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.', 'context': 'Thuốc Biviantac được chỉ định để điều trị các trường hợp do tăng tiết acid quá mức như: - Khó tiêu, nóng rát hay đau vùng thượng vị.\\n- Trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.\\n- Tăng độ acid, đau rát dạ dày.\\n- Các rối loạn thường gặp trong những bệnh lý loét dạ dày tá tràng, thực quản.', 'title': 'Chỉ định của thuốc Biviantac', 'keyword': 'Biviantac', 'topic': 2, 'article_url': 'https://youmed.vn/tin-tuc/thuoc-biviantac-thuoc-dung-cho-cac-roi-loan-tieu-hoa/', 'author': 'Dược sĩ Trần Vân Thy', 'author_url': 'https://youmed.vn/tin-tuc/bac-si/duoc-si-tran-van-thy/'}\nModel and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Step 6: Set up a Text Generation Pipeline and Prepare for Evaluation\nif model and tokenizer:\n    # The pipeline simplifies the process of using the model for text generation.\n    text_generator = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n    )\n    print(\"Text generation pipeline is ready.\")\n\n    # Step 7: Select Samples, Format Prompts, and Collect Data for Ragas\n    if dataset:\n        num_samples_to_evaluate = 3 # Ragas can be slow, so let's start with a small, representative sample.\n\n        # Generate N random indices to select samples from the dataset\n        random_indices = random.sample(range(len(dataset)), num_samples_to_evaluate)\n        eval_dataset = dataset.select(random_indices)\n\n        prompts = []\n        # These lists will store the data needed for Ragas evaluation\n        ids_for_ragas = []\n        questions_for_ragas = []\n        contexts_for_ragas = []\n        ground_truths_for_ragas = []\n\n        for sample in eval_dataset:\n            question_id = sample['question_idx']\n            question = sample['question']\n            context = sample['context']\n            original_answer = sample['answer']\n\n            # Store data for Ragas\n            ids_for_ragas.append(question_id)\n            questions_for_ragas.append(question)\n            contexts_for_ragas.append([context]) # Ragas expects context to be a list of strings\n            ground_truths_for_ragas.append(original_answer)\n\n            # Format the prompt for the model\n            prompt_template = f\"\"\"\n            <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n            You are a helpful medical assistant. Based *only* on the context provided below, answer the question in Vietnamese.\n\n            Context: {context}\n\n            Question: {question}\n\n            <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n            \"\"\"\n            prompts.append(prompt_template)\n\n        print(f\"\\nPreparing to generate answers for {num_samples_to_evaluate} random samples...\")\n\n        # Step 8: Generate Answers for the entire batch\n        try:\n            generated_outputs_batch = text_generator(\n                prompts,\n                max_new_tokens=256,\n                do_sample=True,\n                temperature=0.01,\n                top_p=0.9,\n                eos_token_id=tokenizer.eos_token_id,\n                padding=True,\n                truncation=True\n            )\n\n            # Collect the generated answers for Ragas\n            generated_answers_for_ragas = []\n            answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n\n            print(\"\\n--- Model Generation Complete. Sample Outputs: ---\")\n            for i, output in enumerate(generated_outputs_batch):\n                generated_text = output[0]['generated_text']\n                clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                generated_answers_for_ragas.append(clean_answer)\n\n                # Print a few examples to see the model's performance\n                if i < 3: # Print first 3 examples\n                    print(f\"\\n--- Sample {i+1}/{num_samples_to_evaluate} ---\")\n                    print(f\"Sample ID: {ids_for_ragas[i]}\") \n                    print(f\"Question: {questions_for_ragas[i]}\")\n                    print(f\"Context: {contexts_for_ragas[i]}\")\n                    print(f\"Model Answer: {clean_answer}\")\n                    print(f\"Ground Truth: {ground_truths_for_ragas[i]}\")\n                    print(\"-\" * 50)\n\n        except Exception as e:\n            print(f\"An error occurred during text generation: {e}\")\nelse:\n    print(\"\\nSkipping text generation due to issues with model/tokenizer loading.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:05.869703Z","iopub.execute_input":"2025-09-08T02:10:05.870389Z","iopub.status.idle":"2025-09-08T02:10:08.822731Z","shell.execute_reply.started":"2025-09-08T02:10:05.870340Z","shell.execute_reply":"2025-09-08T02:10:08.821911Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Text generation pipeline is ready.\n\nPreparing to generate answers for 3 random samples...\n\n--- Model Generation Complete. Sample Outputs: ---\n\n--- Sample 1/3 ---\nSample ID: disease_8483\nQuestion: Khi nào thì thai to xảy ra nhiều nhất?\nContext: ['- Tiền sử sinh con to: Thai to sẽ có có nguy cơ hơn khi trước đây bạn đã từng có đứa con trước đó với cân nặng lúc sanh trên 4000 gram.\\n- Di truyền: Nếu bản thân cha mẹ trước đây cũng từng được chẩn đoán là con to lúc sanh, cũng gây nguy cơ sinh con to sau này.\\n- Giới tính: Bé trai thường nặng hơn so với bé gái. Hầu hết các em bé sinh ra với cân nặng lớn đều là bé trai.\\n- Tuổi mẹ: Khi mang thai trên 35 tuổi, có thể tăng khả năng sinh con to.\\n- Mang thai quá ngày (Thai già tháng): Thai trong bụng mẹ vượt quá ngày dự sanh nhiều ngày có thể dẫn đến thai to hơn. Tuy nhiên, điều này lại ít gặp trên thực tế.']\nModel Answer: Khi thai mẹ mang thai quá ngày (thai già tháng), thai to xảy ra nhiều nhất.\nGround Truth: Ít gặp trên thực tế.\n--------------------------------------------------\n\n--- Sample 2/3 ---\nSample ID: disease_13791\nQuestion: Viêm thực quản là tình trạng như thế nào?\nContext: ['Đây là tình trạng các lớp niêm mạc của thực quản bị tổn thương. Người bệnh sẽ có cảm giác đau khi nuốt, khó nuốt, hoặc cảm thấy đau tức ngực trên. Nguyên nhân của viêm thực quản bao gồm trào ngược dạ dày thực quản, nhiễm trùng, do tác dụng phụ của thuốc và dị ứng.']\nModel Answer: Viêm thực quản là tình trạng tổn thương của lớp niêm mạc ở phía trước của thực quản, gây ra các triệu chứng như đau khi nuốt, khó nuốt và đau ngực trên.\nGround Truth: Viêm thực quản là tình trạng các lớp niêm mạc của thực quản bị tổn thương.\n--------------------------------------------------\n\n--- Sample 3/3 ---\nSample ID: drug_8086\nQuestion: Thành phần chính của Chronol là gì?\nContext: ['Trong mỗi viên nén Chronol có các thành phần chính sau: - Disulfiram với lượng là 500 mg.\\n- Tá dược vừa đủ cho 1 viên.']\nModel Answer: Thành phần chính của Chronol là Disulfiram.\nGround Truth: Disulfiram, với hàm lượng 500 mg.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Debugging a single data point","metadata":{}},{"cell_type":"code","source":"# # Step 6 & 7: Isolate and Prepare a SINGLE Sample for Debugging\n\n# # ---------------------------------------------------------------------------------\n# # USER ACTION: CHANGE THIS ID to the specific sample you want to debug.\n# TARGET_SAMPLE_ID = \"body-part_1140\" \n# # ---------------------------------------------------------------------------------\n\n# if model and tokenizer and dataset:\n#     # Find the specific sample in the dataset\n#     target_sample = None\n#     for sample in dataset:\n#         if sample['question_idx'] == TARGET_SAMPLE_ID:\n#             target_sample = sample\n#             break\n            \n#     if target_sample:\n#         print(f\"Found sample with ID: {TARGET_SAMPLE_ID}\")\n        \n#         # Create a new mini-dataset containing only our target sample\n#         # Ragas and other functions expect a Dataset object, so we build one.\n#         eval_data_dict = {key: [value] for key, value in target_sample.items()}\n#         eval_dataset = Dataset.from_dict(eval_data_dict)\n        \n#         # --- The rest of the pipeline now runs on this single sample ---\n        \n#         text_generator = pipeline(\n#             \"text-generation\", model=model, tokenizer=tokenizer,\n#             torch_dtype=torch.bfloat16, device_map=\"auto\"\n#         )\n#         print(\"Text generation pipeline is ready.\")\n\n#         # Prepare prompts and data lists (now they will only have one item)\n#         prompts, ids_for_ragas, questions_for_ragas, contexts_for_ragas, ground_truths_for_ragas = [], [], [], [], []\n#         for sample in eval_dataset:\n#             ids_for_ragas.append(sample['question_idx'])\n#             questions_for_ragas.append(sample['question'])\n#             contexts_for_ragas.append([sample['context']])\n#             ground_truths_for_ragas.append(sample['answer'])\n#             prompt_template = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n#             Based on the context below, answer the question. \n            \n#             **Rules:**\n#             1. You MUST extract the answer directly from the context.\n#             2. The answer must be the exact, continuous text from the context.\n#             3. DO NOT add extra words or form a full sentence.\n            \n#             **Example:**\n#             - Context: \"Đến năm 1327, đây là thị trấn lớn thứ ba tại Warwickshire.\"\n#             - Question: \"Vào thế kỉ XIV, Birmingham trở thành thị trấn lớn thứ mấy tại Warwickshire?\"\n#             - Correct Answer: \"lớn thứ ba\"\n\n#             **Now, perform the task with the following:**\n            \n#             Context: {sample['context']}\n            \n#             Question: {sample['question']}\n\n#             <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n#             prompts.append(prompt_template)\n\n#         print(f\"\\nPreparing to generate an answer for sample {TARGET_SAMPLE_ID}...\")\n\n#         # Step 8: Generate the single answer\n#         try:\n#             generated_output = text_generator(\n#                 # prompts, max_new_tokens=256, do_sample=True, temperature=0.1, top_p=0.9,\n#                 prompts, max_new_tokens=256, do_sample=False, temperature=0.0, # Temperature 0.0 for deterministic extraction\n#                 eos_token_id=tokenizer.eos_token_id, padding=True, truncation=True\n#             )[0] # Get the first and only result\n\n#             generated_answers_for_ragas = []\n#             answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n#             clean_answer = generated_output[0]['generated_text'].split(answer_start_tag)[-1].strip()\n#             generated_answers_for_ragas.append(clean_answer)\n\n#             print(\"\\n--- Model Generation Complete ---\")\n#             print(f\"Sample ID: {ids_for_ragas[0]}\")\n#             print(f\"Question: {questions_for_ragas[0]}\")\n#             print(f\"Model Answer: {clean_answer}\")\n#             print(f\"Ground Truth: {ground_truths_for_ragas[0]}\")\n            \n#         except Exception as e:\n#             print(f\"An error occurred during text generation: {e}\")\n            \n#     else:\n#         print(f\"ERROR: Could not find any sample with ID '{TARGET_SAMPLE_ID}' in the dataset.\")\n\n# else:\n#     print(\"\\nSkipping generation due to issues with model, tokenizer, or dataset loading.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:08.831008Z","iopub.execute_input":"2025-09-08T02:10:08.831257Z","iopub.status.idle":"2025-09-08T02:10:08.836507Z","shell.execute_reply.started":"2025-09-08T02:10:08.831239Z","shell.execute_reply":"2025-09-08T02:10:08.835925Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Step 9: Prepare the Dataset for Ragas Evaluation\n# Ragas expects a Hugging Face Dataset object with specific column names:\n# - question: The question asked.\n# - contexts: A list of context strings.\n# - answer: The answer generated by the model.\n# - ground_truth: The reference answer from the original dataset.\n\nif 'generated_answers_for_ragas' in locals():\n    # Create a dictionary with the collected data\n    ragas_data = {\n        \"question\": questions_for_ragas,\n        \"contexts\": contexts_for_ragas,\n        \"answer\": generated_answers_for_ragas,\n        \"ground_truth\": ground_truths_for_ragas\n    }\n\n    # Convert the dictionary to a Hugging Face Dataset\n    ragas_dataset = Dataset.from_dict(ragas_data)\n\n    print(\"Dataset prepared for Ragas evaluation.\")\n    print(ragas_dataset)\n\nelse:\n    print(\"Could not find generated answers. Skipping Ragas evaluation.\")\n    ragas_dataset = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:08.837241Z","iopub.execute_input":"2025-09-08T02:10:08.837559Z","iopub.status.idle":"2025-09-08T02:10:08.867690Z","shell.execute_reply.started":"2025-09-08T02:10:08.837529Z","shell.execute_reply":"2025-09-08T02:10:08.867065Z"}},"outputs":[{"name":"stdout","text":"Dataset prepared for Ragas evaluation.\nDataset({\n    features: ['question', 'contexts', 'answer', 'ground_truth'],\n    num_rows: 3\n})\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Step 10: Run Ragas with a Robust, Per-Sample, Sequential Evaluation\n\n# Disable debug mode for a cleaner output\nimport langchain\nlangchain.debug = False\n\n# Import Python's built-in warnings module\nimport warnings\nimport pandas as pd\nfrom tqdm.auto import tqdm # Import tqdm for a progress bar\n\nif 'generated_answers_for_ragas' in locals() and openai_api_key:\n    # Prepare the dataset for Ragas\n    ragas_data = {\n        \"question\": questions_for_ragas,\n        \"contexts\": contexts_for_ragas,\n        \"answer\": generated_answers_for_ragas,\n        \"ground_truth\": ground_truths_for_ragas\n    }\n    ragas_dataset = Dataset.from_dict(ragas_data)\n    \n    print(\"\\nStarting robust, per-sample Ragas evaluation...\")\n    print(\"=\"*50)\n\n    # Configure the judge LLM\n    evaluation_llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n\n    # Define the metrics we want to compute\n    metrics_to_run = [\n        faithfulness,\n        answer_relevancy,\n        answer_similarity,\n        answer_correctness,\n    ]\n    \n    # --- This is the robust loop ---\n    all_sample_results = []\n    # Loop 1: Iterate through each sample in the dataset\n    for sample in tqdm(ragas_dataset, desc=\"Evaluating Samples\"):\n        # Create a mini-dataset with just the current sample\n        single_sample_dataset = Dataset.from_dict({k: [v] for k, v in sample.items()})\n        \n        # Dictionary to store all scores for the current sample\n        sample_scores = {\"question\": sample[\"question\"]}\n        \n        # Loop 2: Evaluate each metric sequentially for the current sample\n        for metric in metrics_to_run:\n            metric_name = metric.name\n            try:\n                # Run evaluation for only ONE metric on the ONE sample\n                result = evaluate(\n                    dataset=single_sample_dataset,\n                    metrics=[metric],\n                    llm=evaluation_llm\n                )\n                # Store the successful score\n                sample_scores[metric_name] = result[metric_name]\n            except Exception as e:\n                # If a metric fails, record it as NaN and continue\n                print(f\"  WARNING: Metric '{metric_name}' failed for question '{sample['question'][:50]}...'. Recording as NaN. Error: {e}\")\n                sample_scores[metric_name] = float('nan') # Explicitly set NaN on failure\n            \n        all_sample_results.append(sample_scores)\n\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Ragas per-sample evaluation complete!\")\n    print(\"=\"*50 + \"\\n\")\n\n    # --- Display the results in a clean DataFrame ---\n    # This will now work correctly because all_sample_results is a list of dictionaries,\n    # and we have handled any potential failures by explicitly setting NaN.\n    results_df = pd.DataFrame(all_sample_results)\n    \n    # Reorder columns for better readability\n    column_order = ['question', 'faithfulness', 'answer_relevancy', 'answer_similarity', 'answer_correctness']\n    # Ensure all expected columns exist, adding them with NaN if they are missing\n    for col in column_order:\n        if col not in results_df.columns:\n            results_df[col] = float('nan')\n            \n    results_df = results_df[column_order]\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        display(results_df)\n\n    # --- Optional: Print the final average scores ---\n    print(\"\\n--- Average Ragas Scores ---\")\n    average_scores = results_df.mean(numeric_only=True)\n    print(average_scores)\n\n\nelse:\n    print(\"Skipping Ragas evaluation. Check if generation was successful and if the OpenAI API Key is configured.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:35:43.170424Z","iopub.execute_input":"2025-09-08T02:35:43.170754Z","iopub.status.idle":"2025-09-08T02:36:15.268208Z","shell.execute_reply.started":"2025-09-08T02:35:43.170731Z","shell.execute_reply":"2025-09-08T02:36:15.267149Z"}},"outputs":[{"name":"stdout","text":"\nStarting robust, per-sample Ragas evaluation...\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating Samples:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afff82252b1d495fac619038218858a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b688da92f60c48d4a1f298b24a3706e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b209f1a229264c7cbb517df0a4734e1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f9e5f2ee7f44d09db55ada582a61cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"524ce8e7198149b4a91575e184199052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9c1390d8ef45cfb3bc1e6b6d134552"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbaa24016f3a4ebab5d4734f36f382fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf45f3ff6044a15a36eed8bf2d81013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ee836bd68d34d7b892b996ddf01bb32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac0082618a2b4b39b512c5a9efb12d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"488b1cb0d6524796af45b6067fb714c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f05e2de3e5c9413cacb098a13b532c96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5aca3e6d674452bf6e04a1e2b1d97b"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nRagas per-sample evaluation complete!\n==================================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                    question faithfulness  \\\n0     Khi nào thì thai to xảy ra nhiều nhất?        [0.0]   \n1  Viêm thực quản là tình trạng như thế nào?       [0.75]   \n2        Thành phần chính của Chronol là gì?        [1.0]   \n\n       answer_relevancy     answer_similarity     answer_correctness  \n0  [0.7350011575223752]  [0.7898303282982233]  [0.19745758207455583]  \n1    [0.93895725709794]   [0.957778699510968]   [0.5394446748777421]  \n2  [0.8857837882408912]  [0.8852616106426497]  [0.22131540266066244]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>faithfulness</th>\n      <th>answer_relevancy</th>\n      <th>answer_similarity</th>\n      <th>answer_correctness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Khi nào thì thai to xảy ra nhiều nhất?</td>\n      <td>[0.0]</td>\n      <td>[0.7350011575223752]</td>\n      <td>[0.7898303282982233]</td>\n      <td>[0.19745758207455583]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Viêm thực quản là tình trạng như thế nào?</td>\n      <td>[0.75]</td>\n      <td>[0.93895725709794]</td>\n      <td>[0.957778699510968]</td>\n      <td>[0.5394446748777421]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Thành phần chính của Chronol là gì?</td>\n      <td>[1.0]</td>\n      <td>[0.8857837882408912]</td>\n      <td>[0.8852616106426497]</td>\n      <td>[0.22131540266066244]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n--- Average Ragas Scores ---\nSeries([], dtype: float64)\n","output_type":"stream"}],"execution_count":29}]}