{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:53.120253Z","iopub.execute_input":"2025-09-17T17:52:53.120962Z","iopub.status.idle":"2025-09-17T17:52:56.700308Z","shell.execute_reply.started":"2025-09-17T17:52:53.120919Z","shell.execute_reply":"2025-09-17T17:52:56.699233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate\nimport warnings\nimport time\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import logging\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:56.702362Z","iopub.execute_input":"2025-09-17T17:52:56.703016Z","iopub.status.idle":"2025-09-17T17:52:56.710701Z","shell.execute_reply.started":"2025-09-17T17:52:56.702990Z","shell.execute_reply":"2025-09-17T17:52:56.709895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:56.711446Z","iopub.execute_input":"2025-09-17T17:52:56.711653Z","iopub.status.idle":"2025-09-17T17:52:56.795268Z","shell.execute_reply.started":"2025-09-17T17:52:56.711639Z","shell.execute_reply":"2025-09-17T17:52:56.794591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\ndataset_id = \"tmnam20/ViMedAQA\"\n\nEVAL_FULL_DATASET = False\n\nseed_num = 7\nNUM_SAMPLES_INITIAL = 1\n\n# --- ADDED: Boolean toggle for the second randomization ---\n# Set to True to get the final 50 samples.\n# Set to False to use the initial 200 samples.\nENABLE_SUBSET_SAMPLING = False\nNUM_SAMPLES_FINAL = 50\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"test\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    if EVAL_FULL_DATASET:\n        eval_dataset = dataset\n    else:\n        # --- First Sampling Step: Always get the initial 200 samples ---\n        random.seed(seed_num) # for reproducibility\n        initial_random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_INITIAL)\n        initial_eval_dataset = dataset.select(initial_random_indices)\n    \n        print(f\"Created an initial random evaluation set with {len(initial_eval_dataset)} samples.\")\n    \n        # --- ADDED: Conditional second randomization ---\n        if ENABLE_SUBSET_SAMPLING:\n            print(\"Subset sampling is ENABLED. Performing second randomization...\")\n            # Re-seed to ensure this step is also reproducible\n            random.seed(seed_num)\n            final_random_indices = random.sample(range(len(initial_eval_dataset)), NUM_SAMPLES_FINAL)\n            # Final dataset is the smaller, 50-sample subset\n            eval_dataset = initial_eval_dataset.select(final_random_indices)\n            print(f\"Further randomized and reduced the set to a final size of {len(eval_dataset)} samples.\")\n        else:\n            print(\"Subset sampling is DISABLED.\")\n            # Final dataset is the larger, 200-sample set\n            eval_dataset = initial_eval_dataset\n            print(f\"Using the initial set of {len(eval_dataset)} samples for evaluation.\")\n\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:56.796060Z","iopub.execute_input":"2025-09-17T17:52:56.796240Z","iopub.status.idle":"2025-09-17T17:52:57.688989Z","shell.execute_reply.started":"2025-09-17T17:52:56.796227Z","shell.execute_reply":"2025-09-17T17:52:57.688214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT_STRATEGIES = {\n    # \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\n    # \"Concise_VI\": (\n    #     f\"Dựa CHỈ vào văn bản trong phần Ngữ cảnh dưới đây, hãy trả lời cho Câu hỏi. \"\n    #     f\"Câu trả lời của bạn phải ngắn gọn, đi thẳng vào vấn đề và không chứa bất kỳ thông tin nào không có trong văn bản. \"\n    #     f\"Không giải thích thêm.\"\n    # ),\n    # \"Strict_Rules_VI\": (\n    #     f\"TỪ Ngữ cảnh, TRÍCH XUẤT câu trả lời cho Câu hỏi.\\n\\n\"\n    #     f\"**QUY TẮC:**\\n\"\n    #     f\"1. CHỈ sử dụng thông tin từ Ngữ cảnh.\\n\"\n    #     f\"2. KHÔNG giải thích các bước của bạn.\\n\"\n    #     f\"3. KHÔNG tự suy luận hoặc thêm bất kỳ thông tin bên ngoài nào.\\n\"\n    #     f\"4. Cung cấp câu trả lời được trích xuất trực tiếp.\\n\\n\"\n    #     f\"Dưới đây là Ngữ cảnh và Câu hỏi, hãy đưa câu trả lời TRÍCH XUẤT:\"\n    # ),\n\n    # --- ENGLISH PROMPTS ---\n    \"Extract_EN\": \"Based on the following Context, extract the direct answer from the text without any additional explanation.\",\n    # \"Concise_EN\": (\n    #     f\"Based ONLY on the text in the Context section below, answer the Question. \"\n    #     f\"Your answer must be concise, to the point, and contain no information not present in the text. \"\n    #     f\"Do not add any explanation.\"\n    # ),\n    # \"Strict_Rules_EN\": (\n    #     f\"From the Context, EXTRACT the answer to the Question.\\n\\n\"\n    #     f\"**RULES:**\\n\"\n    #     f\"1. ONLY use information from the Context.\\n\"\n    #     f\"2. DO NOT explain your steps.\\n\"\n    #     f\"3. DO NOT infer or add any external information.\\n\"\n    #     f\"4. Provide the directly extracted answer.\\n\\n\"\n    #     f\"Below is the Context and the Question, provide the EXTRACTED answer:\"\n    # ),\n\n    # \"Direct_VI\": \"Sử dụng Ngữ cảnh sau để trả lời Câu hỏi.\",\n    # \"Direct_EN\": \"Use the following Context to answer the Question.\",\n\n    # \"List_VI\": (\n    #     f\"Từ Ngữ cảnh được cung cấp, hãy **liệt kê tất cả** các thông tin dùng để trả lời cho Câu hỏi.\"\n    #     f\"Trình bày câu trả lời một cách ngắn gọn, chỉ bao gồm các điểm được tìm thấy.\"\n    # ),\n    # \"List_EN\": (\n    #     f\"From the provided Context, **list all** the information that answers the Question. \"\n    #     f\"Present the answer concisely, including only the points found.\"\n    # ),\n}\n\ndefault_instruction_format_vi = \"{base_instruction}\\n\\n### Ngữ cảnh:\\n{context}\\n\\n### Câu hỏi:\\n{question}\"\ndefault_instruction_format_en = \"{base_instruction}\\n\\n### Context:\\n{context}\\n\\n### Question:\\n{question}\"\n\nSYSTEM_PROMPTS = [\n    # (\n    #     \"Expert_SP_VI\",\n    #     \"Bạn là một AI chuyên gia y tế. Dựa trên kiến thức chuyên môn của mình, hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    #     default_instruction_format_vi,\n    # ),\n    (\n        \"Expert_SP_EN\",\n        \"You are a medical expert AI. Based on your expertise, answer the following Question in Vietnamese, using ONLY the provided Context.\",\n        default_instruction_format_en,\n    ),\n\n    # (\n    #     \"Default_SP_VI\",\n    #     \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    #     default_instruction_format_vi,\n    # ),\n    # (\n    #     \"Default_SP_EN\",\n    #     \"You are a helpful medical assistant. Answer the following Question in Vietnamese, based ONLY on the provided Context.\",\n    #     default_instruction_format_en,\n    # ),\n\n    # (\n    #     \"Empty_SP_VI\",\n    #     \"\",\n    #     default_instruction_format_vi,\n    # ),\n    # (\n    #     \"Empty_SP_EN\",\n    #     \"\",\n    #     default_instruction_format_en,\n    # ),\n]\n\n# Mode 1: Run all strategies defined in PROMPT_STRATEGIES (False)\n# Mode 2: Run only the single, specified strategy for a targeted comparison (True)\nUSE_BEST_PROMPT_ONLY = False\nBEST_STRATEGY_NAME = \"Extract_VI\" # Specify the prompt to use in Mode 2\nPRINT_PROMPT = False\n\nmodel_ids = [\n    \"arcee-ai/Arcee-VyLinh\",\n    # \"vilm/vietcuna-3b-v2\",\n    # \"alpha-ai/LLAMA3-3B-Medical-COT\",\n    \n    # \"vilm/vinallama-2.7b-chat\",\n    # \"sail/Sailor-4B\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:57.691397Z","iopub.execute_input":"2025-09-17T17:52:57.691986Z","iopub.status.idle":"2025-09-17T17:52:57.698922Z","shell.execute_reply.started":"2025-09-17T17:52:57.691966Z","shell.execute_reply":"2025-09-17T17:52:57.698102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\ngeneration_times = {}\n\ndef create_prompt_and_get_config(model_id, tokenizer, system_prompt, full_instruction_text):\n    \"\"\"\n    A single, unified function to create a model-specific prompt and return\n    the associated answer start tag for parsing.\n\n    Returns:\n        tuple: (formatted_prompt_string, answer_start_tag_string)\n    \"\"\"  \n    # Models WITHOUT a dedicated System Prompt\n    # For these, we combine the system and user prompts into a single instruction.\n    if model_id in [\"vilm/vietcuna-3b-v2\", \"sail/Sailor-4B\"]:\n        # If a system prompt is provided, prepend it.\n        if system_prompt:\n            combined_instruction = f\"{system_prompt}\\n\\n{full_instruction_text}\"\n        # Otherwise, just use the user instruction directly.\n        else:\n            combined_instruction = full_instruction_text\n\n        if model_id == \"vilm/vietcuna-3b-v2\":\n            prompt = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {combined_instruction}\\nASSISTANT:\"\n            answer_start_tag = \"ASSISTANT:\"\n            return prompt, answer_start_tag\n\n        # https://huggingface.co/sail/Sailor-4B\n        if model_id == \"sail/Sailor-4B\":\n            prompt = f\"{combined_instruction}\\n\\nCâu trả lời:\"\n            answer_start_tag = \"\"\n            return prompt, answer_start_tag\n\n    # For chat models, we build the message list programmatically.\n    messages = []\n    # Only add the system role if the system_prompt is not empty.\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    # Add the user/question role\n    if model_id == \"sail/Sailor-4B-Chat\":\n        messages.append({\"role\": \"question\", \"content\": full_instruction_text})\n    else:\n        # Default to \"user\" role for all other chat models\n        messages.append({\"role\": \"user\", \"content\": full_instruction_text})\n    \n    if model_id == \"arcee-ai/Arcee-VyLinh\":\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        answer_start_tag = \"<|im_start|>assistant\"\n        return prompt, answer_start_tag\n\n    if model_id == \"alpha-ai/LLAMA3-3B-Medical-COT\":\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n        return prompt, answer_start_tag\n    \n    # https://huggingface.co/sail/Sailor-4B-Chat\n    if model_id == \"sail/Sailor-4B-Chat\":\n        prompt = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        answer_start_tag = \"answer:\"\n        return prompt, answer_start_tag\n\n    if \"vilm/vinallama-2.7b\" in model_id:\n        # This model's template is custom and doesn't use apply_chat_template,\n        # so we handle it separately.\n        if system_prompt:\n            prompt = (\n                f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n                f\"<|im_start|>user\\n{full_instruction_text}<|im_end|>\\n\"\n                f\"<|im_start|>assistant\"\n            )\n        else:\n            # Version without a system prompt\n            prompt = (\n                f\"<|im_start|>user\\n{full_instruction_text}<|im_end|>\\n\"\n                f\"<|im_start|>assistant\"\n            )\n        answer_start_tag = \"<|im_start|>assistant\"\n        return prompt, answer_start_tag\n\n    # Nothing matches\n    return \"\", \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:57.699809Z","iopub.execute_input":"2025-09-17T17:52:57.700017Z","iopub.status.idle":"2025-09-17T17:52:57.720228Z","shell.execute_reply.started":"2025-09-17T17:52:57.700001Z","shell.execute_reply":"2025-09-17T17:52:57.719605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\nintermediate_results = {} \n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n\n    wide_results = []\n    for i, sample in enumerate(eval_dataset):\n        wide_results.append({\n            \"Sample_ID\": i,\n            \"Question\": sample['question'],\n            \"Context\": sample['context'],\n            \"Ground_Truth_Answer\": ground_truth_answers[i][0]\n        })\n    \n    # Loop through each model to generate answers\n    for model_id in model_ids:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id}\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config, # <-- PASS THE CONFIG OBJECT HERE\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            experiments_to_run = []\n            prompt_strategies_to_run = list(PROMPT_STRATEGIES.keys()) if not USE_BEST_PROMPT_ONLY else [BEST_STRATEGY_NAME]\n\n            for sp_name, system_prompt, instruction_format in SYSTEM_PROMPTS:\n                # If the format is a template, create a combination for each task instruction\n                if \"{base_instruction}\" in instruction_format:\n                    for prompt_name in prompt_strategies_to_run:\n                        # Only pair English prompts with English strategies, and VI with VI\n                        if (sp_name.endswith(\"_EN\") and prompt_name.endswith(\"_EN\")) or \\\n                           (sp_name.endswith(\"_VI\") and prompt_name.endswith(\"_VI\")):\n                            experiments_to_run.append((prompt_name, sp_name, system_prompt, instruction_format))\n                # If it's self-contained, add it just once\n                else:\n                    experiments_to_run.append((\"Self-Contained\", sp_name, system_prompt, instruction_format))\n\n                \n            for prompt_name, sp_name, system_prompt, instruction_format in experiments_to_run:\n                print(f\"\\n===== Running Experiment: [Task: {prompt_name}] | [System Prompt: {sp_name}] =====\")\n                # Fetch the base instruction, providing an empty string for the \"Self-Contained\" case\n                base_instruction = PROMPT_STRATEGIES.get(prompt_name, \"\")\n                result_key_tuple = (model_id, prompt_name, sp_name)\n                prompts = []\n                answer_start_tag = \"\"\n            \n                for sample in eval_dataset:\n                    # If the base instruction is empty (our special case), use a cleaner format.\n                    if not base_instruction:\n                        if sp_name.endswith(\"_EN\"):\n                            full_instruction_text = (\n                                f\"### Context:\\n{sample['context']}\\n\\n\"\n                                f\"### Question:\\n{sample['question']}\"\n                            )\n                        else: # Default to Vietnamese\n                            full_instruction_text = (\n                                f\"### Ngữ cảnh:\\n{sample['context']}\\n\\n\"\n                                f\"### Câu hỏi:\\n{sample['question']}\"\n                            )\n                    # Otherwise, use the standard template format.\n                    else:\n                        full_instruction_text = instruction_format.format(\n                            base_instruction=base_instruction, \n                            context=sample['context'], \n                            question=sample['question']\n                        )\n                    \n                    prompt, tag = create_prompt_and_get_config(model_id, tokenizer, system_prompt, full_instruction_text)\n                    if PRINT_PROMPT:\n                        print(prompt)\n                    \n                    prompts.append(prompt)\n                    if not answer_start_tag: answer_start_tag = tag\n            \n                if prompts == \"\" and answer_start_tag == \"\":\n                    print(\"No model matches\")\n                    break\n\n                start_time = time.time()\n\n                # print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n                # Generate answers for the entire batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=False,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n                end_time = time.time()\n                generation_time = end_time - start_time\n                generation_times[result_key_tuple] = generation_time # Lưu thời gian đã đo\n    \n                # Extract the clean answers\n                model_answers = []\n                \n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0]['generated_text']\n                    # Check if the tag is not empty AND exists in the text before splitting\n                    if answer_start_tag and answer_start_tag in generated_text:\n                        clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                    else:\n                        # Fallback for base models or if the tag isn't found\n                        clean_answer = generated_text.replace(prompts[i], \"\").strip()\n\n                    if prompt_name == \"Extract_EN\":\n                        redundant_prefix = \"Direct answer:\\n\"\n                        if clean_answer.startswith(redundant_prefix):\n                            clean_answer = clean_answer[len(redundant_prefix):].strip()\n                    \n                    model_answers.append(clean_answer)\n    \n                all_generated_answers[result_key_tuple] = model_answers\n                \n                answer_column_name = f\"{prompt_name}\"\n                for i, answer in enumerate(model_answers):\n                    # The key identifies a unique row in the final table\n                    row_key = (i, model_id, sp_name)\n                    \n                    # If this is the first time we see this row, initialize it with static info\n                    if row_key not in intermediate_results:\n                        intermediate_results[row_key] = {\n                            \"Sample_ID\": i,\n                            \"Question\": eval_dataset[i]['question'],\n                            \"Context\": eval_dataset[i]['context'],\n                            \"Ground_Truth_Answer\": ground_truth_answers[i][0],\n                            \"Model\": model_id,\n                            \"System_Prompt\": sp_name\n                        }\n                    \n                    # Add the generated answer to the correct column for this row\n                    intermediate_results[row_key][answer_column_name] = answer\n\n                print(f\"Time for generating answer: {generation_time:.2f} seconds.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Check if variables were successfully created before deleting\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n\n            import gc\n            gc.collect()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:52:57.721093Z","iopub.execute_input":"2025-09-17T17:52:57.721305Z","iopub.status.idle":"2025-09-17T17:53:12.023095Z","shell.execute_reply.started":"2025-09-17T17:52:57.721280Z","shell.execute_reply":"2025-09-17T17:53:12.022512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 5.5 - Assemble and Save Results in Hybrid-Format CSV File ---\nif intermediate_results:\n    print(\"\\n\" + \"=\"*50)\n    print(\"Assembling results into the desired hybrid format...\")\n    print(\"=\"*50)\n    \n    final_results_list = list(intermediate_results.values())\n    results_df_hybrid = pd.DataFrame(final_results_list)\n\n    # --- Define a clear column order for the final CSV ---\n    static_columns = [\n        \"Sample_ID\", \"Model\", \"System_Prompt\", \n        \"Question\", \"Context\", \"Ground_Truth_Answer\"\n    ]\n    \n    # Dynamically create the answer column names based on the strategies that were run\n    prompt_strategies_to_run = list(PROMPT_STRATEGIES.keys()) if not USE_BEST_PROMPT_ONLY else [BEST_STRATEGY_NAME]\n    answer_columns = sorted([f\"{p_name}\" for p_name in prompt_strategies_to_run])\n    \n    # Add the special \"Self-Contained\" column if it exists in the dataframe\n    if \"Self-Contained\" in results_df_hybrid.columns:\n        answer_columns.append(\"Self-Contained\")\n    \n    # Combine and reorder the DataFrame, handling missing columns gracefully\n    final_column_order = static_columns + answer_columns\n    # Ensure we only try to order by columns that actually exist in the dataframe\n    existing_columns_in_order = [col for col in final_column_order if col in results_df_hybrid.columns]\n    results_df_hybrid = results_df_hybrid[existing_columns_in_order]\n\n    results_df_hybrid = results_df_hybrid.sort_values(by=[\"Sample_ID\", \"Model\", \"System_Prompt\"]).reset_index(drop=True)\n\n    output_file_path = \"/kaggle/working/results.csv\"\n    results_df_hybrid.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n    \n    print(f\"Complete! Saved {len(results_df_hybrid)} rows to the file:\")\n    print(output_file_path)\n    \n    display(results_df_hybrid.head())\n    \nelse:\n    print(\"\\nNo results were generated to save.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:53:12.029253Z","iopub.execute_input":"2025-09-17T17:53:12.029529Z","iopub.status.idle":"2025-09-17T17:53:12.046079Z","shell.execute_reply.started":"2025-09-17T17:53:12.029502Z","shell.execute_reply":"2025-09-17T17:53:12.045456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\n\n# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    # Load all the metrics we need\n    rouge_metric = evaluate.load('rouge')\n    bleu_metric = evaluate.load('bleu')\n    meteor_metric = evaluate.load('meteor')\n    bertscore_metric = evaluate.load('bertscore')\n\n    evaluation_results = []\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        model_name, prompt_name, sp_name = result_key\n        # Check for empty predictions to prevent ZeroDivisionError in BLEU ---\n        # The `any()` function returns False if all strings in the list are empty.\n        if not any(predictions):\n            print(f\"  WARNING: Model & Prompt Strategy '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & Prompt Strategy\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0,\n                \"Avg-Score\": 0.0\n            }\n            evaluation_results.append(result_row)\n            # Use `continue` to skip the rest of the loop and move to the next model\n            continue\n    \n        # If predictions are valid, compute metrics as normal\n        rouge_scores = rouge_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bleu_scores = bleu_metric.compute(predictions=predictions, references=ground_truth_answers)\n        meteor_scores = meteor_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bertscore_scores = bertscore_metric.compute(predictions=predictions, references=ground_truth_answers, lang=\"vi\")\n        \n        # Calculate individual scores for the current result_key\n        rouge_l = round(rouge_scores['rougeL'], 4)\n        bleu = round(bleu_scores['bleu'], 4)\n        meteor = round(meteor_scores['meteor'], 4)\n        bertscore_f1 = round(sum(bertscore_scores['f1']) / len(bertscore_scores['f1']), 4)\n\n        # Calculate the average score\n        avg_score = round((rouge_l + bleu + meteor + bertscore_f1) / 4, 4)\n    \n        # Store results (this part is the same as before)\n        result_row = {\n            \"Model\": model_name,\n            \"Prompt_Strategy\": prompt_name,\n            \"System_Prompt\": sp_name,\n            \"ROUGE-L\": rouge_l,\n            \"BLEU\": bleu,\n            \"METEOR\": meteor,\n            \"BERTScore-F1\": bertscore_f1,\n            \"Avg-Score\": avg_score,\n            \"Generation Time (s)\": round(generation_times.get(result_key, 0), 2),\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    # Sort for better comparison\n    results_df = results_df.sort_values(by=\"Avg-Score\", ascending=False).reset_index(drop=True)\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    display(results_df)\n\n    # Save the evaluation results DataFrame to a separate CSV file\n    evaluation_output_path = \"/kaggle/working/eval_results.csv\"\n    results_df.to_csv(evaluation_output_path, index=False, encoding='utf-8-sig')\n    print(f\"\\nEvaluation results successfully saved to: {evaluation_output_path}\")\n\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:53:12.046786Z","iopub.execute_input":"2025-09-17T17:53:12.047016Z","iopub.status.idle":"2025-09-17T17:53:14.756350Z","shell.execute_reply.started":"2025-09-17T17:53:12.046992Z","shell.execute_reply":"2025-09-17T17:53:14.755630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"    # \"Few_Shot_VI\": (\n    #     f\"Dựa vào các Ví dụ sau đây, hãy trả lời Câu hỏi cuối cùng bằng cách trích xuất thông tin từ Ngữ cảnh được cung cấp.\\n\\n\"\n    #     f\"--- Ví dụ 1 ---\\n\"\n    #     f\"Ngữ cảnh: Thuốc Biviantac được chỉ định để điều trị các trường hợp do tăng tiết acid quá mức như: - Khó tiêu, nóng rát hay đau vùng thượng vị. - Trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua. - Tăng độ acid, đau rát dạ dày. - Các rối loạn thường gặp trong những bệnh lý loét dạ dày tá tràng, thực quản.\\n\"\n    #     f\"Câu hỏi: Biviantac có thể điều trị trướng bụng, đầy hơi không?\\n\"\n    #     f\"Câu trả lời: Có, Biviantac có thể điều trị các tình trạng như trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.\\n\\n\"\n    #     f\"--- Ví dụ 2 ---\\n\"\n    #     f\"Ngữ cảnh: Thuốc Atorvastatin T.V Pharm được dùng đường uống.\\n\"\n    #     f\"Câu hỏi: Tổng hợp các cách dùng hiệu quả để quản lý Atorvastatin T.V Pharm?\\n\"\n    #     f\"Câu trả lời: Các cách thức dùng thuốc Atorvastatin T.V Pharm hiệu quả là sử dụng đường uống.\\n\\n\"\n    #     f\"--- Ví dụ 3 ---\\n\"\n    #     f\"Ngữ cảnh: - Buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột. - Mệt mỏi. - Ban, mày đay. - Thiếu máu tan huyết. - Yếu cơ. - Khó thở, sốc phản vệ.\\n\"\n    #     f\"Câu hỏi: Các tác dụng phụ thường gặp của thuốc Aspirin 81 là gì?\\n\"\n    #     f\"Câu trả lời: Các tác dụng phụ thường gặp của thuốc Aspirin 81 bao gồm buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột.\\n\\n\"\n    #     f\"--- Ví dụ 4 ---\\n\"\n    #     f\"Ngữ cảnh: Các chị em có thể thỉnh thoảng thấy kinh nguyệt ra nhiều hoặc ra máu giữa các kỳ kinh (chảy máu giữa kỳ kinh nguyệt).\\n\"\n    #     f\"Câu hỏi: Các chị em có thể gặp tình trạng rong kinh không?\\n\"\n    #     f\"Câu trả lời: Có.\\n\\n\"\n    #     f\"--- Bây giờ, hãy trả lời Câu hỏi sau dựa trên Ngữ cảnh của nó---\"\n    # ),\n    # \"Full_VI\": (\n    #     f\"Dựa vào Ngữ cảnh sau, hãy trích xuất câu trả lời **đầy đủ và toàn diện nhất** có thể từ văn bản.\"\n    #     f\"Đảm bảo rằng bạn đã bao gồm **tất cả** các điểm có liên quan để trả lời cho câu hỏi.\"\n    # ),\n\n    # \"Full_EN\": (\n    #     f\"Based on the following Context, extract the most **complete and comprehensive** answer possible from the text. \"\n    #     f\"Ensure you have included **all** relevant points to answer the question.\"\n    # ),\n\n    # \"ViMedAQA\": \"\",\n\n    # (\n    #     \"ViMedAQA_SP_VI\",\n    #     \"Dựa vào ngữ cảnh sau và kiến thức của bạn, trả lời câu hỏi sau bằng tiếng Việt\",\n    #     \"### Ngữ cảnh:\\n{context}\\n\\n### Câu hỏi:\\n{question}\",\n    # ),\n    # (\n    #     \"ViMedAQA_SP_EN\",\n    #     \"Based on the following context and your knowledge, answer the following question in Vietnamese.\",\n    #     \"### Context:\\n{context}\\n\\n### Question:\\n{question}\",\n    # ),","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-17T17:53:14.757227Z","iopub.execute_input":"2025-09-17T17:53:14.757479Z","iopub.status.idle":"2025-09-17T17:53:14.763371Z","shell.execute_reply.started":"2025-09-17T17:53:14.757461Z","shell.execute_reply":"2025-09-17T17:53:14.762246Z"}},"outputs":[],"execution_count":null}]}