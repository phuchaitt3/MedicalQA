{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:45:28.245549Z","iopub.execute_input":"2025-09-13T16:45:28.245837Z","iopub.status.idle":"2025-09-13T16:45:32.254458Z","shell.execute_reply.started":"2025-09-13T16:45:28.245815Z","shell.execute_reply":"2025-09-13T16:45:32.253620Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate # Hugging Face's library for NLP evaluation\nimport warnings\nimport time\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:45:32.255994Z","iopub.execute_input":"2025-09-13T16:45:32.256237Z","iopub.status.idle":"2025-09-13T16:45:32.261576Z","shell.execute_reply.started":"2025-09-13T16:45:32.256215Z","shell.execute_reply":"2025-09-13T16:45:32.260896Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:45:32.262359Z","iopub.execute_input":"2025-09-13T16:45:32.262693Z","iopub.status.idle":"2025-09-13T16:45:32.356370Z","shell.execute_reply.started":"2025-09-13T16:45:32.262666Z","shell.execute_reply":"2025-09-13T16:45:32.355674Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\ndataset_id = \"tmnam20/ViMedAQA\"\nNUM_SAMPLES_TO_EVALUATE = 1\n# Prefer chat variants that ship with proper chat templates\nmodel_ids = [\n    \"sail/Sailor-4B-Chat\",         \n    \"vilm/vinallama-2.7b-chat\"     \n]\n\n# In case upstream code still passes the older/base IDs, map them here:\nMODEL_ID_ALIAS = {\n    \"sail/Sailor-4B\": \"sail/Sailor-4B-Chat\",\n    \"vilm/vinallama-2.7b\": \"vilm/vinallama-2.7b-chat\",\n}\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    # Create a small, random, representative sample for evaluation\n    random.seed(42) # for reproducibility\n    random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_TO_EVALUATE)\n    eval_dataset = dataset.select(random_indices)\n\n    print(f\"Created a random evaluation set with {len(eval_dataset)} samples.\")\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:45:32.357773Z","iopub.execute_input":"2025-09-13T16:45:32.357980Z","iopub.status.idle":"2025-09-13T16:45:33.260563Z","shell.execute_reply.started":"2025-09-13T16:45:32.357964Z","shell.execute_reply":"2025-09-13T16:45:33.259904Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully! Total samples: 39881\nCreated a random evaluation set with 1 samples.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\n# Mode 1: Run all strategies defined in PROMPT_STRATEGIES (False)\n# Mode 2: Run only the single, specified strategy for a targeted comparison (True)\nUSE_BEST_PROMPT_ONLY = True\nBEST_STRATEGY_NAME = \"Extract_VI\" # Specify the prompt to use in Mode 2\ngeneration_times = {}\n\nPROMPT_STRATEGIES = {\n    # Các chiến lược ban đầu của bạn\n    \"Direct_VI\": \"Sử dụng Ngữ cảnh sau để trả lời Câu hỏi.\",\n    \"RolePlay_VI\": \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\n    \"Current_Best_VI\": (\n        \"Bạn là một chuyên gia y tế AI với nhiệm vụ trích xuất thông tin chính xác. \"\n        \"Dựa CHỈ vào văn bản trong phần Ngữ cảnh dưới đây, hãy trả lời cho Câu hỏi. \"\n        \"Câu trả lời của bạn phải ngắn gọn, đi thẳng vào vấn đề và không chứa bất kỳ thông tin nào không có trong văn bản. \"\n        \"Không giải thích thêm.\"\n    ),\n\n    # Các chiến lược được thêm vào\n    \"Chain_of_Thought_VI\": (\n        \"Dựa vào Ngữ cảnh sau, hãy suy nghĩ từng bước một để đưa ra câu trả lời cho Câu hỏi. \"\n        \"Hãy trình bày rõ ràng các bước suy luận của bạn.\"\n    ),\n    \"Few_Shot_VI_ViMedAQA\": (\n        \"Dựa vào các Ví dụ sau đây, hãy trả lời Câu hỏi cuối cùng bằng cách trích xuất thông tin từ Ngữ cảnh được cung cấp.\\n\\n\"\n        \"--- Ví dụ 1 ---\\n\"\n        \"Ngữ cảnh: Thuốc Biviantac được chỉ định để điều trị các trường hợp do tăng tiết acid quá mức như: - Khó tiêu, nóng rát hay đau vùng thượng vị. - Trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua. - Tăng độ acid, đau rát dạ dày. - Các rối loạn thường gặp trong những bệnh lý loét dạ dày tá tràng, thực quản.\\n\"\n        \"Câu hỏi: Biviantac có thể điều trị trướng bụng, đầy hơi không?\\n\"\n        \"Câu trả lời: Có, Biviantac có thể điều trị các tình trạng như trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.\\n\\n\"\n        \"--- Ví dụ 2 ---\\n\"\n        \"Ngữ cảnh: Thuốc Atorvastatin T.V Pharm được dùng đường uống.\\n\"\n        \"Câu hỏi: Tổng hợp các cách dùng hiệu quả để quản lý Atorvastatin T.V Pharm?\\n\"\n        \"Câu trả lời: Các cách thức dùng thuốc Atorvastatin T.V Pharm hiệu quả là sử dụng đường uống.\\n\\n\"\n        \"--- Ví dụ 3 ---\\n\"\n        \"Ngữ cảnh: - Buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột. - Mệt mỏi. - Ban, mày đay. - Thiếu máu tan huyết. - Yếu cơ. - Khó thở, sốc phản vệ.\\n\"\n        \"Câu hỏi: Các tác dụng phụ thường gặp của thuốc Aspirin 81 là gì?\\n\"\n        \"Câu trả lời: Các tác dụng phụ thường gặp của thuốc Aspirin 81 bao gồm buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột.\\n\\n\"\n        \"--- Bây giờ, hãy trả lời Câu hỏi sau dựa trên Ngữ cảnh của nó---\\n\"\n    ),\n    \"Expert_Persona_VI\": (\n        \"Bạn là một chuyên gia trong lĩnh vực y tế.\"\n        \"Dựa trên kiến thức chuyên môn của mình, hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\"\n    ),\n}\n\ndef _resolve_model_id(model_id: str) -> str:\n    return MODEL_ID_ALIAS.get(model_id, model_id)\n    \ndef create_prompt(sample, model_id, tokenizer, strategy_name):\n    \"\"\"\n    Create a fairly-compared prompt with model-specific chat formatting.\n    - Sailor (Qwen1.5-based): roles = [\"system\", \"question\"]\n    - VinaLLaMA Chat (ChatML): roles = [\"system\", \"user\"]\n    \"\"\"\n    context = sample[\"context\"]\n    question = sample[\"question\"]\n\n    # 1) Instruction text\n    base_instruction = PROMPT_STRATEGIES.get(strategy_name)\n    if not base_instruction:\n        raise ValueError(f\"Strategy '{strategy_name}' not found in PROMPT_STRATEGIES.\")\n    full_instruction_text = f\"{base_instruction}\\n\\nNgữ cảnh: {context}\\n\\nCâu hỏi: {question}\"\n\n    # 2) Model-specific formatting\n    mid = _resolve_model_id(model_id)\n\n    # --- Sailor-*-Chat (Qwen1.5 family): expects 'system' + 'question' ---\n    if mid.startswith(\"sail/Sailor\") and mid.endswith(\"-Chat\"):\n        system_prompt = \"Bạn là một trợ lý hữu ích, trả lời CHỈ dựa vào Ngữ cảnh.\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            # IMPORTANT: Sailor examples use role 'question' instead of 'user'\n            {\"role\": \"question\", \"content\": full_instruction_text},\n        ]\n        return tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n    # --- VinaLLaMA 2.7B-Chat (ChatML): 'system' + 'user' ---\n    if mid.startswith(\"vilm/vinallama-2.7b-chat\"):\n        system_prompt = \"Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": full_instruction_text},\n        ]\n        return tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n    # Fallback: try generic LLaMA/Qwen-style roles (system+user)\n    messages = [\n        {\"role\": \"system\", \"content\": \"Bạn là một trợ lý hữu ích.\"},\n        {\"role\": \"user\", \"content\": full_instruction_text},\n    ]\n    return tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:45:33.261363Z","iopub.execute_input":"2025-09-13T16:45:33.262085Z","iopub.status.idle":"2025-09-13T16:45:33.272180Z","shell.execute_reply.started":"2025-09-13T16:45:33.262066Z","shell.execute_reply":"2025-09-13T16:45:33.271480Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n\n    wide_results = []\n    for i, sample in enumerate(eval_dataset):\n        wide_results.append({\n            \"Sample_ID\": i,\n            \"Question\": sample['question'],\n            \"Context\": sample['context'],\n            \"Ground_Truth_Answer\": ground_truth_answers[i][0]\n        })\n    \n    # Loop through each model to generate answers\n    for raw_model_id in model_ids:\n        model_id = _resolve_model_id(raw_model_id)  # dùng alias sang biến thể -Chat nếu cần\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id} (requested: {raw_model_id})\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token, trust_remote_code=True)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            # Chọn danh sách chiến lược prompt\n            if USE_BEST_PROMPT_ONLY:\n                prompt_variations = [BEST_STRATEGY_NAME]\n                print(f\"Mode: Best Prompt Only. Running with the fair strategy: '{BEST_STRATEGY_NAME}'\")\n            else:\n                prompt_variations = list(PROMPT_STRATEGIES.keys())\n                print(f\"Mode: Exploration. Running all {len(prompt_variations)} fair strategies: {prompt_variations}\")\n\n            # Thiết lập token kết thúc/đệm an toàn\n            eos_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n            pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_id\n\n            for prompt_name in prompt_variations:\n                print(f\"\\n--- Testing Prompt Strategy: {prompt_name} ---\")\n                result_key = f\"{raw_model_id} ({prompt_name})\"\n\n                # Tạo toàn bộ prompts bằng hàm factory đã chuẩn hóa định dạng theo model\n                prompts = [create_prompt(sample, model_id, tokenizer, prompt_name) for sample in eval_dataset]\n                print(prompts)\n\n                start_time = time.time()\n                print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n\n                # Generate answers cho cả batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=False,             # greedy\n                    eos_token_id=eos_id,\n                    pad_token_id=pad_id,\n                )\n                end_time = time.time()\n                generation_time = end_time - start_time\n                generation_times[result_key] = generation_time\n                print(f\"Time for generating answer: {generation_time:.2f} seconds.\")\n    \n                # Extract the clean answers bằng cách cắt phần completion sau prompt\n                model_answers = []\n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0][\"generated_text\"]\n                    prompt_text = prompts[i]\n                    if generated_text.startswith(prompt_text):\n                        clean_answer = generated_text[len(prompt_text):].strip()\n                    else:\n                        # fallback rất hiếm khi cần (do pipeline đôi khi không echo full prompt)\n                        clean_answer = generated_text.replace(prompt_text, \"\").strip()\n                    model_answers.append(clean_answer)\n    \n                # Lưu kết quả\n                all_generated_answers[result_key] = model_answers\n                print(f\"Successfully generated answers for {result_key}.\")\n\n                # Thêm cột vào wide_results\n                answer_column_name = f\"Answer_{result_key}\"\n                for i in range(len(model_answers)):\n                    wide_results[i][answer_column_name] = model_answers[i]\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Dọn tài nguyên\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n            import gc; gc.collect()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:45:33.272935Z","iopub.execute_input":"2025-09-13T16:45:33.273116Z","iopub.status.idle":"2025-09-13T16:47:09.356942Z","shell.execute_reply.started":"2025-09-13T16:45:33.273102Z","shell.execute_reply":"2025-09-13T16:47:09.356036Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nLoading model: sail/Sailor-4B-Chat (requested: sail/Sailor-4B-Chat)\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Mode: Best Prompt Only. Running with the fair strategy: 'Extract_VI'\n\n--- Testing Prompt Strategy: Extract_VI ---\n['<|im_start|>system\\nBạn là một trợ lý hữu ích, trả lời CHỈ dựa vào Ngữ cảnh.<|im_end|>\\n<|im_start|>question\\nDựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\\n\\nNgữ cảnh: Thuốc D-Cure là sản phẩm có chứa hàm lượng vitamin D, giúp bổ sung vitamin D cho cơ thể. Sản phẩm này có tác dụng điều trị cũng như giúp dự phòng các bệnh cụ thể như: - Tình trạng thiếu hụt vitamin D.\\n- Bị bệnh loãng xương. \\n\\nCâu hỏi: D-Cure có tác dụng dự phòng những bệnh gì?<|im_end|>\\n<|im_start|>answer\\n']\nGenerating answers for 1 prompts using sail/Sailor-4B-Chat with 'Extract_VI' strategy...\nTime for generating answer: 25.76 seconds.\nSuccessfully generated answers for sail/Sailor-4B-Chat (Extract_VI).\n\n==================================================\nLoading model: vilm/vinallama-2.7b-chat (requested: vilm/vinallama-2.7b-chat)\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"232435b289924d9a831d59767a6bac79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a6151e0cdb4d6e896ade39c4e1d80f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/558 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"616161845bae4ea781c8837af1b36934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1924a3f6fb7b45989a61ada4fe2130ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/5.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a8052a44d04a099ed4771b44908822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03616e7afe8142d18c2c07dc2c26fe2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a98c1dbd5b17461da3502f3437a8a06b"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Mode: Best Prompt Only. Running with the fair strategy: 'Extract_VI'\n\n--- Testing Prompt Strategy: Extract_VI ---\nAn error occurred while processing vilm/vinallama-2.7b-chat: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- Bước 5.5 - Lưu tất cả các câu trả lời đã tạo vào một tệp CSV ---\nimport pandas as pd\nfrom IPython.display import display\nimport datetime\n\nif wide_results:\n    print(\"\\n\" + \"=\"*50)\n    print(\"Đang lưu kết quả định dạng rộng vào tệp CSV...\")\n    print(\"=\"*50)\n    \n    # Chuyển đổi danh sách kết quả thành một DataFrame\n    results_df_wide = pd.DataFrame(wide_results)\n    \n    # Đặt tên file có timestamp để tránh bị ghi đè\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_file_path = f\"/kaggle/working/generation_results_wide_{timestamp}.csv\"\n    \n    # Lưu DataFrame vào tệp CSV\n    results_df_wide.to_csv(output_file_path, index=False, encoding=\"utf-8-sig\")\n    \n    print(f\"Hoàn tất! Đã lưu {len(results_df_wide)} hàng (mẫu) vào tệp:\")\n    print(output_file_path)\n    print(\"Các cột trong file CSV:\")\n    print(results_df_wide.columns.tolist())\n    \n    # Hiển thị 10 hàng đầu tiên để xem nhanh\n    display(results_df_wide.head(10))\nelse:\n    print(\"\\nKhông có kết quả nào để lưu.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:47:09.358076Z","iopub.execute_input":"2025-09-13T16:47:09.358292Z","iopub.status.idle":"2025-09-13T16:47:09.424635Z","shell.execute_reply.started":"2025-09-13T16:47:09.358276Z","shell.execute_reply":"2025-09-13T16:47:09.423904Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nĐang lưu kết quả định dạng rộng vào tệp CSV...\n==================================================\nHoàn tất! Đã lưu 1 hàng (mẫu) vào tệp:\n/kaggle/working/generation_results_wide_20250913_164709.csv\nCác cột trong file CSV:\n['Sample_ID', 'Question', 'Context', 'Ground_Truth_Answer', 'Answer_sail/Sailor-4B-Chat (Extract_VI)']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Sample_ID                                    Question  \\\n0          0  D-Cure có tác dụng dự phòng những bệnh gì?   \n\n                                             Context  \\\n0  Thuốc D-Cure là sản phẩm có chứa hàm lượng vit...   \n\n                                 Ground_Truth_Answer  \\\n0  D-Cure có tác dụng dự phòng tình trạng thiếu h...   \n\n             Answer_sail/Sailor-4B-Chat (Extract_VI)  \n0  Sản phẩm D-Cure có tác dụng dự phòng tình trạn...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sample_ID</th>\n      <th>Question</th>\n      <th>Context</th>\n      <th>Ground_Truth_Answer</th>\n      <th>Answer_sail/Sailor-4B-Chat (Extract_VI)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>D-Cure có tác dụng dự phòng những bệnh gì?</td>\n      <td>Thuốc D-Cure là sản phẩm có chứa hàm lượng vit...</td>\n      <td>D-Cure có tác dụng dự phòng tình trạng thiếu h...</td>\n      <td>Sản phẩm D-Cure có tác dụng dự phòng tình trạn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    import evaluate\n    import pandas as pd\n\n    # Load metrics\n    rouge_metric = evaluate.load(\"rouge\")\n    bleu_metric = evaluate.load(\"bleu\")\n    meteor_metric = evaluate.load(\"meteor\")\n    bertscore_metric = evaluate.load(\"bertscore\")\n\n    evaluation_results = []\n\n    # --- Chuẩn hóa dữ liệu đầu vào cho các metric ---\n    # predictions: List[str]\n    # ground_truth_answers: List[List[str]] (đã có dạng này từ Step 5)\n    # -> riêng cho BERTScore cần List[str], nên flatten tham chiếu\n    references_for_bert = [refs[0] if isinstance(refs, list) and len(refs) > 0 else \"\" \n                           for refs in ground_truth_answers]\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        print(f\"\\n--- Evaluating {result_key} ---\")\n\n        # Làm sạch outputs để tránh lỗi metric do None/ khoảng trắng thừa\n        predictions = [(\"\" if p is None else str(p)).strip() for p in predictions]\n\n        # Check empty predictions\n        if not any(predictions):\n            print(f\"  WARNING: Model & Prompt Strategy '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & Prompt Strategy\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0,\n                \"Generation Time (s)\": round(generation_times.get(result_key, 0), 2),\n            }\n            evaluation_results.append(result_row)\n            continue\n\n        # Align lengths nếu số câu trả lời != số ground truths\n        if len(predictions) != len(ground_truth_answers):\n            min_len = min(len(predictions), len(ground_truth_answers))\n            print(f\"  NOTE: Mismatch sizes (pred={len(predictions)}, refs={len(ground_truth_answers)}). Truncating to {min_len}.\")\n            predictions = predictions[:min_len]\n            refs_nested = ground_truth_answers[:min_len]\n            refs_bert = references_for_bert[:min_len]\n        else:\n            refs_nested = ground_truth_answers\n            refs_bert = references_for_bert\n\n        # Tính các metric\n        try:\n            rouge_scores = rouge_metric.compute(predictions=predictions, references=refs_nested)\n        except Exception as e:\n            print(f\"  ERROR computing ROUGE: {e}\")\n            rouge_scores = {\"rougeL\": 0.0}\n\n        try:\n            bleu_scores = bleu_metric.compute(predictions=predictions, references=refs_nested)\n        except Exception as e:\n            print(f\"  ERROR computing BLEU: {e}\")\n            bleu_scores = {\"bleu\": 0.0}\n\n        try:\n            meteor_scores = meteor_metric.compute(predictions=predictions, references=refs_nested)\n        except Exception as e:\n            print(f\"  ERROR computing METEOR: {e}\")\n            meteor_scores = {\"meteor\": 0.0}\n\n        try:\n            bertscore_scores = bertscore_metric.compute(\n                predictions=predictions,\n                references=refs_bert,     # <- BERTScore cần list[str]\n                lang=\"vi\"\n            )\n            bert_f1 = sum(bertscore_scores.get(\"f1\", [])) / max(1, len(bertscore_scores.get(\"f1\", [])))\n        except Exception as e:\n            print(f\"  ERROR computing BERTScore: {e}\")\n            bert_f1 = 0.0\n\n        # Tổng hợp kết quả\n        result_row = {\n            \"Model & Prompt Strategy\": result_key,\n            \"ROUGE-L\": round(float(rouge_scores.get(\"rougeL\", 0.0)), 4),\n            \"BLEU\": round(float(bleu_scores.get(\"bleu\", 0.0)), 4),\n            \"METEOR\": round(float(meteor_scores.get(\"meteor\", 0.0)), 4),\n            \"BERTScore-F1\": round(float(bert_f1), 4),\n            \"Generation Time (s)\": round(generation_times.get(result_key, 0), 2),\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    results_df = results_df.sort_values(by=\"BERTScore-F1\", ascending=False).reset_index(drop=True)\n\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    try:\n        from IPython.display import display\n        display(results_df)\n    except Exception:\n        print(results_df.to_string(index=False))\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T16:47:09.425689Z","iopub.execute_input":"2025-09-13T16:47:09.426376Z","iopub.status.idle":"2025-09-13T16:47:31.213692Z","shell.execute_reply.started":"2025-09-13T16:47:09.426358Z","shell.execute_reply":"2025-09-13T16:47:31.212455Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"277d55eee02f4007935b47542bd7b4cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730ceccfa9bb41acb5e5b661be4f2166"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2568760ce6d14633be455e6ce81108c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63d82bcf50484328a492b6eb86b98126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92cebd8e21441718d9d614b36755d78"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82809bf34d604394ae702889b037751d"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nCalculating Evaluation Metrics\n==================================================\n\n--- Evaluating sail/Sailor-4B-Chat (Extract_VI) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c09b84ec04a417eabdbd15699277b53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d64c64e2604f69b9276559e84624c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509c92554b0b4a91b0ec027cbbd85f45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df1309b0fb44469bd9e58c665f16e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"878a9dcd36cb42569cd4fe99307faaf6"}},"metadata":{}},{"name":"stdout","text":"\n--- Comparative Evaluation Results ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            Model & Prompt Strategy  ROUGE-L    BLEU  METEOR  BERTScore-F1  \\\n0  sail/Sailor-4B-Chat (Extract_VI)   0.1379  0.0617  0.4459        0.6724   \n\n   Generation Time (s)  \n0                25.76  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model &amp; Prompt Strategy</th>\n      <th>ROUGE-L</th>\n      <th>BLEU</th>\n      <th>METEOR</th>\n      <th>BERTScore-F1</th>\n      <th>Generation Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sail/Sailor-4B-Chat (Extract_VI)</td>\n      <td>0.1379</td>\n      <td>0.0617</td>\n      <td>0.4459</td>\n      <td>0.6724</td>\n      <td>25.76</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14}]}