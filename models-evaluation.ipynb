{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"NUM_SAMPLES_TO_EVALUATE = 1\nUSE_BEST_PROMPT_ONLY = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:34.643329Z","iopub.execute_input":"2025-09-12T15:44:34.644176Z","iopub.status.idle":"2025-09-12T15:44:34.648112Z","shell.execute_reply.started":"2025-09-12T15:44:34.644150Z","shell.execute_reply":"2025-09-12T15:44:34.647015Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:34.649338Z","iopub.execute_input":"2025-09-12T15:44:34.649640Z","iopub.status.idle":"2025-09-12T15:44:38.277658Z","shell.execute_reply.started":"2025-09-12T15:44:34.649622Z","shell.execute_reply":"2025-09-12T15:44:38.276772Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate # Hugging Face's library for NLP evaluation\nimport warnings\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:38.278797Z","iopub.execute_input":"2025-09-12T15:44:38.279061Z","iopub.status.idle":"2025-09-12T15:44:39.681400Z","shell.execute_reply.started":"2025-09-12T15:44:38.279038Z","shell.execute_reply":"2025-09-12T15:44:39.680625Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:39.683049Z","iopub.execute_input":"2025-09-12T15:44:39.683797Z","iopub.status.idle":"2025-09-12T15:44:39.795628Z","shell.execute_reply.started":"2025-09-12T15:44:39.683777Z","shell.execute_reply":"2025-09-12T15:44:39.795069Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\nmodel_ids = [\n    \"alpha-ai/LLAMA3-3B-Medical-COT\",\n    \"vilm/vietcuna-3b-v2\",\n    \"arcee-ai/Arcee-VyLinh\",\n]\ndataset_id = \"tmnam20/ViMedAQA\"\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    # Create a small, random, representative sample for evaluation\n    random.seed(42) # for reproducibility\n    random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_TO_EVALUATE)\n    eval_dataset = dataset.select(random_indices)\n\n    print(f\"Created a random evaluation set with {len(eval_dataset)} samples.\")\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:39.796254Z","iopub.execute_input":"2025-09-12T15:44:39.796431Z","iopub.status.idle":"2025-09-12T15:44:42.018564Z","shell.execute_reply.started":"2025-09-12T15:44:39.796417Z","shell.execute_reply":"2025-09-12T15:44:42.017987Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a61811a4f064d97a5f7b14f986fba92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/20.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28daa3c9e7bf4148bd3a7e02910131e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192894e6bb634f98a69d0224cadd4bc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c178136148a9412b832833756a4b504a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/39881 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a245fc2d714a8390e88c422bce6caf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2217 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cc6545732794b7fbbd6b91a03ece8da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2215 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f7c7b19412428283913eee7d40b30f"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded successfully! Total samples: 39881\nCreated a random evaluation set with 1 samples.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\nBEST_PROMPTS = {\n    \"alpha-ai/LLAMA3-3B-Medical-COT\": \"Llama3_Direct_VI\",\n    \"vilm/vietcuna-3b-v2\": \"Vietcuna_Direct_NoExplain_VI\",\n    \"arcee-ai/Arcee-VyLinh\": \"VyLinh_ChatTemplate_VI\",\n}\n\ndef create_llama3_prompts(sample, tokenizer=None):\n    \"\"\"\n    Creates a set of prompt variations for Llama 3 / ChatML format,\n    with instructions in both English and Vietnamese.\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n\n    prompts = {}\n\n    # --- Strategy 1: Original (Role-playing, strict context) ---\n    # English Instruction\n    prompts[\"Llama3_RolePlay_Strict_EN\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are a helpful medical assistant. Based *only* on the context provided below, answer the question in Vietnamese.\n\nContext: {context}\n\nQuestion: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    # Vietnamese Instruction\n    prompts[\"Llama3_RolePlay_Strict_VI\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nBạn là một trợ lý y tế hữu ích. Dựa *chỉ* vào ngữ cảnh được cung cấp dưới đây, hãy trả lời câu hỏi bằng tiếng Việt.\n\nNgữ cảnh: {context}\n\nCâu hỏi: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n\n    # --- Strategy 2: Direct and Simple ---\n    # English Instruction\n    prompts[\"Llama3_Direct_EN\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nUse the following context to answer the question.\n\nContext: {context}\n\nQuestion: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    # Vietnamese Instruction\n    prompts[\"Llama3_Direct_VI\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nSử dụng ngữ cảnh sau để trả lời câu hỏi.\n\nNgữ cảnh: {context}\n\nCâu hỏi: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    # --- Strategy 3: Chain-of-Thought Style ---\n    # English Instruction\n    prompts[\"Llama3_CoT_EN\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nRead the context, think step-by-step, and then answer the user's question based only on the information in the context.\n\nContext: {context}\n\nQuestion: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n    \n    # Vietnamese Instruction\n    prompts[\"Llama3_CoT_VI\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHãy đọc ngữ cảnh, suy nghĩ từng bước, và sau đó trả lời câu hỏi của người dùng chỉ dựa vào thông tin trong ngữ cảnh.\n\nNgữ cảnh: {context}\n\nCâu hỏi: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n    \n    return prompts\n\ndef create_vietcuna_prompts(sample, tokenizer=None):\n    \"\"\"\n    Creates a set of prompt variations for the Vicuna format,\n    with instructions in both English and Vietnamese.\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n    \n    prompts = {}\n\n    # --- Strategy 1: Original (Direct, no explanation) ---\n    # Vietnamese Instruction (Original)\n    instruction_vi_1 = (\n        \"Dựa vào ngữ cảnh sau đây để trả lời câu hỏi. Chỉ trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\\n\\n\"\n        f\"Ngữ cảnh: {context}\\n\\n\"\n        f\"Câu hỏi: {question}\"\n    )\n    prompts[\"Vietcuna_Direct_NoExplain_VI\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_vi_1}\\nASSISTANT:\"\n\n    # English Instruction\n    instruction_en_1 = (\n        \"Based on the following context, answer the question. Only extract the direct answer from the text, do not add any explanation.\\n\\n\"\n        f\"Context: {context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n    prompts[\"Vietcuna_Direct_NoExplain_EN\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_en_1}\\nASSISTANT:\"\n\n    # --- Strategy 2: Role-playing ---\n    # Vietnamese Instruction\n    instruction_vi_2 = (\n        \"Bạn là một trợ lý y tế hữu ích. Dựa vào thông tin trong ngữ cảnh được cung cấp để trả lời câu hỏi của người dùng.\\n\\n\"\n        f\"Ngữ cảnh: {context}\\n\\n\"\n        f\"Câu hỏi: {question}\"\n    )\n    prompts[\"Vietcuna_RolePlay_VI\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_vi_2}\\nASSISTANT:\"\n\n    # English Instruction\n    instruction_en_2 = (\n        \"You are a helpful medical assistant. Based on the information in the provided context, answer the user's question.\\n\\n\"\n        f\"Context: {context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n    prompts[\"Vietcuna_RolePlay_EN\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_en_2}\\nASSISTANT:\"\n\n    # --- Strategy 3: Simplified (Context + Question only) ---\n    # Vietnamese Instruction\n    instruction_vi_3 = (\n        f\"Ngữ cảnh: {context}\\n\\n\"\n        f\"Câu hỏi: {question}\"\n    )\n    prompts[\"Vietcuna_Simplified_VI\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_vi_3}\\nASSISTANT:\"\n\n    # English Instruction\n    instruction_en_3 = (\n        f\"Context: {context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n    prompts[\"Vietcuna_Simplified_EN\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_en_3}\\nASSISTANT:\"\n    \n    return prompts\n\ndef create_vylinh_prompts(sample, tokenizer):\n    \"\"\"\n    Creates prompt variations for Arcee-VyLinh using the official tokenizer.apply_chat_template method.\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n    prompts = {}\n\n    # --- Vietnamese Instruction ---\n    # Construct the message list for the chat template\n    messages_vi = [\n        {\"role\": \"system\", \"content\": \"Bạn là một trợ lý y tế hữu ích. Dựa vào ngữ cảnh được cung cấp để trả lời câu hỏi.\"},\n        {\"role\": \"user\", \"content\": f\"Ngữ cảnh: {context}\\n\\nCâu hỏi: {question}\"}\n    ]\n    # Apply the template\n    prompts[\"VyLinh_ChatTemplate_VI\"] = tokenizer.apply_chat_template(\n        messages_vi,\n        tokenize=False,\n        add_generation_prompt=True # Crucial for telling the model to start its response\n    )\n\n    # --- English Instruction ---\n    messages_en = [\n        {\"role\": \"system\", \"content\": \"You are a helpful medical assistant. Use the provided context to answer the question.\"},\n        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"}\n    ]\n    prompts[\"VyLinh_ChatTemplate_EN\"] = tokenizer.apply_chat_template(\n        messages_en,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    return prompts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:42.019339Z","iopub.execute_input":"2025-09-12T15:44:42.019619Z","iopub.status.idle":"2025-09-12T15:44:42.030362Z","shell.execute_reply.started":"2025-09-12T15:44:42.019591Z","shell.execute_reply":"2025-09-12T15:44:42.029849Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n    \n    # Loop through each model to generate answers\n    for model_id in model_ids:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id}\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config, # <-- PASS THE CONFIG OBJECT HERE\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            # Determine which set of prompt functions to use\n            if 'vietcuna' in model_id:\n                # This model has no chat template, so we MUST format the string manually.\n                prompt_function = create_vietcuna_prompts\n                answer_start_tag = \"ASSISTANT:\"\n            elif 'Arcee-VyLinh' in model_id:\n                # This model uses the ChatML format, revealed by apply_chat_template.\n                prompt_function = create_vylinh_prompts \n                answer_start_tag = \"<|im_start|>assistant\" # Corrected from previous versions\n            # --------------------------\n            else: # Default to Llama3\n                # This model uses the Llama 3 chat format.\n                prompt_function = create_llama3_prompts\n                answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n\n            # -Logic to select which prompts to run based on the toggle\n            if USE_BEST_PROMPT_ONLY:\n                # If the model has a defined best prompt, run only that one\n                if model_id in BEST_PROMPTS:\n                    prompt_variations = [BEST_PROMPTS[model_id]]\n                    print(f\"Mode: Best Prompt Only. Running with '{prompt_variations[0]}'\")\n                else:\n                    print(f\"Warning: No best prompt defined for {model_id}. Skipping.\")\n                    continue\n            else:\n                # Run all defined prompt variations for the model\n                prompt_variations = prompt_function(eval_dataset[0], tokenizer).keys() \n                print(f\"Mode: Exploration. Running all {len(prompt_variations)} prompt strategies.\")\n\n            for prompt_name in prompt_variations:\n                print(f\"\\n--- Testing Prompt Strategy: {prompt_name} ---\")\n                \n                # Generate all prompts for the current strategy\n                prompts = [prompt_function(sample, tokenizer)[prompt_name] for sample in eval_dataset]\n\n                print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n                # Generate answers for the entire batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=True,\n                    temperature=0.1,\n                    top_p=0.9,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id, # Set pad_token_id to avoid warnings\n                )\n    \n                # Extract the clean answers\n                model_answers = []\n                \n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0]['generated_text']\n                    # Use the appropriate start tag for splitting\n                    if answer_start_tag in generated_text:\n                        clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                    else:\n                        # Fallback remains the same\n                        clean_answer = generated_text.replace(prompts[i], \"\").strip()\n                    model_answers.append(clean_answer)\n\n                # Create a unique key for each result set\n                result_key = f\"{model_id} ({prompt_name})\"\n    \n                # Use the unique result_key to store the answers\n                all_generated_answers[result_key] = model_answers\n                print(f\"Successfully generated answers for {result_key}.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Check if variables were successfully created before deleting\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:44:42.031040Z","iopub.execute_input":"2025-09-12T15:44:42.031243Z","iopub.status.idle":"2025-09-12T15:48:29.141831Z","shell.execute_reply.started":"2025-09-12T15:44:42.031228Z","shell.execute_reply":"2025-09-12T15:48:29.141173Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nLoading model: alpha-ai/LLAMA3-3B-Medical-COT\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c6b5e81b8449d59901caedf4d19d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f21d46e5bbb40ab844d5348efaa7438"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"837461c8b0124d5eb64c4442984c0fb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/982 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"577df99481e44335b03755cdf88a6756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8557bebdbd1f4ee887ac1227698ecf78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e4e2fe9594f400a8a44b983c14d8881"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\nDevice set to use cuda:1\n","output_type":"stream"},{"name":"stdout","text":"Mode: Best Prompt Only. Running with 'Llama3_Direct_VI'\n\n--- Testing Prompt Strategy: Llama3_Direct_VI ---\nGenerating answers for 1 prompts using alpha-ai/LLAMA3-3B-Medical-COT with 'Llama3_Direct_VI' strategy...\nSuccessfully generated answers for alpha-ai/LLAMA3-3B-Medical-COT (Llama3_Direct_VI).\n\n==================================================\nLoading model: vilm/vietcuna-3b-v2\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd9481856ad347258777addf2330c639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f309be48f0e4613b0fa8ea417d1f5aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"450503ffb93643b5b341c53c9aef8aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d3729093ae048bca2605ad6076ecd39"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:1\n","output_type":"stream"},{"name":"stdout","text":"Mode: Best Prompt Only. Running with 'Vietcuna_Direct_NoExplain_VI'\n\n--- Testing Prompt Strategy: Vietcuna_Direct_NoExplain_VI ---\nGenerating answers for 1 prompts using vilm/vietcuna-3b-v2 with 'Vietcuna_Direct_NoExplain_VI' strategy...\nSuccessfully generated answers for vilm/vietcuna-3b-v2 (Vietcuna_Direct_NoExplain_VI).\n\n==================================================\nLoading model: arcee-ai/Arcee-VyLinh\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af89a5bcd24348e799c05c632e4bcd06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e183daaa05046f8b7f652c409247457"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb0a92d837a49009a58aab00ee52814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74fefba404c64e5b999e11268b7409a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a51806ae83a4a29baa5492ac51d86b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dcc743eb4624ac6864741437d1e41d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/731 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1baec70749b948aa884d451828fb7f7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f86cb88b5804ecca7678f13bd3e923b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87cbccb9193c4c5b83ec844186b4a663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e7e31f98e14bd2891714cd737421fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c313982a164d7c85b7760a111523f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2171bab50daf40739cd40d73e16f59d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad08996c98146ccb85b28cc06826aba"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Mode: Best Prompt Only. Running with 'VyLinh_ChatTemplate_VI'\n\n--- Testing Prompt Strategy: VyLinh_ChatTemplate_VI ---\nGenerating answers for 1 prompts using arcee-ai/Arcee-VyLinh with 'VyLinh_ChatTemplate_VI' strategy...\nSuccessfully generated answers for arcee-ai/Arcee-VyLinh (VyLinh_ChatTemplate_VI).\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    # Load all the metrics we need\n    rouge_metric = evaluate.load('rouge')\n    bleu_metric = evaluate.load('bleu')\n    meteor_metric = evaluate.load('meteor')\n    bertscore_metric = evaluate.load('bertscore')\n\n    evaluation_results = []\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        print(f\"\\n--- Evaluating {result_key} ---\")\n    \n        # Check for empty predictions to prevent ZeroDivisionError in BLEU ---\n        # The `any()` function returns False if all strings in the list are empty.\n        if not any(predictions):\n            print(f\"  WARNING: Model & Prompt Strategy '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & Prompt Strategy\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0\n            }\n            evaluation_results.append(result_row)\n            # Use `continue` to skip the rest of the loop and move to the next model\n            continue\n    \n        # If predictions are valid, compute metrics as normal\n        rouge_scores = rouge_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bleu_scores = bleu_metric.compute(predictions=predictions, references=ground_truth_answers)\n        meteor_scores = meteor_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bertscore_scores = bertscore_metric.compute(predictions=predictions, references=ground_truth_answers, lang=\"vi\")\n    \n        # Store results (this part is the same as before)\n        result_row = {\n            \"Model & Prompt Strategy\": result_key,\n            \"ROUGE-L\": round(rouge_scores['rougeL'], 4),\n            \"BLEU\": round(bleu_scores['bleu'], 4),\n            \"METEOR\": round(meteor_scores['meteor'], 4),\n            \"BERTScore-F1\": round(sum(bertscore_scores['f1']) / len(bertscore_scores['f1']), 4)\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    # Sort for better comparison\n    results_df = results_df.sort_values(by=\"BERTScore-F1\", ascending=False).reset_index(drop=True)\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    display(results_df)\n\n    # # Example generation\n    # print(\"\\n--- Example Generations ---\")\n    # example_df_data = {\n    #     \"Question\": questions[:3],\n    #     \"Ground Truth\": [gt[0] for gt in ground_truth_answers[:3]] # Unpack the list for cleaner display\n    # }\n    # # Use the new result_key for column headers\n    # for result_key, answers in all_generated_answers.items():\n    #     example_df_data[f\"Answer: {result_key}\"] = answers[:3]\n\n    # example_df = pd.DataFrame(example_df_data)\n    # display(example_df)\n\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T15:48:29.145642Z","iopub.execute_input":"2025-09-12T15:48:29.145873Z","iopub.status.idle":"2025-09-12T15:48:42.657063Z","shell.execute_reply.started":"2025-09-12T15:48:29.145857Z","shell.execute_reply":"2025-09-12T15:48:42.656417Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d5789ca042455eaf6d22f5b45817e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67873e2cd4ff45a5a3dd2af28a7794dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c059da1dff34de8a1c7724fc2c1bcf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54cc42179edc4c2fbe861adb32d5b289"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9f66c14bef47a58244dc31ecfd4540"}},"metadata":{}},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad5ff9b02166456aac836c37fc08f807"}},"metadata":{}},{"name":"stdout","text":"\n==================================================\nCalculating Evaluation Metrics\n==================================================\n\n--- Evaluating alpha-ai/LLAMA3-3B-Medical-COT (Llama3_Direct_VI) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43c96052568467d93ff15146e46fba6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4de5b7ea27348e7b49c663d2daf55ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0881abc89d8d4af4bc2184ea9826f4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c09da10101c940a2819b286fb381980f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a854733730d24d3dad2c0d87654403c3"}},"metadata":{}},{"name":"stdout","text":"\n--- Evaluating vilm/vietcuna-3b-v2 (Vietcuna_Direct_NoExplain_VI) ---\n\n--- Evaluating arcee-ai/Arcee-VyLinh (VyLinh_ChatTemplate_VI) ---\n\n--- Comparative Evaluation Results ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                             Model & Prompt Strategy  ROUGE-L    BLEU  METEOR  \\\n0  alpha-ai/LLAMA3-3B-Medical-COT (Llama3_Direct_VI)   1.0000  1.0000  0.9998   \n1  vilm/vietcuna-3b-v2 (Vietcuna_Direct_NoExplain...   0.5970  0.3305  0.8730   \n2     arcee-ai/Arcee-VyLinh (VyLinh_ChatTemplate_VI)   0.4255  0.1553  0.7473   \n\n   BERTScore-F1  \n0        1.0000  \n1        0.8920  \n2        0.7747  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model &amp; Prompt Strategy</th>\n      <th>ROUGE-L</th>\n      <th>BLEU</th>\n      <th>METEOR</th>\n      <th>BERTScore-F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>alpha-ai/LLAMA3-3B-Medical-COT (Llama3_Direct_VI)</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>0.9998</td>\n      <td>1.0000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>vilm/vietcuna-3b-v2 (Vietcuna_Direct_NoExplain...</td>\n      <td>0.5970</td>\n      <td>0.3305</td>\n      <td>0.8730</td>\n      <td>0.8920</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>arcee-ai/Arcee-VyLinh (VyLinh_ChatTemplate_VI)</td>\n      <td>0.4255</td>\n      <td>0.1553</td>\n      <td>0.7473</td>\n      <td>0.7747</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21}]}