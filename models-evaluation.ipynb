{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:50.110698Z","iopub.execute_input":"2025-09-18T00:59:50.111052Z","iopub.status.idle":"2025-09-18T00:59:53.827703Z","shell.execute_reply.started":"2025-09-18T00:59:50.111031Z","shell.execute_reply":"2025-09-18T00:59:53.826549Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate\nimport warnings\nimport time\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import logging\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:53.829878Z","iopub.execute_input":"2025-09-18T00:59:53.830200Z","iopub.status.idle":"2025-09-18T00:59:53.840496Z","shell.execute_reply.started":"2025-09-18T00:59:53.830168Z","shell.execute_reply":"2025-09-18T00:59:53.839617Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:53.841312Z","iopub.execute_input":"2025-09-18T00:59:53.841568Z","iopub.status.idle":"2025-09-18T00:59:53.945743Z","shell.execute_reply.started":"2025-09-18T00:59:53.841545Z","shell.execute_reply":"2025-09-18T00:59:53.944971Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\ndataset_id = \"tmnam20/ViMedAQA\"\n\nEVAL_FULL_DATASET = False\n\nseed_num = 6\nNUM_SAMPLES_INITIAL = 100\n\n# --- ADDED: Boolean toggle for the second randomization ---\n# Set to True to get the final 50 samples.\n# Set to False to use the initial 200 samples.\nENABLE_SUBSET_SAMPLING = False\nNUM_SAMPLES_FINAL = 50\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"test\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    if EVAL_FULL_DATASET:\n        eval_dataset = dataset\n    else:\n        # --- First Sampling Step: Always get the initial 200 samples ---\n        random.seed(seed_num) # for reproducibility\n        initial_random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_INITIAL)\n        initial_eval_dataset = dataset.select(initial_random_indices)\n    \n        print(f\"Created an initial random evaluation set with {len(initial_eval_dataset)} samples.\")\n    \n        # --- ADDED: Conditional second randomization ---\n        if ENABLE_SUBSET_SAMPLING:\n            print(\"Subset sampling is ENABLED. Performing second randomization...\")\n            # Re-seed to ensure this step is also reproducible\n            random.seed(seed_num)\n            final_random_indices = random.sample(range(len(initial_eval_dataset)), NUM_SAMPLES_FINAL)\n            # Final dataset is the smaller, 50-sample subset\n            eval_dataset = initial_eval_dataset.select(final_random_indices)\n            print(f\"Further randomized and reduced the set to a final size of {len(eval_dataset)} samples.\")\n        else:\n            print(\"Subset sampling is DISABLED.\")\n            # Final dataset is the larger, 200-sample set\n            eval_dataset = initial_eval_dataset\n            print(f\"Using the initial set of {len(eval_dataset)} samples for evaluation.\")\n\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:53.947880Z","iopub.execute_input":"2025-09-18T00:59:53.948156Z","iopub.status.idle":"2025-09-18T00:59:54.981184Z","shell.execute_reply.started":"2025-09-18T00:59:53.948133Z","shell.execute_reply":"2025-09-18T00:59:54.980359Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully! Total samples: 2217\nCreated an initial random evaluation set with 1 samples.\nSubset sampling is DISABLED.\nUsing the initial set of 1 samples for evaluation.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# USER_PROMPTS = {\n#     \"Extract_EN\": \"Based on the following Context, extract the direct answer from the text without any additional explanation.\",\n# }\n\n# SYSTEM_PROMPTS = [\n#     (\n#         \"Expert_SP_EN\",\n#         \"You are a medical expert AI. Based on your expertise, answer the following Question in Vietnamese, using ONLY the provided Context.\",\n#         \"{Extract_EN}\\n\\n### Context:\\n{context}\\n\\n### Question:\\n{question}\",\n#     ),\n# ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:54.981988Z","iopub.execute_input":"2025-09-18T00:59:54.982338Z","iopub.status.idle":"2025-09-18T00:59:54.986528Z","shell.execute_reply.started":"2025-09-18T00:59:54.982314Z","shell.execute_reply":"2025-09-18T00:59:54.985767Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"USER_PROMPTS = {\n    # \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\n    # \"Concise_VI\": (\n    #     f\"Dựa CHỈ vào văn bản trong phần Ngữ cảnh dưới đây, hãy trả lời cho Câu hỏi. \"\n    #     f\"Câu trả lời của bạn phải ngắn gọn, đi thẳng vào vấn đề và không chứa bất kỳ thông tin nào không có trong văn bản. \"\n    #     f\"Không giải thích thêm.\"\n    # ),\n    # \"Strict_Rules_VI\": (\n    #     f\"TỪ Ngữ cảnh, TRÍCH XUẤT câu trả lời cho Câu hỏi.\\n\\n\"\n    #     f\"**QUY TẮC:**\\n\"\n    #     f\"1. CHỈ sử dụng thông tin từ Ngữ cảnh.\\n\"\n    #     f\"2. KHÔNG giải thích các bước của bạn.\\n\"\n    #     f\"3. KHÔNG tự suy luận hoặc thêm bất kỳ thông tin bên ngoài nào.\\n\"\n    #     f\"4. Cung cấp câu trả lời được trích xuất trực tiếp.\\n\\n\"\n    #     f\"Dưới đây là Ngữ cảnh và Câu hỏi, hãy đưa câu trả lời TRÍCH XUẤT:\"\n    # ),\n\n    # --- ENGLISH PROMPTS ---\n    \"Extract_EN\": \"Based on the following Context, extract the direct answer from the text without any additional explanation.\",\n    # \"Concise_EN\": (\n    #     f\"Based ONLY on the text in the Context section below, answer the Question. \"\n    #     f\"Your answer must be concise, to the point, and contain no information not present in the text. \"\n    #     f\"Do not add any explanation.\"\n    # ),\n    # \"Strict_Rules_EN\": (\n    #     f\"From the Context, EXTRACT the answer to the Question.\\n\\n\"\n    #     f\"**RULES:**\\n\"\n    #     f\"1. ONLY use information from the Context.\\n\"\n    #     f\"2. DO NOT explain your steps.\\n\"\n    #     f\"3. DO NOT infer or add any external information.\\n\"\n    #     f\"4. Provide the directly extracted answer.\\n\\n\"\n    #     f\"Below is the Context and the Question, provide the EXTRACTED answer:\"\n    # ),\n\n    # \"Direct_VI\": \"Sử dụng Ngữ cảnh sau để trả lời Câu hỏi.\",\n    # \"Direct_EN\": \"Use the following Context to answer the Question.\",\n\n    # \"List_VI\": (\n    #     f\"Từ Ngữ cảnh được cung cấp, hãy **liệt kê tất cả** các thông tin dùng để trả lời cho Câu hỏi.\"\n    #     f\"Trình bày câu trả lời một cách ngắn gọn, chỉ bao gồm các điểm được tìm thấy.\"\n    # ),\n    # \"List_EN\": (\n    #     f\"From the provided Context, **list all** the information that answers the Question. \"\n    #     f\"Present the answer concisely, including only the points found.\"\n    # ),\n\n    \"ViMedAQA\": \"\",\n}\n\ndefault_instruction_format_vi = \"{base_instruction}\\n\\n### Ngữ cảnh:\\n{context}\\n\\n### Câu hỏi:\\n{question}\"\ndefault_instruction_format_en = \"{base_instruction}\\n\\n### Context:\\n{context}\\n\\n### Question:\\n{question}\"\n\nSYSTEM_PROMPTS = [\n    # (\n    #     \"Expert_SP_VI\",\n    #     \"Bạn là một AI chuyên gia y tế. Dựa trên kiến thức chuyên môn của mình, hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    #     default_instruction_format_vi,\n    # ),\n    (\n        \"Expert_SP_EN\",\n        \"You are a medical expert AI. Based on your expertise, answer the following Question in Vietnamese, using ONLY the provided Context.\",\n        default_instruction_format_en,\n    ),\n\n    # (\n    #     \"Default_SP_VI\",\n    #     \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    #     default_instruction_format_vi,\n    # ),\n    # (\n    #     \"Default_SP_EN\",\n    #     \"You are a helpful medical assistant. Answer the following Question in Vietnamese, based ONLY on the provided Context.\",\n    #     default_instruction_format_en,\n    # ),\n\n    # (\n    #     \"Empty_SP_VI\",\n    #     \"\",\n    #     default_instruction_format_vi,\n    # ),\n    # (\n    #     \"Empty_SP_EN\",\n    #     \"\",\n    #     default_instruction_format_en,\n    # ),\n\n    (\n        \"ViMedAQA_SP_VI\",\n        \"Dựa vào ngữ cảnh sau và kiến thức của bạn, trả lời câu hỏi sau bằng tiếng Việt.\",\n        default_instruction_format_vi,\n    ),\n    (\n        \"ViMedAQA_SP_EN\",\n        \"Based on the following context and your knowledge, answer the following question in Vietnamese.\",\n        default_instruction_format_en,\n    ),\n]\n\nFIXED_EXPERIMENT_PAIRINGS = [\n    (\"Extract_EN\", \"Expert_SP_EN\"),\n    (\"ViMedAQA\", \"ViMedAQA_SP_VI\"),\n    (\"ViMedAQA\", \"ViMedAQA_SP_EN\"),\n]\n\n# Mode 1: Run all strategies defined in USER_PROMPTS (False)\n# Mode 2: Run only the single, specified strategy for a targeted comparison (True)\nUSE_BEST_PROMPT_ONLY = False\nBEST_STRATEGY_NAME = \"Extract_VI\" # Specify the prompt to use in Mode 2\nPRINT_PROMPT = False\n\nmodel_ids = [\n    \"arcee-ai/Arcee-VyLinh\",\n    # \"vilm/vietcuna-3b-v2\",\n    # \"alpha-ai/LLAMA3-3B-Medical-COT\",\n    \n    # \"vilm/vinallama-2.7b-chat\",\n    # \"sail/Sailor-4B\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:54.987428Z","iopub.execute_input":"2025-09-18T00:59:54.987674Z","iopub.status.idle":"2025-09-18T00:59:55.007459Z","shell.execute_reply.started":"2025-09-18T00:59:54.987653Z","shell.execute_reply":"2025-09-18T00:59:55.006728Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\ngeneration_times = {}\n\ndef create_prompt_and_get_config(model_id, tokenizer, system_prompt, full_instruction_text):\n    \"\"\"\n    A single, unified function to create a model-specific prompt and return\n    the associated answer start tag for parsing.\n\n    Returns:\n        tuple: (formatted_prompt_string, answer_start_tag_string)\n    \"\"\"  \n    # Models WITHOUT a dedicated System Prompt\n    # For these, we combine the system and user prompts into a single instruction.\n    if model_id in [\"vilm/vietcuna-3b-v2\", \"sail/Sailor-4B\"]:\n        # If a system prompt is provided, prepend it.\n        if system_prompt:\n            combined_instruction = f\"{system_prompt}\\n\\n{full_instruction_text}\"\n        # Otherwise, just use the user instruction directly.\n        else:\n            combined_instruction = full_instruction_text\n\n        if model_id == \"vilm/vietcuna-3b-v2\":\n            prompt = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {combined_instruction}\\nASSISTANT:\"\n            answer_start_tag = \"ASSISTANT:\"\n            return prompt, answer_start_tag\n\n        # https://huggingface.co/sail/Sailor-4B\n        if model_id == \"sail/Sailor-4B\":\n            prompt = f\"{combined_instruction}\\n\\nCâu trả lời:\"\n            answer_start_tag = \"\"\n            return prompt, answer_start_tag\n\n    # For chat models, we build the message list programmatically.\n    messages = []\n    # Only add the system role if the system_prompt is not empty.\n    if system_prompt:\n        messages.append({\"role\": \"system\", \"content\": system_prompt})\n    \n    # Add the user/question role\n    if model_id == \"sail/Sailor-4B-Chat\":\n        messages.append({\"role\": \"question\", \"content\": full_instruction_text})\n    else:\n        # Default to \"user\" role for all other chat models\n        messages.append({\"role\": \"user\", \"content\": full_instruction_text})\n    \n    if model_id == \"arcee-ai/Arcee-VyLinh\":\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        answer_start_tag = \"<|im_start|>assistant\"\n        return prompt, answer_start_tag\n\n    if model_id == \"alpha-ai/LLAMA3-3B-Medical-COT\":\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n        return prompt, answer_start_tag\n    \n    # https://huggingface.co/sail/Sailor-4B-Chat\n    if model_id == \"sail/Sailor-4B-Chat\":\n        prompt = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        answer_start_tag = \"answer:\"\n        return prompt, answer_start_tag\n\n    if \"vilm/vinallama-2.7b\" in model_id:\n        # This model's template is custom and doesn't use apply_chat_template,\n        # so we handle it separately.\n        if system_prompt:\n            prompt = (\n                f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n                f\"<|im_start|>user\\n{full_instruction_text}<|im_end|>\\n\"\n                f\"<|im_start|>assistant\"\n            )\n        else:\n            # Version without a system prompt\n            prompt = (\n                f\"<|im_start|>user\\n{full_instruction_text}<|im_end|>\\n\"\n                f\"<|im_start|>assistant\"\n            )\n        answer_start_tag = \"<|im_start|>assistant\"\n        return prompt, answer_start_tag\n\n    # Nothing matches\n    return \"\", \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:55.008510Z","iopub.execute_input":"2025-09-18T00:59:55.008834Z","iopub.status.idle":"2025-09-18T00:59:55.023211Z","shell.execute_reply.started":"2025-09-18T00:59:55.008808Z","shell.execute_reply":"2025-09-18T00:59:55.022435Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\nintermediate_results = {} \n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n\n    wide_results = []\n    for i, sample in enumerate(eval_dataset):\n        wide_results.append({\n            \"Sample_ID\": i,\n            \"Question\": sample['question'],\n            \"Context\": sample['context'],\n            \"Ground_Truth_Answer\": ground_truth_answers[i][0]\n        })\n    \n    # Loop through each model to generate answers\n    for model_id in model_ids:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id}\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config, # <-- PASS THE CONFIG OBJECT HERE\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            experiments_to_run = []\n            used_user_prompts = set()\n            used_system_prompts = set()\n            # Create a dictionary for quick lookups of system prompts\n            system_prompts_map = {sp[0]: sp for sp in SYSTEM_PROMPTS}\n            \n            user_prompts_to_run = list(USER_PROMPTS.keys()) if not USE_BEST_PROMPT_ONLY else [BEST_STRATEGY_NAME]\n\n            # 2. Process your fixed pairings first.\n            print(\"--- Processing Fixed Experiment Pairings ---\")\n            for prompt_name, sp_name in FIXED_EXPERIMENT_PAIRINGS:\n                # Ensure the prompts exist before pairing\n                if prompt_name not in USER_PROMPTS:\n                    print(f\"Warning: Fixed user prompt '{prompt_name}' not found. Skipping.\")\n                    continue\n                if sp_name not in system_prompts_map:\n                    print(f\"Warning: Fixed system prompt '{sp_name}' not found. Skipping.\")\n                    continue\n                \n                # Retrieve details for the system prompt\n                _, system_prompt_content, instruction_format = system_prompts_map[sp_name]\n                \n                # Add the specific pair to the list of experiments\n                experiments_to_run.append((prompt_name, sp_name, system_prompt_content, instruction_format))\n                \n                # Mark these prompts as \"used\" so they aren't paired again\n                used_user_prompts.add(prompt_name)\n                used_system_prompts.add(sp_name)\n                print(f\"  + Added fixed pair: ('{prompt_name}', '{sp_name}')\")\n\n            # 3. Automatically pair the remaining (unused) prompts.\n            print(\"\\\\n--- Processing Automatic Remaining Pairings ---\")\n            for sp_name, system_prompt, instruction_format in SYSTEM_PROMPTS:\n                # If this system prompt was already in a fixed pair, skip it\n                if sp_name in used_system_prompts:\n                    continue\n                \n                # If the format is a template, create a combination for each task instruction\n                if \"{base_instruction}\" in instruction_format:\n                    for prompt_name in user_prompts_to_run:\n                        # Only pair English prompts with English strategies, and VI with VI\n                        if (sp_name.endswith(\"_EN\") and prompt_name.endswith(\"_EN\")) or \\\n                           (sp_name.endswith(\"_VI\") and prompt_name.endswith(\"_VI\")):\n                            experiments_to_run.append((prompt_name, sp_name, system_prompt, instruction_format))\n                # If it's self-contained, add it just once\n                else:\n                    experiments_to_run.append((\"Self-Contained\", sp_name, system_prompt, instruction_format))\n\n            for prompt_name, sp_name, system_prompt, instruction_format in experiments_to_run:\n                print(f\"\\n===== Running Experiment: [Task: {prompt_name}] | [System Prompt: {sp_name}] =====\")\n                # Fetch the base instruction, providing an empty string for the \"Self-Contained\" case\n                base_instruction = USER_PROMPTS.get(prompt_name, \"\")\n                result_key_tuple = (model_id, prompt_name, sp_name)\n                prompts = []\n                answer_start_tag = \"\"\n            \n                for sample in eval_dataset:\n                    # If the base instruction is empty (our special case), use a cleaner format.\n                    if not base_instruction:\n                        if sp_name.endswith(\"_EN\"):\n                            full_instruction_text = (\n                                f\"### Context:\\n{sample['context']}\\n\\n\"\n                                f\"### Question:\\n{sample['question']}\"\n                            )\n                        else: # Default to Vietnamese\n                            full_instruction_text = (\n                                f\"### Ngữ cảnh:\\n{sample['context']}\\n\\n\"\n                                f\"### Câu hỏi:\\n{sample['question']}\"\n                            )\n                    # Otherwise, use the standard template format.\n                    else:\n                        full_instruction_text = instruction_format.format(\n                            base_instruction=base_instruction, \n                            context=sample['context'], \n                            question=sample['question']\n                        )\n                    \n                    prompt, tag = create_prompt_and_get_config(model_id, tokenizer, system_prompt, full_instruction_text)\n                    if PRINT_PROMPT:\n                        print(prompt)\n                    \n                    prompts.append(prompt)\n                    if not answer_start_tag: answer_start_tag = tag\n            \n                if prompts == \"\" and answer_start_tag == \"\":\n                    print(\"No model matches\")\n                    break\n\n                start_time = time.time()\n\n                # print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n                # Generate answers for the entire batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=False,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n                end_time = time.time()\n                generation_time = end_time - start_time\n                generation_times[result_key_tuple] = generation_time # Lưu thời gian đã đo\n    \n                # Extract the clean answers\n                model_answers = []\n                \n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0]['generated_text']\n                    # Check if the tag is not empty AND exists in the text before splitting\n                    if answer_start_tag and answer_start_tag in generated_text:\n                        clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                    else:\n                        # Fallback for base models or if the tag isn't found\n                        clean_answer = generated_text.replace(prompts[i], \"\").strip()\n\n                    if prompt_name == \"Extract_EN\":\n                        redundant_prefix = \"Direct answer:\\n\"\n                        if clean_answer.startswith(redundant_prefix):\n                            clean_answer = clean_answer[len(redundant_prefix):].strip()\n                    \n                    model_answers.append(clean_answer)\n    \n                all_generated_answers[result_key_tuple] = model_answers\n                \n                answer_column_name = f\"{prompt_name}\"\n                for i, answer in enumerate(model_answers):\n                    # The key identifies a unique row in the final table\n                    row_key = (i, model_id, sp_name)\n                    \n                    # If this is the first time we see this row, initialize it with static info\n                    if row_key not in intermediate_results:\n                        intermediate_results[row_key] = {\n                            \"Sample_ID\": i,\n                            \"Question\": eval_dataset[i]['question'],\n                            \"Context\": eval_dataset[i]['context'],\n                            \"Ground_Truth_Answer\": ground_truth_answers[i][0],\n                            \"Model\": model_id,\n                            \"System_Prompt\": sp_name\n                        }\n                    \n                    # Add the generated answer to the correct column for this row\n                    intermediate_results[row_key][answer_column_name] = answer\n\n                print(f\"Time for generating answer: {generation_time:.2f} seconds.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Check if variables were successfully created before deleting\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n\n            import gc\n            gc.collect()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T00:59:55.024132Z","iopub.execute_input":"2025-09-18T00:59:55.024417Z","iopub.status.idle":"2025-09-18T01:00:29.333202Z","shell.execute_reply.started":"2025-09-18T00:59:55.024394Z","shell.execute_reply":"2025-09-18T01:00:29.332413Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nLoading model: arcee-ai/Arcee-VyLinh\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50f0203a6b404fa18821be89206f682d"}},"metadata":{}},{"name":"stdout","text":"--- Processing Fixed Experiment Pairings ---\n  + Added fixed pair: ('Extract_EN', 'Expert_SP_EN')\n  + Added fixed pair: ('ViMedAQA', 'ViMedAQA_SP_VI')\n  + Added fixed pair: ('ViMedAQA', 'ViMedAQA_SP_EN')\n\\n--- Processing Automatic Remaining Pairings ---\n\n===== Running Experiment: [Task: Extract_EN] | [System Prompt: Expert_SP_EN] =====\n<|im_start|>system\nYou are a medical expert AI. Based on your expertise, answer the following Question in Vietnamese, using ONLY the provided Context.<|im_end|>\n<|im_start|>user\nBased on the following Context, extract the direct answer from the text without any additional explanation.\n\n### Context:\nKhi quá liều Alzental Shinpoong, bệnh nhân sẽ có triệu chứng như phản ứng sốc phản vệ, phát ban, rụng tóc nhiều. Nhiều biểu hiện nghiêm trọng hơn nhiều như suy giảm bạch cầu, suy giảm chức năng gan.4 Do đó, hãy đến bệnh viện ngay lập tức và mang theo thuốc để bác sĩ có thể tìm lí do dễ dàng và xử trí kịp thời.\n\n### Question:\nTại sao cần đưa bệnh nhân đến bệnh viện ngay khi dùng quá liều Alzental Shinpoong?<|im_end|>\n<|im_start|>assistant\n\nTime for generating answer: 2.04 seconds.\n\n===== Running Experiment: [Task: ViMedAQA] | [System Prompt: ViMedAQA_SP_VI] =====\n<|im_start|>system\nDựa vào ngữ cảnh sau và kiến thức của bạn, trả lời câu hỏi sau bằng tiếng Việt.<|im_end|>\n<|im_start|>user\n### Ngữ cảnh:\nKhi quá liều Alzental Shinpoong, bệnh nhân sẽ có triệu chứng như phản ứng sốc phản vệ, phát ban, rụng tóc nhiều. Nhiều biểu hiện nghiêm trọng hơn nhiều như suy giảm bạch cầu, suy giảm chức năng gan.4 Do đó, hãy đến bệnh viện ngay lập tức và mang theo thuốc để bác sĩ có thể tìm lí do dễ dàng và xử trí kịp thời.\n\n### Câu hỏi:\nTại sao cần đưa bệnh nhân đến bệnh viện ngay khi dùng quá liều Alzental Shinpoong?<|im_end|>\n<|im_start|>assistant\n\nTime for generating answer: 13.87 seconds.\n\n===== Running Experiment: [Task: ViMedAQA] | [System Prompt: ViMedAQA_SP_EN] =====\n<|im_start|>system\nBased on the following context and your knowledge, answer the following question in Vietnamese.<|im_end|>\n<|im_start|>user\n### Context:\nKhi quá liều Alzental Shinpoong, bệnh nhân sẽ có triệu chứng như phản ứng sốc phản vệ, phát ban, rụng tóc nhiều. Nhiều biểu hiện nghiêm trọng hơn nhiều như suy giảm bạch cầu, suy giảm chức năng gan.4 Do đó, hãy đến bệnh viện ngay lập tức và mang theo thuốc để bác sĩ có thể tìm lí do dễ dàng và xử trí kịp thời.\n\n### Question:\nTại sao cần đưa bệnh nhân đến bệnh viện ngay khi dùng quá liều Alzental Shinpoong?<|im_end|>\n<|im_start|>assistant\n\nTime for generating answer: 8.28 seconds.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# --- Step 5.5 - Assemble and Save Results in Hybrid-Format CSV File ---\nif intermediate_results:\n    print(\"\\n\" + \"=\"*50)\n    print(\"Assembling results into the desired hybrid format...\")\n    print(\"=\"*50)\n    \n    final_results_list = list(intermediate_results.values())\n    results_df_hybrid = pd.DataFrame(final_results_list)\n\n    # --- Define a clear column order for the final CSV ---\n    static_columns = [\n        \"Sample_ID\", \"Model\", \"System_Prompt\", \n        \"Question\", \"Context\", \"Ground_Truth_Answer\"\n    ]\n    \n    # Dynamically create the answer column names based on the strategies that were run\n    user_prompts_to_run = list(USER_PROMPTS.keys()) if not USE_BEST_PROMPT_ONLY else [BEST_STRATEGY_NAME]\n    answer_columns = sorted([f\"{p_name}\" for p_name in user_prompts_to_run])\n    \n    # Add the special \"Self-Contained\" column if it exists in the dataframe\n    if \"Self-Contained\" in results_df_hybrid.columns:\n        answer_columns.append(\"Self-Contained\")\n    \n    # Combine and reorder the DataFrame, handling missing columns gracefully\n    final_column_order = static_columns + answer_columns\n    # Ensure we only try to order by columns that actually exist in the dataframe\n    existing_columns_in_order = [col for col in final_column_order if col in results_df_hybrid.columns]\n    results_df_hybrid = results_df_hybrid[existing_columns_in_order]\n\n    results_df_hybrid = results_df_hybrid.sort_values(by=[\"Sample_ID\", \"Model\", \"System_Prompt\"]).reset_index(drop=True)\n\n    output_file_path = \"/kaggle/working/results.csv\"\n    results_df_hybrid.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n    \n    print(f\"Complete! Saved {len(results_df_hybrid)} rows to the file:\")\n    print(output_file_path)\n    \n    display(results_df_hybrid.head())\n    \nelse:\n    print(\"\\nNo results were generated to save.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:00:29.338797Z","iopub.execute_input":"2025-09-18T01:00:29.339022Z","iopub.status.idle":"2025-09-18T01:00:29.356783Z","shell.execute_reply.started":"2025-09-18T01:00:29.339006Z","shell.execute_reply":"2025-09-18T01:00:29.355895Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nAssembling results into the desired hybrid format...\n==================================================\nComplete! Saved 3 rows to the file:\n/kaggle/working/results.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   Sample_ID                  Model   System_Prompt  \\\n0          0  arcee-ai/Arcee-VyLinh    Expert_SP_EN   \n1          0  arcee-ai/Arcee-VyLinh  ViMedAQA_SP_EN   \n2          0  arcee-ai/Arcee-VyLinh  ViMedAQA_SP_VI   \n\n                                            Question  \\\n0  Tại sao cần đưa bệnh nhân đến bệnh viện ngay k...   \n1  Tại sao cần đưa bệnh nhân đến bệnh viện ngay k...   \n2  Tại sao cần đưa bệnh nhân đến bệnh viện ngay k...   \n\n                                             Context  \\\n0  Khi quá liều Alzental Shinpoong, bệnh nhân sẽ ...   \n1  Khi quá liều Alzental Shinpoong, bệnh nhân sẽ ...   \n2  Khi quá liều Alzental Shinpoong, bệnh nhân sẽ ...   \n\n                                 Ground_Truth_Answer  \\\n0  Việc đưa bệnh nhân đến bệnh viện ngay khi dùng...   \n1  Việc đưa bệnh nhân đến bệnh viện ngay khi dùng...   \n2  Việc đưa bệnh nhân đến bệnh viện ngay khi dùng...   \n\n                                          Extract_EN  \\\n0  Để bác sĩ có thể tìm lí do dễ dàng và xử trí k...   \n1                                                NaN   \n2                                                NaN   \n\n                                            ViMedAQA  \n0                                                NaN  \n1  Khi bệnh nhân dùng quá liều Alzental Shinpoong...  \n2  Khi bệnh nhân dùng quá liều Alzental Shinpoong...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sample_ID</th>\n      <th>Model</th>\n      <th>System_Prompt</th>\n      <th>Question</th>\n      <th>Context</th>\n      <th>Ground_Truth_Answer</th>\n      <th>Extract_EN</th>\n      <th>ViMedAQA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>arcee-ai/Arcee-VyLinh</td>\n      <td>Expert_SP_EN</td>\n      <td>Tại sao cần đưa bệnh nhân đến bệnh viện ngay k...</td>\n      <td>Khi quá liều Alzental Shinpoong, bệnh nhân sẽ ...</td>\n      <td>Việc đưa bệnh nhân đến bệnh viện ngay khi dùng...</td>\n      <td>Để bác sĩ có thể tìm lí do dễ dàng và xử trí k...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>arcee-ai/Arcee-VyLinh</td>\n      <td>ViMedAQA_SP_EN</td>\n      <td>Tại sao cần đưa bệnh nhân đến bệnh viện ngay k...</td>\n      <td>Khi quá liều Alzental Shinpoong, bệnh nhân sẽ ...</td>\n      <td>Việc đưa bệnh nhân đến bệnh viện ngay khi dùng...</td>\n      <td>NaN</td>\n      <td>Khi bệnh nhân dùng quá liều Alzental Shinpoong...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>arcee-ai/Arcee-VyLinh</td>\n      <td>ViMedAQA_SP_VI</td>\n      <td>Tại sao cần đưa bệnh nhân đến bệnh viện ngay k...</td>\n      <td>Khi quá liều Alzental Shinpoong, bệnh nhân sẽ ...</td>\n      <td>Việc đưa bệnh nhân đến bệnh viện ngay khi dùng...</td>\n      <td>NaN</td>\n      <td>Khi bệnh nhân dùng quá liều Alzental Shinpoong...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\n\n# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    # Load all the metrics we need\n    rouge_metric = evaluate.load('rouge')\n    bleu_metric = evaluate.load('bleu')\n    meteor_metric = evaluate.load('meteor')\n    bertscore_metric = evaluate.load('bertscore')\n\n    evaluation_results = []\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        model_name, prompt_name, sp_name = result_key\n        # Check for empty predictions to prevent ZeroDivisionError in BLEU ---\n        # The `any()` function returns False if all strings in the list are empty.\n        if not any(predictions):\n            print(f\"  WARNING: Model & User Prompt '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & User Prompt\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0,\n                \"Avg-Score\": 0.0\n            }\n            evaluation_results.append(result_row)\n            # Use `continue` to skip the rest of the loop and move to the next model\n            continue\n    \n        # If predictions are valid, compute metrics as normal\n        rouge_scores = rouge_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bleu_scores = bleu_metric.compute(predictions=predictions, references=ground_truth_answers)\n        meteor_scores = meteor_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bertscore_scores = bertscore_metric.compute(predictions=predictions, references=ground_truth_answers, lang=\"vi\")\n        \n        # Calculate individual scores for the current result_key\n        rouge_l = round(rouge_scores['rougeL'], 4)\n        bleu = round(bleu_scores['bleu'], 4)\n        meteor = round(meteor_scores['meteor'], 4)\n        bertscore_f1 = round(sum(bertscore_scores['f1']) / len(bertscore_scores['f1']), 4)\n\n        # Calculate the average score\n        avg_score = round((rouge_l + bleu + meteor + bertscore_f1) / 4, 4)\n    \n        # Store results (this part is the same as before)\n        result_row = {\n            \"Model\": model_name,\n            \"User_Prompt\": prompt_name,\n            \"System_Prompt\": sp_name,\n            \"ROUGE-L\": rouge_l,\n            \"BLEU\": bleu,\n            \"METEOR\": meteor,\n            \"BERTScore-F1\": bertscore_f1,\n            \"Avg-Score\": avg_score,\n            \"Generation Time (s)\": round(generation_times.get(result_key, 0), 2),\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    # Sort for better comparison\n    results_df = results_df.sort_values(by=\"Avg-Score\", ascending=False).reset_index(drop=True)\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    display(results_df)\n\n    # Save the evaluation results DataFrame to a separate CSV file\n    evaluation_output_path = \"/kaggle/working/eval_results.csv\"\n    results_df.to_csv(evaluation_output_path, index=False, encoding='utf-8-sig')\n    print(f\"\\nEvaluation results successfully saved to: {evaluation_output_path}\")\n\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:00:29.358830Z","iopub.execute_input":"2025-09-18T01:00:29.359066Z","iopub.status.idle":"2025-09-18T01:00:32.549002Z","shell.execute_reply.started":"2025-09-18T01:00:29.359042Z","shell.execute_reply":"2025-09-18T01:00:32.547798Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nCalculating Evaluation Metrics\n==================================================\n\n--- Comparative Evaluation Results ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                   Model User_Prompt   System_Prompt  ROUGE-L    BLEU  METEOR  \\\n0  arcee-ai/Arcee-VyLinh    ViMedAQA  ViMedAQA_SP_EN   0.4072  0.1475  0.7121   \n1  arcee-ai/Arcee-VyLinh    ViMedAQA  ViMedAQA_SP_VI   0.3235  0.1422  0.6396   \n2  arcee-ai/Arcee-VyLinh  Extract_EN    Expert_SP_EN   0.1519  0.0000  0.0680   \n\n   BERTScore-F1  Avg-Score  Generation Time (s)  \n0        0.8029     0.5174                 8.28  \n1        0.7888     0.4735                13.87  \n2        0.6877     0.2269                 2.04  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>User_Prompt</th>\n      <th>System_Prompt</th>\n      <th>ROUGE-L</th>\n      <th>BLEU</th>\n      <th>METEOR</th>\n      <th>BERTScore-F1</th>\n      <th>Avg-Score</th>\n      <th>Generation Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>arcee-ai/Arcee-VyLinh</td>\n      <td>ViMedAQA</td>\n      <td>ViMedAQA_SP_EN</td>\n      <td>0.4072</td>\n      <td>0.1475</td>\n      <td>0.7121</td>\n      <td>0.8029</td>\n      <td>0.5174</td>\n      <td>8.28</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>arcee-ai/Arcee-VyLinh</td>\n      <td>ViMedAQA</td>\n      <td>ViMedAQA_SP_VI</td>\n      <td>0.3235</td>\n      <td>0.1422</td>\n      <td>0.6396</td>\n      <td>0.7888</td>\n      <td>0.4735</td>\n      <td>13.87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>arcee-ai/Arcee-VyLinh</td>\n      <td>Extract_EN</td>\n      <td>Expert_SP_EN</td>\n      <td>0.1519</td>\n      <td>0.0000</td>\n      <td>0.0680</td>\n      <td>0.6877</td>\n      <td>0.2269</td>\n      <td>2.04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nEvaluation results successfully saved to: /kaggle/working/eval_results.csv\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"    # \"Few_Shot_VI\": (\n    #     f\"Dựa vào các Ví dụ sau đây, hãy trả lời Câu hỏi cuối cùng bằng cách trích xuất thông tin từ Ngữ cảnh được cung cấp.\\n\\n\"\n    #     f\"--- Ví dụ 1 ---\\n\"\n    #     f\"Ngữ cảnh: Thuốc Biviantac được chỉ định để điều trị các trường hợp do tăng tiết acid quá mức như: - Khó tiêu, nóng rát hay đau vùng thượng vị. - Trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua. - Tăng độ acid, đau rát dạ dày. - Các rối loạn thường gặp trong những bệnh lý loét dạ dày tá tràng, thực quản.\\n\"\n    #     f\"Câu hỏi: Biviantac có thể điều trị trướng bụng, đầy hơi không?\\n\"\n    #     f\"Câu trả lời: Có, Biviantac có thể điều trị các tình trạng như trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.\\n\\n\"\n    #     f\"--- Ví dụ 2 ---\\n\"\n    #     f\"Ngữ cảnh: Thuốc Atorvastatin T.V Pharm được dùng đường uống.\\n\"\n    #     f\"Câu hỏi: Tổng hợp các cách dùng hiệu quả để quản lý Atorvastatin T.V Pharm?\\n\"\n    #     f\"Câu trả lời: Các cách thức dùng thuốc Atorvastatin T.V Pharm hiệu quả là sử dụng đường uống.\\n\\n\"\n    #     f\"--- Ví dụ 3 ---\\n\"\n    #     f\"Ngữ cảnh: - Buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột. - Mệt mỏi. - Ban, mày đay. - Thiếu máu tan huyết. - Yếu cơ. - Khó thở, sốc phản vệ.\\n\"\n    #     f\"Câu hỏi: Các tác dụng phụ thường gặp của thuốc Aspirin 81 là gì?\\n\"\n    #     f\"Câu trả lời: Các tác dụng phụ thường gặp của thuốc Aspirin 81 bao gồm buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột.\\n\\n\"\n    #     f\"--- Ví dụ 4 ---\\n\"\n    #     f\"Ngữ cảnh: Các chị em có thể thỉnh thoảng thấy kinh nguyệt ra nhiều hoặc ra máu giữa các kỳ kinh (chảy máu giữa kỳ kinh nguyệt).\\n\"\n    #     f\"Câu hỏi: Các chị em có thể gặp tình trạng rong kinh không?\\n\"\n    #     f\"Câu trả lời: Có.\\n\\n\"\n    #     f\"--- Bây giờ, hãy trả lời Câu hỏi sau dựa trên Ngữ cảnh của nó---\"\n    # ),\n    # \"Full_VI\": (\n    #     f\"Dựa vào Ngữ cảnh sau, hãy trích xuất câu trả lời **đầy đủ và toàn diện nhất** có thể từ văn bản.\"\n    #     f\"Đảm bảo rằng bạn đã bao gồm **tất cả** các điểm có liên quan để trả lời cho câu hỏi.\"\n    # ),\n\n    # \"Full_EN\": (\n    #     f\"Based on the following Context, extract the most **complete and comprehensive** answer possible from the text. \"\n    #     f\"Ensure you have included **all** relevant points to answer the question.\"\n    # ),\n\n    # \"ViMedAQA\": \"\",\n\n    # (\n    #     \"ViMedAQA_SP_VI\",\n    #     \"Dựa vào ngữ cảnh sau và kiến thức của bạn, trả lời câu hỏi sau bằng tiếng Việt\",\n    #     \"### Ngữ cảnh:\\n{context}\\n\\n### Câu hỏi:\\n{question}\",\n    # ),\n    # (\n    #     \"ViMedAQA_SP_EN\",\n    #     \"Based on the following context and your knowledge, answer the following question in Vietnamese.\",\n    #     \"### Context:\\n{context}\\n\\n### Question:\\n{question}\",\n    # ),","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T01:00:32.549731Z","iopub.execute_input":"2025-09-18T01:00:32.549960Z","iopub.status.idle":"2025-09-18T01:00:32.555333Z","shell.execute_reply.started":"2025-09-18T01:00:32.549943Z","shell.execute_reply":"2025-09-18T01:00:32.554291Z"}},"outputs":[],"execution_count":33}]}