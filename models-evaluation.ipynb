{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:21.700040Z","iopub.execute_input":"2025-09-13T13:54:21.700886Z","iopub.status.idle":"2025-09-13T13:54:25.291636Z","shell.execute_reply.started":"2025-09-13T13:54:21.700847Z","shell.execute_reply":"2025-09-13T13:54:25.290811Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate\nimport warnings\nimport time\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import logging\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:25.293724Z","iopub.execute_input":"2025-09-13T13:54:25.294016Z","iopub.status.idle":"2025-09-13T13:54:25.302490Z","shell.execute_reply.started":"2025-09-13T13:54:25.293983Z","shell.execute_reply":"2025-09-13T13:54:25.301546Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:25.303368Z","iopub.execute_input":"2025-09-13T13:54:25.303567Z","iopub.status.idle":"2025-09-13T13:54:25.408256Z","shell.execute_reply.started":"2025-09-13T13:54:25.303553Z","shell.execute_reply":"2025-09-13T13:54:25.407752Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\ndataset_id = \"tmnam20/ViMedAQA\"\nNUM_SAMPLES_TO_EVALUATE = 1\nmodel_ids = [\n    # \"alpha-ai/LLAMA3-3B-Medical-COT\",\n    # \"vilm/vietcuna-3b-v2\",\n    # \"arcee-ai/Arcee-VyLinh\",\n]\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    # Create a small, random, representative sample for evaluation\n    random.seed(42) # for reproducibility\n    random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_TO_EVALUATE)\n    eval_dataset = dataset.select(random_indices)\n\n    print(f\"Created a random evaluation set with {len(eval_dataset)} samples.\")\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:25.408938Z","iopub.execute_input":"2025-09-13T13:54:25.409203Z","iopub.status.idle":"2025-09-13T13:54:26.735170Z","shell.execute_reply.started":"2025-09-13T13:54:25.409187Z","shell.execute_reply":"2025-09-13T13:54:26.734522Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully! Total samples: 39881\nCreated a random evaluation set with 10 samples.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"PROMPT_STRATEGIES = {\n    # \"Direct_VI\": \"Sử dụng Ngữ cảnh sau để trả lời Câu hỏi.\",\n    # \"RolePlay_VI\": \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\n    # \"Current_Best_VI\": (\n    #     \"Bạn là một chuyên gia y tế AI với nhiệm vụ trích xuất thông tin chính xác. \"\n    #     \"Dựa CHỈ vào văn bản trong phần Ngữ cảnh dưới đây, hãy trả lời cho Câu hỏi. \"\n    #     \"Câu trả lời của bạn phải ngắn gọn, đi thẳng vào vấn đề và không chứa bất kỳ thông tin nào không có trong văn bản. \"\n    #     \"Không giải thích thêm.\"\n    # ),\n    # \"Chain_of_Thought_VI\": (\n    #     \"Dựa vào Ngữ cảnh sau, hãy suy nghĩ từng bước một để đưa ra câu trả lời cho Câu hỏi. \"\n    #     \"Hãy trình bày rõ ràng các bước suy luận của bạn.\"\n    # ),\n    # \"Few_Shot_VI_ViMedAQA\": (\n    #     \"Dựa vào các Ví dụ sau đây, hãy trả lời Câu hỏi cuối cùng bằng cách trích xuất thông tin từ Ngữ cảnh được cung cấp.\\n\\n\"\n    #     \"--- Ví dụ 1 ---\\n\"\n    #     \"Ngữ cảnh: Thuốc Biviantac được chỉ định để điều trị các trường hợp do tăng tiết acid quá mức như: - Khó tiêu, nóng rát hay đau vùng thượng vị. - Trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua. - Tăng độ acid, đau rát dạ dày. - Các rối loạn thường gặp trong những bệnh lý loét dạ dày tá tràng, thực quản.\\n\"\n    #     \"Câu hỏi: Biviantac có thể điều trị trướng bụng, đầy hơi không?\\n\"\n    #     \"Câu trả lời: Có, Biviantac có thể điều trị các tình trạng như trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.\\n\\n\"\n    #     \"--- Ví dụ 2 ---\\n\"\n    #     \"Ngữ cảnh: Thuốc Atorvastatin T.V Pharm được dùng đường uống.\\n\"\n    #     \"Câu hỏi: Tổng hợp các cách dùng hiệu quả để quản lý Atorvastatin T.V Pharm?\\n\"\n    #     \"Câu trả lời: Các cách thức dùng thuốc Atorvastatin T.V Pharm hiệu quả là sử dụng đường uống.\\n\\n\"\n    #     \"--- Ví dụ 3 ---\\n\"\n    #     \"Ngữ cảnh: - Buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột. - Mệt mỏi. - Ban, mày đay. - Thiếu máu tan huyết. - Yếu cơ. - Khó thở, sốc phản vệ.\\n\"\n    #     \"Câu hỏi: Các tác dụng phụ thường gặp của thuốc Aspirin 81 là gì?\\n\"\n    #     \"Câu trả lời: Các tác dụng phụ thường gặp của thuốc Aspirin 81 bao gồm buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột.\\n\\n\"\n    #     \"--- Bây giờ, hãy trả lời Câu hỏi sau dựa trên Ngữ cảnh của nó---\\n\"\n    # ),\n    # \"Expert_Persona_VI\": (\n    #     \"Bạn là một chuyên gia trong lĩnh vực y tế.\"\n    #     \"Dựa trên kiến thức chuyên môn của mình, hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\"\n    # ),\n    \"Full\": (\n        \"Dựa vào Ngữ cảnh sau, hãy trích xuất câu trả lời **đầy đủ và toàn diện nhất** có thể từ văn bản.\"\n        \"Đảm bảo rằng bạn đã bao gồm **tất cả** các điểm có liên quan để trả lời cho câu hỏi.\"\n    ),\n    \"List\": (\n        \"Từ Ngữ cảnh được cung cấp, hãy **liệt kê tất cả** các thông tin dùng để trả lời cho Câu hỏi.\"\n        \"Trình bày câu trả lời một cách ngắn gọn, chỉ bao gồm các điểm được tìm thấy.\"\n    ),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:26.736908Z","iopub.execute_input":"2025-09-13T13:54:26.737133Z","iopub.status.idle":"2025-09-13T13:54:26.743006Z","shell.execute_reply.started":"2025-09-13T13:54:26.737115Z","shell.execute_reply":"2025-09-13T13:54:26.742239Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\n# Mode 1: Run all strategies defined in PROMPT_STRATEGIES (False)\n# Mode 2: Run only the single, specified strategy for a targeted comparison (True)\nUSE_BEST_PROMPT_ONLY = True\nBEST_STRATEGY_NAME = \"Extract_VI\" # Specify the prompt to use in Mode 2\n# \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\ngeneration_times = {}\n\ndef create_prompt(sample, model_id, tokenizer, strategy_name):\n    \"\"\"\n    A single, unified function to create a fairly-compared prompt.\n\n    It takes a strategy name, finds the corresponding instruction text from the\n    central dictionary, and then wraps it in the correct format for the target model.\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n\n    # 1. Get the unified instruction text for the chosen strategy\n    base_instruction = PROMPT_STRATEGIES.get(strategy_name)\n    if not base_instruction:\n        raise ValueError(f\"Strategy '{strategy_name}' not found in PROMPT_STRATEGIES dictionary.\")\n\n    full_instruction_text = f\"{base_instruction}\\n\\nNgữ cảnh: {context}\\n\\nCâu hỏi: {question}\"\n\n    # 2. Apply the correct, model-specific formatting\n    if 'vietcuna' in model_id:\n        return f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {full_instruction_text}\\nASSISTANT:\"\n        # return f\"Một cuộc trò chuyện giữa một người dùng tò mò và một trợ lý trí tuệ nhân tạo.\\nUSER: {full_instruction_text}\\nASSISTANT:\"\n\n    elif 'Arcee-VyLinh' in model_id or 'LLAMA3' in model_id:\n        messages = [\n            {\"role\": \"system\", \"content\": \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi của người dùng CHỈ dựa vào Ngữ cảnh được cung cấp.\"},\n            {\"role\": \"user\", \"content\": full_instruction_text}\n        ]\n        return tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n    else: \n        # Default to Llama 3 format\n        # return f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{full_instruction_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n        print(\"ERROR: No model matches\")\n        return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:26.743895Z","iopub.execute_input":"2025-09-13T13:54:26.744176Z","iopub.status.idle":"2025-09-13T13:54:26.761878Z","shell.execute_reply.started":"2025-09-13T13:54:26.744152Z","shell.execute_reply":"2025-09-13T13:54:26.761237Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n\n    wide_results = []\n    for i, sample in enumerate(eval_dataset):\n        wide_results.append({\n            \"Sample_ID\": i,\n            \"Question\": sample['question'],\n            \"Context\": sample['context'],\n            \"Ground_Truth_Answer\": ground_truth_answers[i][0]\n        })\n    \n    # Loop through each model to generate answers\n    for model_id in model_ids:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id}\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config, # <-- PASS THE CONFIG OBJECT HERE\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            # Determine which set of prompt functions to use\n            if 'vietcuna' in model_id:\n                answer_start_tag = \"ASSISTANT:\"\n            elif 'Arcee-VyLinh' in model_id:\n                answer_start_tag = \"<|im_start|>assistant\"\n            else: # Default to Llama3\n                answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n\n            prompt_variations = []\n            if USE_BEST_PROMPT_ONLY:\n                # Mode 2: Run only the single specified prompt strategy\n                prompt_variations = [BEST_STRATEGY_NAME]\n                print(f\"Mode: Best Prompt Only. Running with the fair strategy: '{BEST_STRATEGY_NAME}'\")\n            else:\n                # Mode 1: Run all defined prompt strategies for a full exploration\n                prompt_variations = list(PROMPT_STRATEGIES.keys())\n                print(f\"Mode: Exploration. Running all {len(prompt_variations)} fair strategies: {prompt_variations}\")\n\n                \n            for prompt_name in prompt_variations:\n                print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n                # Create a unique key for each result set\n                result_key = f\"{model_id} ({prompt_name})\"\n                \n                # Generate all prompts using the single, fair prompt factory function\n                prompts = [create_prompt(sample, model_id, tokenizer, prompt_name) for sample in eval_dataset]\n                # print(prompts)\n\n                start_time = time.time()\n\n                # print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n                # Generate answers for the entire batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=False,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n                end_time = time.time()\n                generation_time = end_time - start_time\n                generation_times[result_key] = generation_time # Lưu thời gian đã đo\n    \n                # Extract the clean answers\n                model_answers = []\n                \n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0]['generated_text']\n                    # Use the appropriate start tag for splitting\n                    if answer_start_tag in generated_text:\n                        clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                    else:\n                        # Fallback remains the same\n                        clean_answer = generated_text.replace(prompts[i], \"\").strip()\n                    model_answers.append(clean_answer)\n    \n                # Use the unique result_key to store the answers\n                all_generated_answers[result_key] = model_answers\n                # print(f\"Successfully generated answers for {result_key}.\")\n\n                # --- Thêm các câu trả lời đã tạo như một cột mới vào cấu trúc \"rộng\" ---\n                # Tên cột sẽ là nhãn của prompt, ví dụ: \"Answer_vilm/vietcuna-3b-v2 (Current_Best_VI)\"\n                answer_column_name = f\"Answer_{result_key}\"\n                \n                # Lặp qua các câu trả lời và thêm chúng vào đúng hàng trong wide_results\n                for i in range(len(model_answers)):\n                    wide_results[i][answer_column_name] = model_answers[i]\n\n                print(f\"Time for generating answer: {generation_time:.2f} seconds.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Check if variables were successfully created before deleting\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n\n            import gc\n            gc.collect()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:54:26.762890Z","iopub.execute_input":"2025-09-13T13:54:26.763137Z","iopub.status.idle":"2025-09-13T13:55:52.749107Z","shell.execute_reply.started":"2025-09-13T13:54:26.763122Z","shell.execute_reply":"2025-09-13T13:55:52.748510Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nLoading model: vilm/vietcuna-3b-v2\n==================================================\nMode: Exploration. Running all 2 fair strategies: ['Full', 'List']\n\n--- Prompt Strategy: Full ---\nTime for generating answer: 38.35 seconds.\n\n--- Prompt Strategy: List ---\nTime for generating answer: 37.02 seconds.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# --- Bước 5.5 - Lưu tất cả các câu trả lời đã tạo vào một tệp CSV ---\nif wide_results:\n    print(\"\\n\" + \"=\"*50)\n    print(\"Đang lưu kết quả định dạng rộng vào tệp CSV...\")\n    print(\"=\"*50)\n    \n    # Chuyển đổi danh sách kết quả thành một DataFrame của pandas\n    results_df_wide = pd.DataFrame(wide_results)\n    \n    # Chỉ định đường dẫn tệp đầu ra trong thư mục làm việc của Kaggle\n    output_file_path = \"/kaggle/working/results.csv\"\n    \n    # Lưu DataFrame vào tệp CSV\n    results_df_wide.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n    \n    print(f\"Hoàn tất! Đã lưu {len(results_df_wide)} hàng (mẫu) vào tệp:\")\n    print(output_file_path)\n    \n    # Hiển thị 5 hàng đầu tiên của tệp đã lưu để xem trước\n    # Bạn sẽ thấy các cột câu trả lời khác nhau cho mỗi chiến lược prompt\n    # display(results_df_wide.head())\n    \nelse:\n    print(\"\\nKhông có kết quả nào để lưu.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:55:52.751426Z","iopub.execute_input":"2025-09-13T13:55:52.751627Z","iopub.status.idle":"2025-09-13T13:55:52.759363Z","shell.execute_reply.started":"2025-09-13T13:55:52.751612Z","shell.execute_reply":"2025-09-13T13:55:52.758706Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nĐang lưu kết quả định dạng rộng vào tệp CSV...\n==================================================\nHoàn tất! Đã lưu 10 hàng (mẫu) vào tệp:\n/kaggle/working/results.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    # Load all the metrics we need\n    rouge_metric = evaluate.load('rouge')\n    bleu_metric = evaluate.load('bleu')\n    meteor_metric = evaluate.load('meteor')\n    bertscore_metric = evaluate.load('bertscore')\n\n    evaluation_results = []\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        print(f\"\\n--- Evaluating {result_key} ---\")\n    \n        # Check for empty predictions to prevent ZeroDivisionError in BLEU ---\n        # The `any()` function returns False if all strings in the list are empty.\n        if not any(predictions):\n            print(f\"  WARNING: Model & Prompt Strategy '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & Prompt Strategy\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0\n            }\n            evaluation_results.append(result_row)\n            # Use `continue` to skip the rest of the loop and move to the next model\n            continue\n    \n        # If predictions are valid, compute metrics as normal\n        rouge_scores = rouge_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bleu_scores = bleu_metric.compute(predictions=predictions, references=ground_truth_answers)\n        meteor_scores = meteor_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bertscore_scores = bertscore_metric.compute(predictions=predictions, references=ground_truth_answers, lang=\"vi\")\n    \n        # Store results (this part is the same as before)\n        result_row = {\n            \"Model & Prompt Strategy\": result_key,\n            \"ROUGE-L\": round(rouge_scores['rougeL'], 4),\n            \"BLEU\": round(bleu_scores['bleu'], 4),\n            \"METEOR\": round(meteor_scores['meteor'], 4),\n            \"BERTScore-F1\": round(sum(bertscore_scores['f1']) / len(bertscore_scores['f1']), 4),\n            \"Generation Time (s)\": round(generation_times.get(result_key, 0), 2), # Lấy thời gian đã lưu\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    # Sort for better comparison\n    results_df = results_df.sort_values(by=\"BERTScore-F1\", ascending=False).reset_index(drop=True)\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    display(results_df)\n\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T13:55:52.760314Z","iopub.execute_input":"2025-09-13T13:55:52.760559Z","iopub.status.idle":"2025-09-13T13:55:56.808467Z","shell.execute_reply.started":"2025-09-13T13:55:52.760538Z","shell.execute_reply":"2025-09-13T13:55:56.807508Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nCalculating Evaluation Metrics\n==================================================\n\n--- Evaluating vilm/vietcuna-3b-v2 (Full) ---\n\n--- Evaluating vilm/vietcuna-3b-v2 (List) ---\n\n--- Comparative Evaluation Results ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      Model & Prompt Strategy  ROUGE-L    BLEU  METEOR  BERTScore-F1  \\\n0  vilm/vietcuna-3b-v2 (Full)   0.4400  0.1614  0.5258        0.8098   \n1  vilm/vietcuna-3b-v2 (List)   0.4316  0.1565  0.5010        0.8074   \n\n   Generation Time (s)  \n0                38.35  \n1                37.02  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model &amp; Prompt Strategy</th>\n      <th>ROUGE-L</th>\n      <th>BLEU</th>\n      <th>METEOR</th>\n      <th>BERTScore-F1</th>\n      <th>Generation Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>vilm/vietcuna-3b-v2 (Full)</td>\n      <td>0.4400</td>\n      <td>0.1614</td>\n      <td>0.5258</td>\n      <td>0.8098</td>\n      <td>38.35</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>vilm/vietcuna-3b-v2 (List)</td>\n      <td>0.4316</td>\n      <td>0.1565</td>\n      <td>0.5010</td>\n      <td>0.8074</td>\n      <td>37.02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26}]}