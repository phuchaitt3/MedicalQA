{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:48:18.007862Z","iopub.execute_input":"2025-09-15T06:48:18.008360Z","iopub.status.idle":"2025-09-15T06:48:22.388247Z","shell.execute_reply.started":"2025-09-15T06:48:18.008317Z","shell.execute_reply":"2025-09-15T06:48:22.387139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate\nimport warnings\nimport time\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import logging\nlogging.set_verbosity_error()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:48:22.390208Z","iopub.execute_input":"2025-09-15T06:48:22.390458Z","iopub.status.idle":"2025-09-15T06:48:22.398586Z","shell.execute_reply.started":"2025-09-15T06:48:22.390438Z","shell.execute_reply":"2025-09-15T06:48:22.397830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:48:22.399449Z","iopub.execute_input":"2025-09-15T06:48:22.399691Z","iopub.status.idle":"2025-09-15T06:48:22.590257Z","shell.execute_reply.started":"2025-09-15T06:48:22.399666Z","shell.execute_reply":"2025-09-15T06:48:22.589718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\ndataset_id = \"tmnam20/ViMedAQA\"\nseed_num = 42\nNUM_SAMPLES_INITIAL = 200\n\n# --- ADDED: Boolean toggle for the second randomization ---\n# Set to True to get the final 50 samples.\n# Set to False to use the initial 200 samples.\nENABLE_SUBSET_SAMPLING = True\nNUM_SAMPLES_FINAL = 50\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    # --- First Sampling Step: Always get the initial 200 samples ---\n    random.seed(seed_num) # for reproducibility\n    initial_random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_INITIAL)\n    initial_eval_dataset = dataset.select(initial_random_indices)\n\n    print(f\"Created an initial random evaluation set with {len(initial_eval_dataset)} samples.\")\n\n    # --- ADDED: Conditional second randomization ---\n    if ENABLE_SUBSET_SAMPLING:\n        print(\"Subset sampling is ENABLED. Performing second randomization...\")\n        # Re-seed to ensure this step is also reproducible\n        random.seed(seed_num)\n        final_random_indices = random.sample(range(len(initial_eval_dataset)), NUM_SAMPLES_FINAL)\n        # Final dataset is the smaller, 50-sample subset\n        eval_dataset = initial_eval_dataset.select(final_random_indices)\n        print(f\"Further randomized and reduced the set to a final size of {len(eval_dataset)} samples.\")\n    else:\n        print(\"Subset sampling is DISABLED.\")\n        # Final dataset is the larger, 200-sample set\n        eval_dataset = initial_eval_dataset\n        print(f\"Using the initial set of {len(eval_dataset)} samples for evaluation.\")\n\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:10:13.203258Z","iopub.execute_input":"2025-09-15T08:10:13.203601Z","iopub.status.idle":"2025-09-15T08:10:14.973880Z","shell.execute_reply.started":"2025-09-15T08:10:13.203577Z","shell.execute_reply":"2025-09-15T08:10:14.972876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PROMPT_STRATEGIES = {\n    \"Direct_VI\": \"Sử dụng Ngữ cảnh sau để trả lời Câu hỏi.\",\n    \"RolePlay_VI\": \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi CHỈ dựa vào Ngữ cảnh được cung cấp.\",\n    \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\n    \"Current_Best_VI\": (\n        \"Bạn là một chuyên gia y tế AI với nhiệm vụ trích xuất thông tin chính xác. \"\n        \"Dựa CHỈ vào văn bản trong phần Ngữ cảnh dưới đây, hãy trả lời cho Câu hỏi. \"\n        \"Câu trả lời của bạn phải ngắn gọn, đi thẳng vào vấn đề và không chứa bất kỳ thông tin nào không có trong văn bản. \"\n        \"Không giải thích thêm.\"\n    ),\n    \"Few_Shot_VI\": (\n        \"Dựa vào các Ví dụ sau đây, hãy trả lời Câu hỏi cuối cùng bằng cách trích xuất thông tin từ Ngữ cảnh được cung cấp.\\n\\n\"\n        \"--- Ví dụ 1 ---\\n\"\n        \"Ngữ cảnh: Thuốc Biviantac được chỉ định để điều trị các trường hợp do tăng tiết acid quá mức như: - Khó tiêu, nóng rát hay đau vùng thượng vị. - Trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua. - Tăng độ acid, đau rát dạ dày. - Các rối loạn thường gặp trong những bệnh lý loét dạ dày tá tràng, thực quản.\\n\"\n        \"Câu hỏi: Biviantac có thể điều trị trướng bụng, đầy hơi không?\\n\"\n        \"Câu trả lời: Có, Biviantac có thể điều trị các tình trạng như trướng bụng, đầy hơi, ợ nóng, ợ hơi hay ợ chua.\\n\\n\"\n        \"--- Ví dụ 2 ---\\n\"\n        \"Ngữ cảnh: Thuốc Atorvastatin T.V Pharm được dùng đường uống.\\n\"\n        \"Câu hỏi: Tổng hợp các cách dùng hiệu quả để quản lý Atorvastatin T.V Pharm?\\n\"\n        \"Câu trả lời: Các cách thức dùng thuốc Atorvastatin T.V Pharm hiệu quả là sử dụng đường uống.\\n\\n\"\n        \"--- Ví dụ 3 ---\\n\"\n        \"Ngữ cảnh: - Buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột. - Mệt mỏi. - Ban, mày đay. - Thiếu máu tan huyết. - Yếu cơ. - Khó thở, sốc phản vệ.\\n\"\n        \"Câu hỏi: Các tác dụng phụ thường gặp của thuốc Aspirin 81 là gì?\\n\"\n        \"Câu trả lời: Các tác dụng phụ thường gặp của thuốc Aspirin 81 bao gồm buồn nôn, nôn, khó tiêu, khó chịu ở thượng vị, ợ nóng, đau dạ dày, loét dạ dày – ruột.\\n\\n\"\n        \"--- Ví dụ 4 ---\\n\"\n        \"Ngữ cảnh: Các chị em có thể thỉnh thoảng thấy kinh nguyệt ra nhiều hoặc ra máu giữa các kỳ kinh (chảy máu giữa kỳ kinh nguyệt).\\n\"\n        \"Câu hỏi: Các chị em có thể gặp tình trạng rong kinh không?\\n\"\n        \"Câu trả lời: Có.\\n\\n\"\n        \"--- Bây giờ, hãy trả lời Câu hỏi sau dựa trên Ngữ cảnh của nó---\"\n    ),\n    \"Expert_Persona_VI\": (\n        \"Bạn là một chuyên gia trong lĩnh vực y tế.\"\n        \"Dựa trên kiến thức chuyên môn của mình, hãy trả lời Câu hỏi sau CHỈ dựa vào Ngữ cảnh được cung cấp.\"\n    ),\n    \"Full_VI\": (\n        \"Dựa vào Ngữ cảnh sau, hãy trích xuất câu trả lời **đầy đủ và toàn diện nhất** có thể từ văn bản.\"\n        \"Đảm bảo rằng bạn đã bao gồm **tất cả** các điểm có liên quan để trả lời cho câu hỏi.\"\n    ),\n    \"List_VI\": (\n        \"Từ Ngữ cảnh được cung cấp, hãy **liệt kê tất cả** các thông tin dùng để trả lời cho Câu hỏi.\"\n        \"Trình bày câu trả lời một cách ngắn gọn, chỉ bao gồm các điểm được tìm thấy.\"\n    ),\n    \"No_Verbose_VI\": (\n        f\"TỪ Ngữ cảnh, TRÍCH XUẤT câu trả lời cho Câu hỏi.\\n\\n\"\n        f\"**QUY TẮC:**\\n\"\n        f\"1. CHỈ sử dụng thông tin từ Ngữ cảnh.\\n\"\n        f\"2. KHÔNG giải thích các bước của bạn.\\n\"\n        f\"3. KHÔNG tự suy luận hoặc thêm bất kỳ thông tin bên ngoài nào.\\n\"\n        f\"4. Cung cấp câu trả lời được trích xuất trực tiếp.\\n\\n\"\n        f\"Dưới đây là Ngữ cảnh và Câu hỏi, hãy đưa câu trả lời TRÍCH XUẤT:\"\n    ),\n}\n\n# Mode 1: Run all strategies defined in PROMPT_STRATEGIES (False)\n# Mode 2: Run only the single, specified strategy for a targeted comparison (True)\nUSE_BEST_PROMPT_ONLY = False\nBEST_STRATEGY_NAME = \"Extract_VI\" # Specify the prompt to use in Mode 2\n# \"Extract_VI\": \"Dựa vào Ngữ cảnh sau, trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\",\n\nmodel_ids = [\n    # \"alpha-ai/LLAMA3-3B-Medical-COT\",\n    \"vilm/vietcuna-3b-v2\",\n    # \"arcee-ai/Arcee-VyLinh\",\n    # \"sail/Sailor-4B\",\n    \"vilm/vinallama-2.7b-chat\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:48:24.093933Z","iopub.execute_input":"2025-09-15T06:48:24.094124Z","iopub.status.idle":"2025-09-15T06:48:24.101678Z","shell.execute_reply.started":"2025-09-15T06:48:24.094110Z","shell.execute_reply":"2025-09-15T06:48:24.101025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\ngeneration_times = {}\n\nstandard_system_prompt = \"Bạn là một trợ lý y tế hữu ích. Hãy trả lời Câu hỏi của người dùng CHỈ dựa vào Ngữ cảnh được cung cấp.\"\n\ndef create_prompt_and_get_config(sample, model_id, tokenizer, strategy_name):\n    \"\"\"\n    A single, unified function to create a model-specific prompt and return\n    the associated answer start tag for parsing.\n\n    Returns:\n        tuple: (formatted_prompt_string, answer_start_tag_string)\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n\n    # 1. Get the unified instruction text for the chosen strategy\n    base_instruction = PROMPT_STRATEGIES.get(strategy_name)\n    if not base_instruction:\n        raise ValueError(f\"Strategy '{strategy_name}' not found in PROMPT_STRATEGIES dictionary.\")\n\n    full_instruction_text = f\"{base_instruction}\\\\n\\\\nNgữ cảnh: {context}\\\\n\\\\nCâu hỏi: {question}\"\n\n    # 2. Apply the correct, model-specific formatting and define the answer tag\n    if model_id == \"vilm/vietcuna-3b-v2\":\n        prompt = f\"A chat between a curious user and an artificial intelligence assistant.\\\\nUSER: {full_instruction_text}\\\\nASSISTANT:\"\n        answer_start_tag = \"ASSISTANT:\"\n        return prompt, answer_start_tag\n\n    if model_id == \"arcee-ai/Arcee-VyLinh\":\n        messages = [\n            {\"role\": \"system\", \"content\": standard_system_prompt},\n            {\"role\": \"user\", \"content\": full_instruction_text}\n        ]\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        answer_start_tag = \"<|im_start|>assistant\"\n        return prompt, answer_start_tag\n\n    if model_id == \"alpha-ai/LLAMA3-3B-Medical-COT\":\n        messages = [\n            {\"role\": \"system\", \"content\": standard_system_prompt},\n            {\"role\": \"user\", \"content\": full_instruction_text}\n        ]\n        prompt = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n        return prompt, answer_start_tag\n\n    # https://huggingface.co/sail/Sailor-4B\n    if model_id == \"sail/Sailor-4B\":\n        prompt = f\"{full_instruction_text}\\\\n\\\\nCâu trả lời:\"\n        answer_start_tag = \"\"\n        return prompt, answer_start_tag\n    \n    # https://huggingface.co/sail/Sailor-4B-Chat\n    if model_id == \"sail/Sailor-4B-Chat\":\n        messages = [\n            {\"role\": \"system\", \"content\": standard_system_prompt},\n            # IMPORTANT: Sailor examples use role 'question' instead of 'user'\n            {\"role\": \"question\", \"content\": full_instruction_text},\n        ]\n        prompt = tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        answer_start_tag = \"answer:\"\n        return prompt, answer_start_tag\n\n    if \"vilm/vinallama-2.7b\" in model_id:\n        # system_prompt = \"Bạn là một trợ lí AI hữu ích. Hãy trả lời người dùng một cách chính xác.\"\n        system_prompt = standard_system_prompt\n        # Construct the final prompt string using the specified template\n        prompt = (\n            f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n            f\"<|im_start|>user\\n{full_instruction_text}<|im_end|>\\n\"\n            f\"<|im_start|>assistant\"\n        )\n\n        # The answer_start_tag is the exact string that precedes the model's response\n        answer_start_tag = \"<|im_start|>assistant\"\n\n        return prompt, answer_start_tag\n\n    # Nothing matches\n    return \"\", \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:48:24.102588Z","iopub.execute_input":"2025-09-15T06:48:24.102847Z","iopub.status.idle":"2025-09-15T06:48:24.216538Z","shell.execute_reply.started":"2025-09-15T06:48:24.102820Z","shell.execute_reply":"2025-09-15T06:48:24.215840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n\n    wide_results = []\n    for i, sample in enumerate(eval_dataset):\n        wide_results.append({\n            \"Sample_ID\": i,\n            \"Question\": sample['question'],\n            \"Context\": sample['context'],\n            \"Ground_Truth_Answer\": ground_truth_answers[i][0]\n        })\n    \n    # Loop through each model to generate answers\n    for model_id in model_ids:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id}\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config, # <-- PASS THE CONFIG OBJECT HERE\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            prompt_variations = []\n            if USE_BEST_PROMPT_ONLY:\n                # Mode 2: Run only the single specified prompt strategy\n                prompt_variations = [BEST_STRATEGY_NAME]\n                print(f\"Mode: Best Prompt Only. Running with the fair strategy: '{BEST_STRATEGY_NAME}'\")\n            else:\n                # Mode 1: Run all defined prompt strategies for a full exploration\n                prompt_variations = list(PROMPT_STRATEGIES.keys())\n                print(f\"Mode: Exploration. Running all {len(prompt_variations)} strategies: {prompt_variations}\")\n\n                \n            for prompt_name in prompt_variations:\n                print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n                # Create a unique key for each result set\n                result_key = f\"{model_id} ({prompt_name})\"\n                \n                # Generate prompts and get model config from our new unified function\n                prompt_data = [create_prompt_and_get_config(sample, model_id, tokenizer, prompt_name) for sample in eval_dataset]\n                prompts = [p[0] for p in prompt_data]\n                # if model_id == \"sail/Sailor-4B-Chat\":\n                #     prompts = [\"Hãy cho tôi một giới thiệu ngắn gọn về mô hình ngôn ngữ lớn.\"]\n                answer_start_tag = prompt_data[0][1] # Get tag from the first generated prompt\n                if prompts == \"\" and answer_start_tag == \"\":\n                    print(\"No model matches\")\n                    break\n\n                start_time = time.time()\n\n                # print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n                # Generate answers for the entire batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=False,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n                end_time = time.time()\n                generation_time = end_time - start_time\n                generation_times[result_key] = generation_time # Lưu thời gian đã đo\n    \n                # Extract the clean answers\n                model_answers = []\n                \n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0]['generated_text']\n                    # Check if the tag is not empty AND exists in the text before splitting\n                    if answer_start_tag and answer_start_tag in generated_text:\n                        clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                    else:\n                        # Fallback for base models or if the tag isn't found\n                        clean_answer = generated_text.replace(prompts[i], \"\").strip()\n                    model_answers.append(clean_answer)\n    \n                # Use the unique result_key to store the answers\n                all_generated_answers[result_key] = model_answers\n                # print(f\"Successfully generated answers for {result_key}.\")\n\n                # --- Thêm các câu trả lời đã tạo như một cột mới vào cấu trúc \"rộng\" ---\n                # Tên cột sẽ là nhãn của prompt, ví dụ: \"Answer_vilm/vietcuna-3b-v2 (Current_Best_VI)\"\n                answer_column_name = f\"Answer_{result_key}\"\n                \n                # Lặp qua các câu trả lời và thêm chúng vào đúng hàng trong wide_results\n                for i in range(len(model_answers)):\n                    wide_results[i][answer_column_name] = model_answers[i]\n\n                print(f\"Time for generating answer: {generation_time:.2f} seconds.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Check if variables were successfully created before deleting\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n\n            import gc\n            gc.collect()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:48:24.217525Z","iopub.execute_input":"2025-09-15T06:48:24.217717Z","iopub.status.idle":"2025-09-15T07:37:06.735048Z","shell.execute_reply.started":"2025-09-15T06:48:24.217703Z","shell.execute_reply":"2025-09-15T07:37:06.734464Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Bước 5.5 - Lưu tất cả các câu trả lời đã tạo vào một tệp CSV ---\nif wide_results:\n    print(\"\\n\" + \"=\"*50)\n    print(\"Đang lưu kết quả định dạng rộng vào tệp CSV...\")\n    print(\"=\"*50)\n    \n    # Chuyển đổi danh sách kết quả thành một DataFrame của pandas\n    results_df_wide = pd.DataFrame(wide_results)\n    \n    # Chỉ định đường dẫn tệp đầu ra trong thư mục làm việc của Kaggle\n    output_file_path = \"/kaggle/working/results.csv\"\n    \n    # Lưu DataFrame vào tệp CSV\n    results_df_wide.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n    \n    print(f\"Hoàn tất! Đã lưu {len(results_df_wide)} hàng (mẫu) vào tệp:\")\n    print(output_file_path)\n    \n    # Hiển thị 5 hàng đầu tiên của tệp đã lưu để xem trước\n    # Bạn sẽ thấy các cột câu trả lời khác nhau cho mỗi chiến lược prompt\n    display(results_df_wide.head())\n    \nelse:\n    print(\"\\nKhông có kết quả nào để lưu.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:37:06.741895Z","iopub.execute_input":"2025-09-15T07:37:06.742140Z","iopub.status.idle":"2025-09-15T07:37:06.804082Z","shell.execute_reply.started":"2025-09-15T07:37:06.742124Z","shell.execute_reply":"2025-09-15T07:37:06.803402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    # Load all the metrics we need\n    rouge_metric = evaluate.load('rouge')\n    bleu_metric = evaluate.load('bleu')\n    meteor_metric = evaluate.load('meteor')\n    bertscore_metric = evaluate.load('bertscore')\n\n    evaluation_results = []\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        print(f\"\\n--- Evaluating {result_key} ---\")\n    \n        # Check for empty predictions to prevent ZeroDivisionError in BLEU ---\n        # The `any()` function returns False if all strings in the list are empty.\n        if not any(predictions):\n            print(f\"  WARNING: Model & Prompt Strategy '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & Prompt Strategy\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0\n            }\n            evaluation_results.append(result_row)\n            # Use `continue` to skip the rest of the loop and move to the next model\n            continue\n    \n        # If predictions are valid, compute metrics as normal\n        rouge_scores = rouge_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bleu_scores = bleu_metric.compute(predictions=predictions, references=ground_truth_answers)\n        meteor_scores = meteor_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bertscore_scores = bertscore_metric.compute(predictions=predictions, references=ground_truth_answers, lang=\"vi\")\n    \n        # Store results (this part is the same as before)\n        result_row = {\n            \"Model & Prompt Strategy\": result_key,\n            \"ROUGE-L\": round(rouge_scores['rougeL'], 4),\n            \"BLEU\": round(bleu_scores['bleu'], 4),\n            \"METEOR\": round(meteor_scores['meteor'], 4),\n            \"BERTScore-F1\": round(sum(bertscore_scores['f1']) / len(bertscore_scores['f1']), 4),\n            \"Generation Time (s)\": round(generation_times.get(result_key, 0), 2), # Lấy thời gian đã lưu\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    # Sort for better comparison\n    results_df = results_df.sort_values(by=\"BERTScore-F1\", ascending=False).reset_index(drop=True)\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    display(results_df)\n\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:37:06.804971Z","iopub.execute_input":"2025-09-15T07:37:06.805174Z","iopub.status.idle":"2025-09-15T07:37:21.184923Z","shell.execute_reply.started":"2025-09-15T07:37:06.805157Z","shell.execute_reply":"2025-09-15T07:37:21.184261Z"}},"outputs":[],"execution_count":null}]}