{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Install necessary libraries\n!pip install -q transformers datasets accelerate bitsandbytes torch evaluate rouge_score sentencepiece bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T05:20:58.848138Z","iopub.execute_input":"2025-09-11T05:20:58.848409Z","iopub.status.idle":"2025-09-11T05:21:02.561094Z","shell.execute_reply.started":"2025-09-11T05:20:58.848382Z","shell.execute_reply":"2025-09-11T05:21:02.559994Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport random\nimport evaluate # Hugging Face's library for NLP evaluation\nimport warnings\n\n# Suppress warnings to keep the output clean\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T05:33:41.967533Z","iopub.execute_input":"2025-09-11T05:33:41.967811Z","iopub.status.idle":"2025-09-11T05:33:41.972163Z","shell.execute_reply.started":"2025-09-11T05:33:41.967790Z","shell.execute_reply":"2025-09-11T05:33:41.971444Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face\n# This is required to download gated models like Llama 3\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    # You can manually paste your token here for local testing if needed:\n    # hf_token = \"YOUR_HF_TOKEN\"\n    hf_token = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-11T05:21:02.562468Z","iopub.execute_input":"2025-09-11T05:21:02.562771Z","iopub.status.idle":"2025-09-11T05:21:02.611581Z","shell.execute_reply.started":"2025-09-11T05:21:02.562737Z","shell.execute_reply":"2025-09-11T05:21:02.610768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\nmodel_ids = [\n    \"alpha-ai/LLAMA3-3B-Medical-COT\",\n    \"vilm/vietcuna-3b-v2\"\n]\ndataset_id = \"tmnam20/ViMedAQA\"\nNUM_SAMPLES_TO_EVALUATE = 1\n\n# Step 4: Load and Prepare the Dataset\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(f\"Dataset loaded successfully! Total samples: {len(dataset)}\")\n\n    # Create a small, random, representative sample for evaluation\n    random.seed(42) # for reproducibility\n    random_indices = random.sample(range(len(dataset)), NUM_SAMPLES_TO_EVALUATE)\n    eval_dataset = dataset.select(random_indices)\n\n    print(f\"Created a random evaluation set with {len(eval_dataset)} samples.\")\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    eval_dataset = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- UPDATED: Step 1 - Define Bilingual Prompt Engineering Strategies ---\n\ndef create_llama3_prompts(sample):\n    \"\"\"\n    Creates a set of prompt variations for Llama 3 / ChatML format,\n    with instructions in both English and Vietnamese.\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n\n    prompts = {}\n\n    # --- Strategy 1: Original (Role-playing, strict context) ---\n    # English Instruction\n    prompts[\"Llama3_RolePlay_Strict_EN\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are a helpful medical assistant. Based *only* on the context provided below, answer the question in Vietnamese.\n\nContext: {context}\n\nQuestion: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    # Vietnamese Instruction\n    prompts[\"Llama3_RolePlay_Strict_VI\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nBạn là một trợ lý y tế hữu ích. Dựa *chỉ* vào ngữ cảnh được cung cấp dưới đây, hãy trả lời câu hỏi bằng tiếng Việt.\n\nNgữ cảnh: {context}\n\nCâu hỏi: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n\n    # --- Strategy 2: Direct and Simple ---\n    # English Instruction\n    prompts[\"Llama3_Direct_EN\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nUse the following context to answer the question.\n\nContext: {context}\n\nQuestion: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    # Vietnamese Instruction\n    prompts[\"Llama3_Direct_VI\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nSử dụng ngữ cảnh sau để trả lời câu hỏi.\n\nNgữ cảnh: {context}\n\nCâu hỏi: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    # --- Strategy 3: Chain-of-Thought Style ---\n    # English Instruction\n    prompts[\"Llama3_CoT_EN\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nRead the context, think step-by-step, and then answer the user's question based only on the information in the context.\n\nContext: {context}\n\nQuestion: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n    \n    # Vietnamese Instruction\n    prompts[\"Llama3_CoT_VI\"] = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHãy đọc ngữ cảnh, suy nghĩ từng bước, và sau đó trả lời câu hỏi của người dùng chỉ dựa vào thông tin trong ngữ cảnh.\n\nNgữ cảnh: {context}\n\nCâu hỏi: {question}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n    \n    return prompts\n\ndef create_vietcuna_prompts(sample):\n    \"\"\"\n    Creates a set of prompt variations for the Vicuna format,\n    with instructions in both English and Vietnamese.\n    \"\"\"\n    context = sample['context']\n    question = sample['question']\n    \n    prompts = {}\n\n    # --- Strategy 1: Original (Direct, no explanation) ---\n    # Vietnamese Instruction (Original)\n    instruction_vi_1 = (\n        \"Dựa vào ngữ cảnh sau đây để trả lời câu hỏi. Chỉ trích xuất câu trả lời trực tiếp từ văn bản, không giải thích gì thêm.\\n\\n\"\n        f\"Ngữ cảnh: {context}\\n\\n\"\n        f\"Câu hỏi: {question}\"\n    )\n    prompts[\"Vietcuna_Direct_NoExplain_VI\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_vi_1}\\nASSISTANT:\"\n\n    # English Instruction\n    instruction_en_1 = (\n        \"Based on the following context, answer the question. Only extract the direct answer from the text, do not add any explanation.\\n\\n\"\n        f\"Context: {context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n    prompts[\"Vietcuna_Direct_NoExplain_EN\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_en_1}\\nASSISTANT:\"\n\n    # --- Strategy 2: Role-playing ---\n    # Vietnamese Instruction\n    instruction_vi_2 = (\n        \"Bạn là một trợ lý y tế hữu ích. Dựa vào thông tin trong ngữ cảnh được cung cấp để trả lời câu hỏi của người dùng.\\n\\n\"\n        f\"Ngữ cảnh: {context}\\n\\n\"\n        f\"Câu hỏi: {question}\"\n    )\n    prompts[\"Vietcuna_RolePlay_VI\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_vi_2}\\nASSISTANT:\"\n\n    # English Instruction\n    instruction_en_2 = (\n        \"You are a helpful medical assistant. Based on the information in the provided context, answer the user's question.\\n\\n\"\n        f\"Context: {context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n    prompts[\"Vietcuna_RolePlay_EN\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_en_2}\\nASSISTANT:\"\n\n    # --- Strategy 3: Simplified (Context + Question only) ---\n    # Vietnamese Instruction\n    instruction_vi_3 = (\n        f\"Ngữ cảnh: {context}\\n\\n\"\n        f\"Câu hỏi: {question}\"\n    )\n    prompts[\"Vietcuna_Simplified_VI\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_vi_3}\\nASSISTANT:\"\n\n    # English Instruction\n    instruction_en_3 = (\n        f\"Context: {context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n    prompts[\"Vietcuna_Simplified_EN\"] = f\"A chat between a curious user and an artificial intelligence assistant.\\nUSER: {instruction_en_3}\\nASSISTANT:\"\n    \n    return prompts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Generate Answers from Each Model\nall_generated_answers = {}\n\nif eval_dataset and hf_token:\n    # Wrap each answer in a list to create the required List[List[str]] structure\n    ground_truth_answers = [[sample['answer']] for sample in eval_dataset] \n    questions = [sample['question'] for sample in eval_dataset]\n    \n    # Loop through each model to generate answers\n    for model_id in model_ids:\n        print(\"\\n\" + \"=\"*50)\n        print(f\"Loading model: {model_id}\")\n        print(\"=\"*50)\n\n        model, tokenizer, text_generator = None, None, None\n\n        try:\n            # Load the tokenizer and model with 4-bit quantization to save memory\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16,\n                bnb_4bit_use_double_quant=False,\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n            model = AutoModelForCausalLM.from_pretrained(\n                model_id,\n                token=hf_token,\n                quantization_config=bnb_config, # <-- PASS THE CONFIG OBJECT HERE\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n\n            # Set up the text generation pipeline\n            text_generator = pipeline(\n                \"text-generation\",\n                model=model,\n                tokenizer=tokenizer,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n            )\n\n            # Determine which set of prompt functions to use\n            if 'vietcuna' in model_id:\n                prompt_function = create_vietcuna_prompts\n                answer_start_tag = \"ASSISTANT:\"\n            else:\n                prompt_function = create_llama3_prompts\n                answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n\n            # Create one set of prompts to test. We get the keys from the first sample.\n            # This assumes all samples will generate the same prompt keys.\n            prompt_variations = prompt_function(eval_dataset[0]).keys()\n\n            for prompt_name in prompt_variations:\n                print(f\"\\n--- Testing Prompt Strategy: {prompt_name} ---\")\n                \n                # Generate all prompts for the current strategy\n                prompts = [prompt_function(sample)[prompt_name] for sample in eval_dataset]\n\n                print(f\"Generating answers for {len(prompts)} prompts using {model_id} with '{prompt_name}' strategy...\")\n                # Generate answers for the entire batch\n                generated_outputs_batch = text_generator(\n                    prompts,\n                    max_new_tokens=256,\n                    do_sample=True,\n                    temperature=0.1,\n                    top_p=0.9,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id, # Set pad_token_id to avoid warnings\n                )\n    \n                # Extract the clean answers\n                model_answers = []\n                if 'vietcuna' in model_id:\n                    # Vietcuna doesn't have a special start tag, it just starts generating.\n                    # The prompt ends with \"ASSISTANT:\", so we split on that.\n                    answer_start_tag = \"ASSISTANT:\"\n                else:\n                    # Llama 3's start tag\n                    answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n                \n                for i, output in enumerate(generated_outputs_batch):\n                    generated_text = output[0]['generated_text']\n                    # Use the appropriate start tag for splitting\n                    if answer_start_tag in generated_text:\n                        clean_answer = generated_text.split(answer_start_tag)[-1].strip()\n                    else:\n                        # Fallback remains the same\n                        clean_answer = generated_text.replace(prompts[i], \"\").strip()\n                    model_answers.append(clean_answer)\n    \n                # Validate that the number of generated answers matches the number of prompts\n                if len(model_answers) != len(prompts):\n                    print(f\"  [ERROR] Mismatch in generation count for model '{model_id}'.\")\n                    print(f\"  Expected {len(prompts)} answers, but got {len(model_answers)}.\")\n                    print(\"  This model will be skipped in the evaluation.\")\n                    # Use `continue` to immediately stop processing this model and move to the next one\n                    continue \n    \n                all_generated_answers[model_id] = model_answers\n                print(f\"Successfully generated answers for {model_id}.\")\n\n        except Exception as e:\n            print(f\"An error occurred while processing {model_id}: {e}\")\n        finally:\n            # Check if variables were successfully created before deleting\n            if model is not None: del model\n            if tokenizer is not None: del tokenizer\n            if text_generator is not None: del text_generator\n            torch.cuda.empty_cache()\n\nelse:\n    print(\"Skipping generation due to issues with the dataset or Hugging Face token.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Evaluate the Generated Answers\nif all_generated_answers:\n    # Load all the metrics we need\n    rouge_metric = evaluate.load('rouge')\n    bleu_metric = evaluate.load('bleu')\n    meteor_metric = evaluate.load('meteor')\n    bertscore_metric = evaluate.load('bertscore')\n\n    evaluation_results = []\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Calculating Evaluation Metrics\")\n    print(\"=\"*50)\n\n    for result_key, predictions in all_generated_answers.items():\n        print(f\"\\n--- Evaluating {result_key} ---\")\n    \n        # Check for empty predictions to prevent ZeroDivisionError in BLEU ---\n        # The `any()` function returns False if all strings in the list are empty.\n        if not any(predictions):\n            print(f\"  WARNING: Model & Prompt Strategy '{result_key}' produced empty answers for all samples. Assigning all metric scores to 0.\")\n            result_row = {\n                \"Model & Prompt Strategy\": result_key,\n                \"ROUGE-L\": 0.0,\n                \"BLEU\": 0.0,\n                \"METEOR\": 0.0,\n                \"BERTScore-F1\": 0.0\n            }\n            evaluation_results.append(result_row)\n            # Use `continue` to skip the rest of the loop and move to the next model\n            continue\n    \n        # If predictions are valid, compute metrics as normal\n        rouge_scores = rouge_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bleu_scores = bleu_metric.compute(predictions=predictions, references=ground_truth_answers)\n        meteor_scores = meteor_metric.compute(predictions=predictions, references=ground_truth_answers)\n        bertscore_scores = bertscore_metric.compute(predictions=predictions, references=ground_truth_answers, lang=\"vi\")\n    \n        # Store results (this part is the same as before)\n        result_row = {\n            \"Model & Prompt Strategy\": result_key,\n            \"ROUGE-L\": round(rouge_scores['rougeL'], 4),\n            \"BLEU\": round(bleu_scores['bleu'], 4),\n            \"METEOR\": round(meteor_scores['meteor'], 4),\n            \"BERTScore-F1\": round(sum(bertscore_scores['f1']) / len(bertscore_scores['f1']), 4)\n        }\n        evaluation_results.append(result_row)\n\n    # Step 7: Display Results\n    results_df = pd.DataFrame(evaluation_results)\n    # Sort for better comparison\n    results_df = results_df.sort_values(by=\"BERTScore-F1\", ascending=False).reset_index(drop=True)\n    print(\"\\n--- Comparative Evaluation Results ---\")\n    display(results_df)\n\n    # Display a few examples for manual inspection\n    print(\"\\n--- Example Generations ---\")\n    example_df_data = {\n        \"Question\": questions[:3],\n        \"Ground Truth\": ground_truth_answers[:3]\n    }\n    for model_id, answers in all_generated_answers.items():\n        example_df_data[f\"Answer: {model_id.split('/')[-1]}\"] = answers[:3]\n\n    example_df = pd.DataFrame(example_df_data)\n    display(example_df)\n\nelse:\n    print(\"\\nNo answers were generated. Skipping evaluation.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}