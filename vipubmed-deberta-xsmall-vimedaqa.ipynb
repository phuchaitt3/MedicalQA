{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes torch ragas evaluate rouge_score nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:38:21.803921Z","iopub.execute_input":"2025-09-09T06:38:21.804781Z","iopub.status.idle":"2025-09-09T06:38:26.259286Z","shell.execute_reply.started":"2025-09-09T06:38:21.804751Z","shell.execute_reply":"2025-09-09T06:38:26.258213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom datasets import load_dataset, Dataset\nfrom kaggle_secrets import UserSecretsClient\nimport random\nimport pandas as pd\n\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    answer_similarity,\n    answer_correctness,\n)\nfrom langchain_openai import ChatOpenAI\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:34:28.409216Z","iopub.execute_input":"2025-09-09T06:34:28.40953Z","iopub.status.idle":"2025-09-09T06:35:08.815614Z","shell.execute_reply.started":"2025-09-09T06:34:28.409503Z","shell.execute_reply":"2025-09-09T06:35:08.815003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from evaluate import load\nimport nltk\n\n# Load the metrics and handle potential errors\ntry:\n    print(\"Loading ROUGE metric...\")\n    rouge = load('rouge')\n    print(\"[SUCCESS] ROUGE metric loaded.\")\n    \n    print(\"\\nLoading METEOR metric...\")\n    meteor = load('meteor')\n    print(\"[SUCCESS] METEOR metric loaded.\")\n    \n    # METEOR requires the 'wordnet' corpus. We download it here to be safe.\n    print(\"\\nDownloading 'wordnet' for METEOR...\")\n    nltk.download('wordnet')\n    print(\"[SUCCESS] NLTK 'wordnet' downloaded.\")\n    \nexcept Exception as e:\n    print(f\"\\n[ERROR] Failed to load Hugging Face 'evaluate' metrics.\")\n    print(\"        Please check your notebook's internet connection and ensure all libraries are installed.\")\n    print(f\"        Underlying error: {e}\")\n    rouge = None\n    meteor = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:38:59.51001Z","iopub.execute_input":"2025-09-09T06:38:59.510767Z","iopub.status.idle":"2025-09-09T06:39:00.53242Z","shell.execute_reply.started":"2025-09-09T06:38:59.510734Z","shell.execute_reply":"2025-09-09T06:39:00.531616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Authenticate with Hugging Face and OpenAI\n# We retrieve the tokens you stored in Kaggle Secrets.\n\n# Hugging Face Token for the generation model\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nexcept Exception as e:\n    print(\"Could not retrieve Hugging Face token. Please ensure it is stored as a Kaggle secret named 'HUGGING_FACE_TOKEN'.\")\n    hf_token = None\n\n# OpenAI API Key for the Ragas evaluation model\ntry:\n    openai_api_key = user_secrets.get_secret(\"OPENAI_API_KEY\")\n    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n    # print(\"OpenAI API Key configured successfully.\")\nexcept Exception as e:\n    print(\"Could not retrieve OpenAI API Key. Please ensure it is stored as a Kaggle secret named 'OPENAI_API_KEY'.\")\n    openai_api_key = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:35:08.816367Z","iopub.execute_input":"2025-09-09T06:35:08.816928Z","iopub.status.idle":"2025-09-09T06:35:11.578921Z","shell.execute_reply.started":"2025-09-09T06:35:08.816898Z","shell.execute_reply":"2025-09-09T06:35:11.578151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Define Model and Dataset Identifiers\nmodel_id = \"manhtt-079/vipubmed-deberta-xsmall\"\ndataset_id = \"tmnam20/ViMedAQA\"\n\n# Step 4: Load the Dataset\n# We load the ViMedAQA dataset from Hugging Face.\n# This dataset contains Vietnamese medical questions and answers.\ntry:\n    dataset = load_dataset(dataset_id, split=\"train\")\n    print(\"Dataset loaded successfully!\")\n    print(\"Example from the dataset:\")\n    print(dataset[0])\nexcept Exception as e:\n    print(f\"Failed to load the dataset. Error: {e}\")\n    dataset = None\n\n# Step 5: Load the Model and Tokenizer\n# We will load the model in 4-bit precision (quantization) to save memory,\n# which is highly recommended for running larger models on Kaggle's GPUs.\nif hf_token:\n    try:\n        # Note the change to AutoModelForQuestionAnswering\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n        print(\"Extractive QA model and tokenizer loaded successfully!\")\n        \n    except Exception as e:\n        print(f\"Failed to load the model or tokenizer. Error: {e}\")\n        model = None\n        tokenizer = None\nelse:\n    print(\"Hugging Face token not available. Cannot load the model.\")\n    model = None\n    tokenizer = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:35:11.580351Z","iopub.execute_input":"2025-09-09T06:35:11.580552Z","iopub.status.idle":"2025-09-09T06:35:30.2207Z","shell.execute_reply.started":"2025-09-09T06:35:11.580536Z","shell.execute_reply":"2025-09-09T06:35:30.219889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Set up a Text Generation Pipeline and Prepare for Evaluation\nif model and tokenizer and dataset:\n    # The pipeline simplifies the process of using the model for extractive QA.\n    # We explicitly send it to the GPU (device=0).\n    qa_pipeline = pipeline(\n        \"question-answering\",\n        model=model,\n        tokenizer=tokenizer,\n        device=0 # Use 0 for the first GPU\n    )\n    print(\"Question Answering pipeline is ready.\")\n\n    # Select samples for evaluation\n    num_samples_to_evaluate = 3 \n    random_indices = random.sample(range(len(dataset)), num_samples_to_evaluate)\n    eval_dataset = dataset.select(random_indices)\n\n    # These lists will store the data needed for evaluation\n    ids_for_ragas = []\n    questions_for_ragas = []\n    contexts_for_ragas = []\n    ground_truths_for_ragas = []\n    generated_answers_for_ragas = [] # We'll call them 'generated' to match the old variable name\n\n    print(f\"\\nExtracting answers for {num_samples_to_evaluate} random samples...\")\n\n    # Loop through each sample and extract the answer\n    for i, sample in enumerate(eval_dataset):\n        question = sample['question']\n        context = sample['context']\n        \n        # The QA pipeline takes a question and context directly\n        result = qa_pipeline(question=question, context=context)\n        \n        # Store the results for our evaluation frameworks\n        ids_for_ragas.append(sample['question_idx'])\n        questions_for_ragas.append(question)\n        contexts_for_ragas.append([context]) # Ragas expects a list\n        ground_truths_for_ragas.append(sample['answer'])\n        generated_answers_for_ragas.append(result['answer']) # The pipeline returns a dict with the answer\n\n        # Print a few examples to see the model's performance\n        if i < 3:\n            print(f\"\\n--- Sample {i+1}/{num_samples_to_evaluate} ---\")\n            print(f\"Sample ID: {sample['question_idx']}\")\n            print(f\"Question: {question}\")\n            # print(f\"Context: {context}\") # Optional: uncomment to see the full context\n            print(f\"Model Answer (Extracted): {result['answer']}\")\n            print(f\"Confidence Score: {result['score']:.4f}\")\n            print(f\"Ground Truth: {sample['answer']}\")\n            print(\"-\" * 50)\n            \n    print(\"\\n--- Model Extraction Complete ---\")\n\nelse:\n    print(\"\\nSkipping extraction due to issues with model/tokenizer/dataset loading.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:35:30.221542Z","iopub.execute_input":"2025-09-09T06:35:30.221843Z","iopub.status.idle":"2025-09-09T06:35:43.078838Z","shell.execute_reply.started":"2025-09-09T06:35:30.221821Z","shell.execute_reply":"2025-09-09T06:35:43.077838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NEW CELL TO BE ADDED (with debugging) ---\n\n# Step 8.5: Calculate ROUGE and METEOR scores with Debugging\n\nprint(\"\\n--- [DEBUG] Pre-calculation Check for ROUGE/METEOR ---\")\n\n# --- Debugging Checks ---\n# 1. Check if the necessary lists from the generation step exist\ngenerated_answers_exist = 'generated_answers_for_ragas' in locals()\nground_truths_exist = 'ground_truths_for_ragas' in locals()\nprint(f\"[CHECK] 'generated_answers_for_ragas' list exists: {generated_answers_exist}\")\nprint(f\"[CHECK] 'ground_truths_for_ragas' list exists:   {ground_truths_exist}\")\n\n# 2. Check if the metric objects were loaded correctly\nrouge_loaded = 'rouge' in locals() and rouge is not None\nmeteor_loaded = 'meteor' in locals() and meteor is not None\nprint(f\"[CHECK] ROUGE metric object loaded:  {rouge_loaded}\")\nprint(f\"[CHECK] METEOR metric object loaded: {meteor_loaded}\")\n\n# 3. Perform the final check to decide whether to proceed\nall_checks_passed = generated_answers_exist and ground_truths_exist and rouge_loaded and meteor_loaded\n\nif all_checks_passed:\n    # --- Additional checks on the data itself ---\n    predictions = generated_answers_for_ragas\n    references = ground_truths_for_ragas\n    \n    if len(predictions) > 0 and len(predictions) == len(references):\n        print(\"\\n[SUCCESS] All checks passed. Starting metric calculation...\")\n        print(\"-\" * 50)\n        \n        try:\n            # The compute method can handle a list of predictions and references at once\n            rouge_results = rouge.compute(predictions=predictions, references=references)\n            meteor_results = meteor.compute(predictions=predictions, references=references)\n\n            # --- Display Individual Scores (for detailed analysis) ---\n            print(\"\\n--- Individual Sample Scores ---\")\n            for i in range(len(predictions)):\n                single_rouge = rouge.compute(predictions=[predictions[i]], references=[references[i]])\n                single_meteor = meteor.compute(predictions=[predictions[i]], references=[references[i]])\n                \n                print(f\"\\n--- Sample {i+1} ('{ids_for_ragas[i]}') ---\")\n                print(f\"Model Answer: {predictions[i]}\")\n                print(f\"Ground Truth: {references[i]}\")\n                print(f\"  ROUGE-1: {single_rouge['rouge1']:.4f}\")\n                print(f\"  ROUGE-2: {single_rouge['rouge2']:.4f}\")\n                print(f\"  ROUGE-L: {single_rouge['rougeL']:.4f}\")\n                print(f\"  METEOR:  {single_meteor['meteor']:.4f}\")\n                print(\"-\" * 30)\n\n            # --- Display Average Scores (for overall benchmark) ---\n            print(\"\\n--- Average Scores Across All Samples ---\")\n            print(f\"Average ROUGE-1: {rouge_results['rouge1']:.4f}\")\n            print(f\"Average ROUGE-2: {rouge_results['rouge2']:.4f}\")\n            print(f\"Average ROUGE-L: {rouge_results['rougeL']:.4f}\")\n            print(f\"Average METEOR:  {meteor_results['meteor']:.4f}\")\n\n        except Exception as e:\n            print(f\"\\n[ERROR] An unexpected error occurred during metric calculation: {e}\")\n\n    else:\n        # This handles cases where generation ran but produced no output\n        print(\"\\n[ERROR] Calculation skipped. The prediction/reference lists are empty or have mismatched lengths.\")\n        print(f\"        Number of predictions: {len(predictions)}\")\n        print(f\"        Number of references:  {len(references)}\")\n\nelse:\n    # This block runs if the initial checks fail, providing specific guidance\n    print(\"\\n[ERROR] Calculation skipped due to failed checks. Please review the messages above.\")\n    if not generated_answers_exist or not ground_truths_exist:\n        print(\" -> FIX: Ensure the previous cell (Step 8: Generate Answers) ran without errors and successfully created the 'generated_answers_for_ragas' and 'ground_truths_for_ragas' lists.\")\n    if not rouge_loaded or not meteor_loaded:\n        print(\" -> FIX: Scroll up to the second code cell (where libraries are imported) and check for any error messages when loading the metrics. Ensure your Kaggle notebook has internet enabled.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T06:39:27.778473Z","iopub.execute_input":"2025-09-09T06:39:27.778797Z","iopub.status.idle":"2025-09-09T06:39:31.519563Z","shell.execute_reply.started":"2025-09-09T06:39:27.778776Z","shell.execute_reply":"2025-09-09T06:39:31.518776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Debugging a single data point","metadata":{}},{"cell_type":"code","source":"# # Step 6 & 7: Isolate and Prepare a SINGLE Sample for Debugging\n\n# # ---------------------------------------------------------------------------------\n# # USER ACTION: CHANGE THIS ID to the specific sample you want to debug.\n# TARGET_SAMPLE_ID = \"body-part_1140\" \n# # ---------------------------------------------------------------------------------\n\n# if model and tokenizer and dataset:\n#     # Find the specific sample in the dataset\n#     target_sample = None\n#     for sample in dataset:\n#         if sample['question_idx'] == TARGET_SAMPLE_ID:\n#             target_sample = sample\n#             break\n            \n#     if target_sample:\n#         print(f\"Found sample with ID: {TARGET_SAMPLE_ID}\")\n        \n#         # Create a new mini-dataset containing only our target sample\n#         # Ragas and other functions expect a Dataset object, so we build one.\n#         eval_data_dict = {key: [value] for key, value in target_sample.items()}\n#         eval_dataset = Dataset.from_dict(eval_data_dict)\n        \n#         # --- The rest of the pipeline now runs on this single sample ---\n        \n#         text_generator = pipeline(\n#             \"text-generation\", model=model, tokenizer=tokenizer,\n#             torch_dtype=torch.bfloat16, device_map=\"auto\"\n#         )\n#         print(\"Text generation pipeline is ready.\")\n\n#         # Prepare prompts and data lists (now they will only have one item)\n#         prompts, ids_for_ragas, questions_for_ragas, contexts_for_ragas, ground_truths_for_ragas = [], [], [], [], []\n#         for sample in eval_dataset:\n#             ids_for_ragas.append(sample['question_idx'])\n#             questions_for_ragas.append(sample['question'])\n#             contexts_for_ragas.append([sample['context']])\n#             ground_truths_for_ragas.append(sample['answer'])\n#             prompt_template = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n#             Based on the context below, answer the question. \n            \n#             **Rules:**\n#             1. You MUST extract the answer directly from the context.\n#             2. The answer must be the exact, continuous text from the context.\n#             3. DO NOT add extra words or form a full sentence.\n            \n#             **Example:**\n#             - Context: \"Đến năm 1327, đây là thị trấn lớn thứ ba tại Warwickshire.\"\n#             - Question: \"Vào thế kỉ XIV, Birmingham trở thành thị trấn lớn thứ mấy tại Warwickshire?\"\n#             - Correct Answer: \"lớn thứ ba\"\n\n#             **Now, perform the task with the following:**\n            \n#             Context: {sample['context']}\n            \n#             Question: {sample['question']}\n\n#             <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n#             prompts.append(prompt_template)\n\n#         print(f\"\\nPreparing to generate an answer for sample {TARGET_SAMPLE_ID}...\")\n\n#         # Step 8: Generate the single answer\n#         try:\n#             generated_output = text_generator(\n#                 # prompts, max_new_tokens=256, do_sample=True, temperature=0.1, top_p=0.9,\n#                 prompts, max_new_tokens=256, do_sample=False, temperature=0.0, # Temperature 0.0 for deterministic extraction\n#                 eos_token_id=tokenizer.eos_token_id, padding=True, truncation=True\n#             )[0] # Get the first and only result\n\n#             generated_answers_for_ragas = []\n#             answer_start_tag = \"<|start_header_id|>assistant<|end_header_id|>\"\n#             clean_answer = generated_output[0]['generated_text'].split(answer_start_tag)[-1].strip()\n#             generated_answers_for_ragas.append(clean_answer)\n\n#             print(\"\\n--- Model Generation Complete ---\")\n#             print(f\"Sample ID: {ids_for_ragas[0]}\")\n#             print(f\"Question: {questions_for_ragas[0]}\")\n#             print(f\"Model Answer: {clean_answer}\")\n#             print(f\"Ground Truth: {ground_truths_for_ragas[0]}\")\n            \n#         except Exception as e:\n#             print(f\"An error occurred during text generation: {e}\")\n            \n#     else:\n#         print(f\"ERROR: Could not find any sample with ID '{TARGET_SAMPLE_ID}' in the dataset.\")\n\n# else:\n#     print(\"\\nSkipping generation due to issues with model, tokenizer, or dataset loading.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:08.831008Z","iopub.execute_input":"2025-09-08T02:10:08.831257Z","iopub.status.idle":"2025-09-08T02:10:08.836507Z","shell.execute_reply.started":"2025-09-08T02:10:08.831239Z","shell.execute_reply":"2025-09-08T02:10:08.835925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 9: Prepare the Dataset for Ragas Evaluation\n# Ragas expects a Hugging Face Dataset object with specific column names:\n# - question: The question asked.\n# - contexts: A list of context strings.\n# - answer: The answer generated by the model.\n# - ground_truth: The reference answer from the original dataset.\n\nif 'generated_answers_for_ragas' in locals():\n    # Create a dictionary with the collected data\n    ragas_data = {\n        \"question\": questions_for_ragas,\n        \"contexts\": contexts_for_ragas,\n        \"answer\": generated_answers_for_ragas,\n        \"ground_truth\": ground_truths_for_ragas\n    }\n\n    # Convert the dictionary to a Hugging Face Dataset\n    ragas_dataset = Dataset.from_dict(ragas_data)\n\n    print(\"Dataset prepared for Ragas evaluation.\")\n    print(ragas_dataset)\n\nelse:\n    print(\"Could not find generated answers. Skipping Ragas evaluation.\")\n    ragas_dataset = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:08.837241Z","iopub.execute_input":"2025-09-08T02:10:08.837559Z","iopub.status.idle":"2025-09-08T02:10:08.86769Z","shell.execute_reply.started":"2025-09-08T02:10:08.837529Z","shell.execute_reply":"2025-09-08T02:10:08.867065Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 10: Run Ragas with a Robust, Per-Sample, Sequential Evaluation\n\n# Disable debug mode for a cleaner output\nimport langchain\nlangchain.debug = False\n\n# Import Python's built-in warnings module\nimport warnings\nimport pandas as pd\nfrom tqdm.auto import tqdm # Import tqdm for a progress bar\n\nif 'generated_answers_for_ragas' in locals() and openai_api_key:\n    # Prepare the dataset for Ragas\n    ragas_data = {\n        \"question\": questions_for_ragas,\n        \"contexts\": contexts_for_ragas,\n        \"answer\": generated_answers_for_ragas,\n        \"ground_truth\": ground_truths_for_ragas\n    }\n    ragas_dataset = Dataset.from_dict(ragas_data)\n    \n    print(\"\\nStarting robust, per-sample Ragas evaluation...\")\n    print(\"=\"*50)\n\n    # Configure the judge LLM\n    evaluation_llm = ChatOpenAI(model=\"gpt-4.1-nano\")\n\n    # Define the metrics we want to compute\n    metrics_to_run = [\n        faithfulness,\n        answer_relevancy,\n        answer_similarity,\n        answer_correctness,\n    ]\n    \n    # --- This is the robust loop ---\n    all_sample_results = []\n    # Loop 1: Iterate through each sample in the dataset\n    for sample in tqdm(ragas_dataset, desc=\"Evaluating Samples\"):\n        # Create a mini-dataset with just the current sample\n        single_sample_dataset = Dataset.from_dict({k: [v] for k, v in sample.items()})\n        \n        # Dictionary to store all scores for the current sample\n        sample_scores = {\"question\": sample[\"question\"]}\n        \n        # Loop 2: Evaluate each metric sequentially for the current sample\n        for metric in metrics_to_run:\n            metric_name = metric.name\n            try:\n                # Run evaluation for only ONE metric on the ONE sample\n                result = evaluate(\n                    dataset=single_sample_dataset,\n                    metrics=[metric],\n                    llm=evaluation_llm\n                )\n                # Store the successful score\n                sample_scores[metric_name] = result[metric_name]\n            except Exception as e:\n                # If a metric fails, record it as NaN and continue\n                print(f\"  WARNING: Metric '{metric_name}' failed for question '{sample['question'][:50]}...'. Recording as NaN. Error: {e}\")\n                sample_scores[metric_name] = float('nan') # Explicitly set NaN on failure\n            \n        all_sample_results.append(sample_scores)\n\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"Ragas per-sample evaluation complete!\")\n    print(\"=\"*50 + \"\\n\")\n\n    # --- Display the results in a clean DataFrame ---\n    # This will now work correctly because all_sample_results is a list of dictionaries,\n    # and we have handled any potential failures by explicitly setting NaN.\n    results_df = pd.DataFrame(all_sample_results)\n    \n    # Reorder columns for better readability\n    column_order = ['question', 'faithfulness', 'answer_relevancy', 'answer_similarity', 'answer_correctness']\n    # Ensure all expected columns exist, adding them with NaN if they are missing\n    for col in column_order:\n        if col not in results_df.columns:\n            results_df[col] = float('nan')\n            \n    results_df = results_df[column_order]\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n        display(results_df)\n\n    # --- Optional: Print the final average scores ---\n    print(\"\\n--- Average Ragas Scores ---\")\n    average_scores = results_df.mean(numeric_only=True)\n    print(average_scores)\n\n\nelse:\n    print(\"Skipping Ragas evaluation. Check if generation was successful and if the OpenAI API Key is configured.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:35:43.170424Z","iopub.execute_input":"2025-09-08T02:35:43.170754Z","iopub.status.idle":"2025-09-08T02:36:15.268208Z","shell.execute_reply.started":"2025-09-08T02:35:43.170731Z","shell.execute_reply":"2025-09-08T02:36:15.267149Z"}},"outputs":[],"execution_count":null}]}